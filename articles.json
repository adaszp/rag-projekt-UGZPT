[
    {
        "file_name": "ai_principles1.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "What is Artificial Intelligence (AI)? Artificial intelligence (AI) is a set of technologies that enable computers to perform a variety of  advanced functions, including the ability to see, understand and translate spoken and written  language, analyze data, make recommendations, and more. AI is the backbone of innovation in modern computing, unlocking value for individuals and  businesses. For example, optical character recognition (OCR) uses AI to extract text and data  from images and documents, turns unstructured content into business-ready structured data,  and unlocks valuable insights. Artificial intelligence defined Artificial intelligence is a field of science concerned with building computers and machines that  can reason, learn, and act in such a way that would normally require human intelligence or that  involves data whose scale exceeds what humans can analyze. AI is a broad field that encompasses many different disciplines, including computer science,  data analytics and statistics, hardware and software engineering, linguistics, neuroscience, and  even philosophy and psychology. On an operational level for business use, AI is a set of technologies that are based primarily on  machine learning and deep learning, used for data analytics, predictions and forecasting, object  categorization, natural language processing, recommendations, intelligent data retrieval, and  more. How does AI work? While the specifics vary across different AI techniques, the core principle revolves around data.  AI systems learn and improve through exposure to vast amounts of data, identifying patterns  and relationships that humans may miss. This learning process often involves algorithms, which are sets of rules or instructions that guide  the AI's analysis and decision-making. In machine learning, a popular subset of AI, algorithms  are trained on labeled or unlabeled data to make predictions or categorize information. Deep learning, a further specialization, utilizes artificial neural networks with multiple layers to  process information, mimicking the structure and function of the human brain. Through  continuous learning and adaptation, AI systems become increasingly adept at performing  specific tasks, from recognizing images to translating languages and beyond. Want to learn how to get started with AI? Take the free beginner's introduction to generative AI. Types of artificial intelligence Artificial intelligence can be organized in several ways, depending on stages of development or  actions being performed. For instance, four stages of AI development are commonly recognized. 1. Reactive machines: Limited AI that only reacts to different kinds of stimuli based on preprogrammed rules. Does not use memory and thus cannot learn with new data.  IBM’s Deep Blue that beat chess champion Garry Kasparov in 1997 was an example of a  reactive machine. "
            },
            {
                "page_number": 2,
                "text": "2. Limited memory: Most modern AI is considered to be limited memory. It can use memory to improve over time by being trained with new data, typically through an  artificial neural network or other training model. Deep learning, a subset of machine  learning, is considered limited memory artificial intelligence. 3. Theory of mind: Theory of mind AI does not currently exist, but research is ongoing into its possibilities. It describes AI that can emulate the human mind and has decision- making capabilities equal to that of a human, including recognizing and remembering  emotions and reacting in social situations as a human would. 4. Self aware: A step above theory of mind AI, self-aware AI describes a mythical machine that is aware of its own existence and has the intellectual and emotional capabilities of a  human. Like theory of mind AI, self-aware AI does not currently exist. A more useful way of broadly categorizing types of artificial intelligence is by what the machine  can do. All of what we currently call artificial intelligence is considered artificial “narrow”  intelligence, in that it can perform only narrow sets of actions based on its programming and  training. For instance, an AI algorithm that is used for object classification won’t be able to  perform natural language processing. Google Search is a form of narrow AI, as is predictive  analytics, or virtual assistants. Artificial general intelligence (AGI) would be the ability for a machine to “sense, think, and act”  just like a human. AGI does not currently exist. The next level would be artificial  superintelligence (ASI), in which the machine would be able to function in all ways superior to a  human. Artificial intelligence training models When businesses talk about AI, they often talk about “training data.” But what does that mean?  Remember that limited-memory artificial intelligence is AI that improves over time by being  trained with new data. Machine learning is a subset of artificial intelligence that uses algorithms  to train data to obtain results. In broad strokes, three kinds of learnings models are often used in machine learning: Supervised learning is a machine learning model that maps a specific input to an output using  labeled training data (structured data). In simple terms, to train the algorithm to recognize  pictures of cats, feed it pictures labeled as cats. Unsupervised learning is a machine learning model that learns patterns based on unlabeled  data (unstructured data). Unlike supervised learning, the end result is not known ahead of time.  Rather, the algorithm learns from the data, categorizing it into groups based on attributes. For  instance, unsupervised learning is good at pattern matching and descriptive modeling. In addition to supervised and unsupervised learning, a mixed approach called semi-supervised  learning is often employed, where only some of the data is labeled. In semi-supervised learning,  an end result is known, but the algorithm must figure out how to organize and structure the data  to achieve the desired results. Reinforcement learning is a machine learning model that can be broadly described as “learn  by doing.” An “agent” learns to perform a defined task by trial and error (a feedback loop) until  its performance is within a desirable range. The agent receives positive reinforcement when it "
            },
            {
                "page_number": 3,
                "text": "performs the task well and negative reinforcement when it performs poorly. An example of  reinforcement learning would be teaching a robotic hand to pick up a ball. Common types of artificial neural networks A common type of training model in AI is an artificial neural network, a model loosely based on  the human brain. A neural network is a system of artificial neurons—sometimes called perceptrons—that are  computational nodes used to classify and analyze data. The data is fed into the first layer of a  neural network, with each perceptron making a decision, then passing that information onto  multiple nodes in the next layer. Training models with more than three layers are referred to as  “deep neural networks” or “deep learning.” Some modern neural networks have hundreds or  thousands of layers. The output of the final perceptrons accomplish the task set to the neural  network, such as classify an object or find patterns in data. Some of the most common types of artificial neural networks you may encounter include: Feedforward neural networks (FF) are one of the oldest forms of neural networks, with data  flowing one way through layers of artificial neurons until the output is achieved. In modern days,  most feedforward neural networks are considered “deep feedforward” with several layers (and  more than one “hidden” layer). Feedforward neural networks are typically paired with an error- correction algorithm called “backpropagation” that, in simple terms, starts with the result of the  neural network and works back through to the beginning, finding errors to improve the accuracy  of the neural network. Many simple but powerful neural networks are deep feedforward. Recurrent neural networks (RNN) differ from feedforward neural networks in that they typically  use time series data or data that involves sequences. Unlike feedforward neural networks,  which use weights in each node of the network, recurrent neural networks have “memory” of  what happened in the previous layer as contingent to the output of the current layer. For  instance, when performing natural language processing, RNNs can “keep in mind” other words  used in a sentence. RNNs are often used for speech recognition, translation, and to caption  images. Long/short term memory (LSTM) is an advanced form of RNN that can use memory to  “remember” what happened in previous layers. The difference between RNNs and LSTM is that  LSTM can remember what happened several layers ago, through the use of “memory cells.”  LSTM is often used in speech recognition and making predictions. Convolutional neural networks (CNN) include some of the most common neural networks in  modern artificial intelligence. Most often used in image recognition, CNNs use several distinct  layers (a convolutional layer, then a pooling layer) that filter different parts of an image before  putting it back together (in the fully connected layer). The earlier convolutional layers may look  for simple features of an image, such as colors and edges, before looking for more complex  features in additional layers. Generative adversarial networks (GAN) involve two neural networks competing against each  other in a game that ultimately improves the accuracy of the output. One network (the  generator) creates examples that the other network (the discriminator) attempts to prove true or  false. GANs have been used to create realistic images and even make art. Benefits of AI "
            },
            {
                "page_number": 4,
                "text": "Automation AI can automate workflows and processes or work independently and autonomously from a  human team. For example, AI can help automate aspects of cybersecurity by continuously  monitoring and analyzing network traffic. Similarly, a smart factory may have dozens of different  kinds of AI in use, such as robots using computer vision to navigate the factory floor or to  inspect products for defects, create digital twins, or use real-time analytics to measure  efficiency and output. Reduce human error AI can eliminate manual errors in data processing, analytics, assembly in manufacturing, and  other tasks through automation and algorithms that follow the same processes every single  time. Eliminate repetitive tasks AI can be used to perform repetitive tasks, freeing human capital to work on higher impact  problems. AI can be used to automate processes, like verifying documents, transcribing phone  calls, or answering simple customer questions like “what time do you close?” Robots are often  used to perform “dull, dirty, or dangerous” tasks in the place of a human. Fast and accurate AI can process more information more quickly than a human, finding patterns and discovering  relationships in data that a human may miss. Infinite availability AI is not limited by time of day, the need for breaks, or other human encumbrances. When  running in the cloud, AI and machine learning can be “always on,” continuously working on its  assigned tasks. Accelerated research and development The ability to analyze vast amounts of data quickly can lead to accelerated breakthroughs in  research and development. For instance, AI has been used in predictive modeling of potential  new pharmaceutical treatments, or to quantify the human genome. Applications and use cases for artificial intelligence Speech recognition Automatically convert spoken speech into written text. Image recognition Identify and categorize various aspects of an image. Translation Translate written or spoken words from one language into another. Predictive modeling Mine data to forecast specific outcomes with high degrees of granularity. "
            },
            {
                "page_number": 5,
                "text": "Data analytics Find patterns and relationships in data for business intelligence. Cybersecurity Autonomously scan networks for cyber attacks and threats. "
            }
        ],
        "images": []
    },
    {
        "file_name": "ai_principles2.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Artificial Intelligence AI is ushering in a new era of global innovation. From powering human ingenuity to counter the  spread of infectious diseases, to building smart cities and revolutionizing analytics for all  industries, AI is providing teams with the super-human power needed to do their life’s work. What Is AI? In its most fundamental form, AI is the capability of a computer program or a machine to think  and learn and take actions without being explicitly encoded with commands. AI can be thought  of as the development of computer systems that can perform tasks autonomously, ingesting  and analyzing enormous volumes of data, then recognizing patterns in that data. The large and  growing AI field of study is always oriented around developing systems that perform tasks that  would otherwise require human intelligence to complete—only at speeds beyond any  individual’s or group’s capabilities. For this reason, AI is broadly seen as both disruptive and  highly transformational. A key benefit of AI systems is the ability to actually learn from experiences or learn patterns from  data, adjusting on its own when new inputs and data are fed into these systems. This self- learning allows AI systems to accomplish a stunning variety of tasks, including image  recognition; natural language speech recognition; language translation; crop yield predictions;  medical diagnostics; navigation; loan risk analysis; error-prone boring human tasks; and  hundreds of other use cases. AI Growth Powered By GPU Advances Though the theory and early practice of AI go back three-quarters of a century, it wasn’t until the  21st century that practical AI business applications blossomed. This was the result of a  combination of huge advances in computing power and the enormous amounts of data  available. AI systems combine vast quantities of data with ultra-fast iterative processing  hardware and highly intelligent algorithms that allow the computer to ‘learn’ from data patterns  or data features. "
            },
            {
                "page_number": 2,
                "text": "The ideal hardware for the heavy work of AI systems are graphical processing units, or GPUs.  These specialized, superfast processors make parallel processing very fast and powerful. And  massive amounts of data—essentially the fuel for AI engines— comes from a wide variety of  sources, such as the Internet of Things (IoT); social media; historical databases; operational  data sources; various public and governmental sources; the global science and academic  communities; even genomic sources. Combining GPUs with enormous data stores and almost  infinite storage capabilities, AI is positioned to make an enormous impact on the business  world. Among the many and growing technologies propelling AI to broad usage are application  programming interfaces, or APIs. These are essentially highly portable bundles of code that  allow developers and data scientists to integrate AI functionality to current products and  services, expanding the value of existing investments. For example, APIs can add Q&A  capabilities that describe data or call out interesting insights and patterns. AI Challenges It isn’t an overstatement to say that artificial intelligence, or AI, offers the capability to transform  the productivity potential of the entire global economy. A study by PwC found that AI’s  contribution to the global economy will total nearly $17 trillion within ten years. To participate in  this AI-inspired economy, organizations need to overcome AI challenges. Acquiring raw computing power. The processing power needed to build AI systems and leverage techniques like machine  learning and image processing or language understanding is enormous. NVIDIA is the choice of  AI development teams around the world seeking to infuse AI into existing products and services  as they build out new and exciting ‘native AI’ services for GPUs and AI SDKs. Dealing with data bias. As with any other computer system, AI systems are only as good as the data fed into them. Bad  data can come from business, government, or other sources and contain racial, gender, or other "
            },
            {
                "page_number": 3,
                "text": "biases. Developers and data scientists must take extra precautions to prevent bias in AI data or  risk the trust people have in what AI systems actually learn. AI Use Cases Healthcare The world’s leading organizations are equipping their doctors and scientists with AI, helping  them transform lives and the future of research. With AI, they can tackle interoperable data,  meet the increasing demand for personalized medicine and next-generation clinics, develop  intelligent applications unique to their workflows, and accelerate areas like image analysis and  life science research. Uses cases include:    Pathology. Each year, major hospitals take millions of medical scans and tissue  biopsies, which are often scanned to create digital pathology datasets. Today, doctors  and researchers use AI to comprehensively and efficiently analyze these datasets to  classify a myriad of diseases and reduced mistakes when different pathologists disagree  on a diagnosis.    Patient care. The challenge today, as always, is for clinicians to get the right treatments  to patients as quickly and efficiently as possible. This is more of an acute need in  intensive care units. There, doctors using AI tools can leverage hourly vital sign  measurements to predict eight hours in advance whether patients will need treatments  to help them breathe, blood transfusions, or interventions to boost cardiac functions. . Retail An Accenture report estimates that AI has the potential to create $2.2 trillion worth of value for  retailers by 2035 by boosting growth and profitability. As it undergoes a massive digital  transformation, the industry can increase business value by using AI to improve asset  protection, deliver in-store analytics, and streamline operations.    Demand prediction. With over 100,000 different products in its 4,700 U.S. stores, the  Walmart Labs data science team must predict demand for 500 million items-by-store  combinations every week. By performing forecasting with the NVIDIA RAPIDS™ suite of  open-source data science and machine learning libraries built on NVIDIA CUDA-X™ AI  and NVIDIA GPUs, the Walmart team is able to engineer machine learning features 100X  faster and train algorithms 20X faster.    AiFi is currently pilot testing NanoStore, their 24/7, autonomous, checkout-free store,  with retail giants and universities. NanoStores hold over 500 different products and use  image recognition, powered by NVIDIA T4 Tensor Core GPUs, to capture merchandise  choices and add those to the customer’s tab. Telecommunications AI is opening up new waves of communication in the telecommunications industry. By tapping  into the power of GPUs and the 5G network, smart services can be brought to the edge,  simplifying deployment and enabling them to reach their full potential.    2Hz, Inc., is bringing clarity to live calls with noise-suppression technology powered by  NVIDIA T4 and V100 GPUs. 2Hz’s deep learning algorithms scale up to 20X more than "
            },
            {
                "page_number": 4,
                "text": "CPUs, and by running NVIDIA® TensorRT™ on GPUs, 2Hz meets the 12 millisecond (ms)  latency requirement for real-time communications    5G will deliver multiple computing capabilities, including gigabit speeds with latencies  under 20ms. This has led the Verizon Envrmnt team to deploy powerful NVIDIA GPUs to  beef up Verizon’s high-performance computing operations and create a distributed data  center. 5G will also enable devices to become thinner, lighter, and more battery  efficient, opening the door to memory-intensive parallel processing that can power  rendering, deep learning, and computer vision. Financial Services AI solutions have found a welcoming home in the dynamic world of financial services, with  scores of established and startup vendors rushing these solutions to market. The most popular  applications to date include:    Portfolio management and optimization. Historically, calculating portfolio risk has been  a largely manual and therefore extremely time-consuming process. Using AI, banks can  undertake highly complex queries in seconds without having to move sensitive data.    Risk management. Like portfolio management, risk management calculations are often  done in batch overnight, resulting in lost opportunities that occur 24/7. AI tools can  calculate risk using available data virtually in real-time, resulting in increased portfolio  performance and improved customer experience.    Fraud detection. With the ability to ingest tidal volumes of data and search instantly for  anomalies, AI solutions can then flag suspect patterns and trigger specific actions. Industrial One of the most common AI use cases is the crunching of enormous data streams from various  IoT devices for predictive maintenance. This can pertain to the monitoring of the condition of a  single piece of equipment, such as an electrical generator, or of an entire manufacturing facility  like a factory floor. AI systems harness data not only gathered and transmitted from the devices,  but also from various external sources, such as weather logs. Major railways use AI to predict  failures, applying the fixes before failure occurs—thereby keeping the trains running on time. AI  predictive maintenance on factory floors has been shown to reduce production line downtimes  dramatically.    Download our Ebook, “Implementing AI Solutions For Every Industry” to see customer  use cases in your industry. AI as a tool Data scientists think of AI as a tool and as a procedure that rests on top of other procedures or  methodologies used for deep analysis of data. In addition to languages like R and Python, data  scientists work with data from conventional databases, often extracting data  using SQL  queries. Using certain AI tools, they can quickly undertake tasks to classify and perform  predictions on these more conventional data sources. Why AI matters to... Machine Learning (ML) Researchers "
            },
            {
                "page_number": 5,
                "text": "Most of the researchers are working on AI, as it can be applied to almost any problem. and the  availability of large datasets and huge computation power has helped ML researchers create  breakthrough research in various domains and revolutionizing industries such as autonomous  vehicles, finance, agriculture, etc. Software Developers AI hasn’t advanced to the point where it can write software on its own, though enthusiasts say  that day isn’t far off. But various organizations already use AI to help develop and then test  software solutions, particularly custom software. In the past two years, software vendors have  brought to market an ever-growing number and variety of AI-enabled software development  tools. Some of the hottest and best-funded startups are those pioneering AI development tools. In one particularly exciting application of AI development tools, AI boosted project management  by ingesting enormous quantities of data from previous development projects. Then, the tools  accurately predicted the various tasks, resources, and schedules that would be needed to  manage new projects. This doesn’t mean AI can write software or replace developers, but it is  making the time these valuable developers spend creating custom software far more efficient. Why AI Is Better on an Accelerated Computing Platform AI models can be very large, especially Deep Neural Networks(DNNs), and require massive  computing power. Training these AI models requires highly parallelized tasks because the  computations are independent of each other. This makes it a good use case for distributed  processing on GPUs. With the recent advancements in GPUs, several Vision and Language AI  models can now be trained under a minute. NVIDIA is supercharging AI computing:A long pedigree in artificial intelligence NVIDIA invented the GPU in 1999. Then with the creation of the NVIDIA CUDA® programming  model and Tesla® GPU platform, NVIDIA brought parallel processing to general-purpose  computing. With AI innovation and high-performance computing converging, NVIDIA GPUs  powering AI solutions are enabling the world’s largest industries to tap into accelerated  computing and bring AI to the edge. Big breakthroughs with NVIDIA-powered neural networks Building game-changing AI applications begins with training neural networks. NVIDIA DGX-2™ is  the most powerful tool for AI training, using 16 GPUs to deliver 2 petaflops of training  performance to data teams. Adding in the extreme IO performance of NVIDIA Mellanox  InfiniBand networking, DGX-2 systems quickly scale up to supercomputer-class NVIDIA  SuperPODs. DGX-2 set world records on MLPerf, a new set of industry benchmarks designed to  test deep learning. NVIDIA DGX™ A100 is the most powerful system for all AI workloads, offering  high performance compute density, performance, and flexibility in the world’s first 5 petaFLOPS  AI system. Adding the extreme IO performance of Mellanox InfiniBand networking, DGX-A100  systems can quickly scale up to supercomputer-class NVIDIA POD. Boosting AI in the Cloud Trained AI applications are deployed in large-scale and highly complex cloud data centers  serving up voice, video, image, and other services to billions of users. With the rise of  conversational AI, the demand is increasing for these systems to work extremely fast to make "
            },
            {
                "page_number": 6,
                "text": "these services truly useful. NVIDIA TensorRT software and its T4 GPU combine to optimize,  validate, and accelerate these demanding networks. Meanwhile, as AI spills out of the cloud and into the edge where mountains of raw data are  generated by industries worldwide, the NVIDIA EGX™ platform puts AI performance closer to the  data to drive real-time decisions when and where they’re needed. "
            }
        ],
        "images": [
            "Image_23"
        ]
    },
    {
        "file_name": "attention_enhenced.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Attention Mechanisms in Deep Learning: Enhancing Model Performance In the ever-evolving field of deep learning, one concept that has garnered significant attention  (pun intended) is the Attention Mechanism. This ingenious concept has revolutionized the way  neural networks process and understand data, leading to remarkable improvements in model  performance across various applications. In this article, we will delve into the fascinating world  of attention mechanisms, understand their significance, and explore how they can be  implemented to supercharge your deep learning models. What is an Attention Mechanism? Imagine you are trying to understand a complex image or translate a sentence from one  language to another. Your brain instinctively focuses on specific parts of the image or particular  words in the sentence that are most relevant to your task. This selective focus is what we refer  to as attention, and it’s a fundamental aspect of human cognition. Attention mechanisms in  deep learning aim to mimic this selective focus process in artificial neural networks. The Building Blocks of Attention At its core, an attention mechanism allows a model to focus on different parts of the input  data with varying degrees of importance. It assigns weights to each element in the input  sequence, highlighting the elements that are most relevant to the task at hand. This not only  enhances the model’s understanding of the data but also improves its performance in tasks like  language translation, image captioning, and more. Key Takeaway: Attention mechanisms enable neural networks to mimic human-like selective  focus, improving their ability to process and understand complex data. Why are Attention Mechanisms Important? Attention mechanisms have become indispensable in various deep-learning applications due to  their ability to address some critical challenges: 1. Long Sequences: Traditional neural networks struggle with processing long sequences, such as translating a paragraph from one language to another. Attention mechanisms  allow models to focus on the relevant parts of the input, making them more effective at  handling lengthy data. 2. Contextual Understanding: In tasks like language translation, understanding the context of a word is crucial for accurate translation. Attention mechanisms enable  models to consider the context by assigning different attention weights to each word in  the input sequence. 3. Improved Performance: Models equipped with attention mechanisms often outperform their non-attention counterparts. They achieve state-of-the-art results in  tasks like machine translation, image classification, and speech recognition. Types of Attention Mechanisms There are several types of attention mechanisms, each designed to cater to specific use cases.  Here are a few notable ones: 1. Self-Attention Mechanism "
            },
            {
                "page_number": 2,
                "text": "Self-attention, also known as intra-attention, is commonly used in tasks involving sequences,  such as natural language processing. It allows the model to weigh the importance of each  element in the sequence concerning all the other elements. The Transformer model, for  instance, relies heavily on self-attention. 2. Scaled Dot-Product Attention Scaled Dot-Product Attention is a key component of the Transformer architecture. It calculates  attention scores by taking the dot product of a query vector and the keys, followed by scaling  and applying a softmax function. This type of attention mechanism is highly efficient and has  contributed to the success of Transformers in various applications. 3. Multi-Head Attention Multi-Head Attention extends the idea of attention by allowing the model to focus on different  parts of the input simultaneously. It achieves this by using multiple sets of learnable  parameters, each generating different attention scores. This technique enhances the model’s  ability to capture complex relationships within the data. 4. Location-Based Attention Location-based attention is often used in image-related tasks. It assigns attention scores based  on the spatial location of elements in the input. This can be particularly useful for tasks like  object detection and image captioning. Implementing Attention Mechanisms Now that we understand the importance of attention mechanisms, let’s explore how to  implement them in your deep-learning models. For this, we’ll use Python and the popular deep  learning library, TensorFlow. import tensorflow as tf  from tensorflow import keras  from tensorflow.keras.layers import Input, Dense, Attention    # Define an input layer  input_layer = Input(shape=(sequence_length, input_dimension))    # Add an attention layer  attention_layer = Attention()([input_layer, input_layer])    # Add other layers as needed  # ...    # Compile and train the model  model = keras.Model(inputs=input_layer, outputs=output_layer)  model.compile(optimizer='adam', loss='mean_squared_error')  model.fit(x_train, y_train, epochs=num_epochs, batch_size=batch_size) In this example, we’ve added a simple self-attention layer to your model. Depending on your  specific task, you can experiment with different types of attention mechanisms and  architectures. "
            },
            {
                "page_number": 3,
                "text": "Attention Mechanisms in Real-world Applications Understanding how attention mechanisms work is crucial, but it’s equally important to see how  they shine in real-world applications. Let’s take a closer look at a few domains where attention  mechanisms have made a significant impact. 1. Machine Translation Machine translation is an area where attention mechanisms have revolutionized the game.  Traditionally, translation models struggled with handling long sentences or paragraphs. With  attention mechanisms, these models can now focus on specific words or phrases in the  source language while generating the target language, greatly improving translation accuracy.  Google’s Transformer model, for instance, utilizes attention mechanisms to provide more  fluent and contextually accurate translations. Here is the sample of the Python code for machine translation: import tensorflow as tf  import numpy as np    # Define the input and target language data  input_data = ['I love deep learning', 'Machine translation is fascinating', 'Attention  mechanisms improve models']  target_data = ['J'adore l'apprentissage profond', 'La traduction automatique est fascinante',  'Les mécanismes d'attention améliorent les modèles']    # Tokenize the data  input_tokenizer = tf.keras.layers.TextVectorization()  input_tokenizer.adapt(input_data)  target_tokenizer = tf.keras.layers.TextVectorization()  target_tokenizer.adapt(target_data)    # Create tokenized sequences  input_sequences = input_tokenizer(input_data)  target_sequences = target_tokenizer(target_data)    # Define the model architecture  embedding_dim = 256  units = 1024    # Encoder  encoder_inputs = tf.keras.layers.Input(shape=(None,))  encoder_embedding = tf.keras.layers.Embedding(input_tokenizer.vocabulary_size,  embedding_dim)(encoder_inputs)  encoder, encoder_state_h, encoder_state_c = tf.keras.layers.LSTM(units,  return_state=True)(encoder_embedding)  encoder_states = [encoder_state_h, encoder_state_c]    # Decoder  decoder_inputs = tf.keras.layers.Input(shape=(None,)) "
            },
            {
                "page_number": 4,
                "text": "decoder_embedding = tf.keras.layers.Embedding(target_tokenizer.vocabulary_size,  embedding_dim)(decoder_inputs)  decoder_lstm = tf.keras.layers.LSTM(units, return_sequences=True, return_state=True)  decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)  attention = tf.keras.layers.Attention()([decoder_outputs, encoder_outputs])  decoder_concat = tf.keras.layers.Concatenate(axis=-1)([decoder_outputs, attention])  decoder_dense = tf.keras.layers.Dense(target_tokenizer.vocabulary_size,  activation='softmax')(decoder_concat)    # Create the model  model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_dense)    # Compile the model  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',  metrics=['accuracy'])    # Train the model (you would need a larger dataset for real training)  model.fit([input_sequences, target_sequences[:, :-1]], target_sequences[:, 1:], epochs=50)    # Translate a sample sentence  def translate(input_text):      input_seq = input_tokenizer([input_text])      target_seq = np.zeros((1, target_max_length))      target_seq[0, 0] = target_tokenizer.word_index['<start>']            for i in range(1, target_max_length):          predicted = model.predict([input_seq, target_seq])          predicted_word_index = np.argmax(predicted[:, i-1, :])          target_seq[0, i] = predicted_word_index                    if target_tokenizer.index_word[predicted_word_index] == '<end>':              break            translated_sentence = ' '.join([target_tokenizer.index_word[i] for i in target_seq[0] if i not in [0]])      return translated_sentence    # Example translation  input_text = 'I love deep learning'  translated_text = translate(input_text)  print('Input:', input_text)  print('Translation:', translated_text) 2. Image Captioning When it comes to describing the content of an image in natural language, attention mechanisms  are invaluable. Models equipped with these mechanisms can focus on different regions of the  image, generating captions that not only describe the image accurately but also provide  context (just like GPT 4.0 can analyze an image). This technology is particularly useful in "
            },
            {
                "page_number": 5,
                "text": "applications like autonomous vehicles, where the vehicle needs to understand its surroundings  and communicate effectively. 3. Speech Recognition In speech recognition, understanding context is essential for accurate transcription. Attention  mechanisms have played a crucial role in improving speech recognition systems. By focusing  on specific parts of the audio input, these systems can transcribe spoken words more  accurately, even in noisy environments. 4. Question Answering Question-answering systems, like those used in chatbots or virtual assistants, benefit from  attention mechanisms as well. These mechanisms help the model focus on relevant parts of  the input text while generating responses, leading to more contextually accurate and  coherent answers. The Evolution of Attention Mechanisms As with any technology, attention mechanisms have evolved over time. Researchers continue to  explore new variants and improvements to make these mechanisms even more effective. Some  recent developments include:    Sparse Attention: This approach aims to make attention more efficient by allowing  models to focus on only a subset of the input data, rather than all elements. This can  significantly reduce computational requirements while maintaining performance.    Memory Augmented Networks: These models combine attention mechanisms with  external memory, allowing them to store and retrieve information efficiently. This is  particularly useful in tasks that involve reasoning and long-term dependencies.    Cross-modal Attention: In scenarios where data comes from multiple modalities, such  as text and images, cross-modal attention mechanisms enable models to learn  relationships between different types of data. This is valuable in applications like image  captioning. Conclusion In the world of deep learning, attention mechanisms stand out as a powerful tool for  enhancing model performance. Their ability to mimic human-like selective focus has  transformed a wide range of applications, from machine translation to image captioning. As you  continue your journey into the depths of deep learning, consider attention mechanisms as a  vital ingredient in your model’s success. Remember that the key to mastering attention mechanisms lies in practice and  experimentation. Try different types of attention, explore various architectures, and fine- tune your models to suit your specific tasks. And, of course, keep an eye on the latest research  and developments in this exciting field, as the world of deep learning is constantly evolving. So, whether you’re a seasoned deep learning practitioner or just getting started, harness the  power of attention mechanisms to take your models to new heights. With the right focus, your  models can achieve levels of performance that were once thought to be beyond reach. Happy  deep learning! "
            },
            {
                "page_number": 6,
                "text": "FAQs (Frequently Asked Questions) What is the fundamental concept behind an Attention Mechanism? An Attention Mechanism in deep learning simulates the human cognitive process of selective  focus. It enables neural networks to assign varying degrees of importance to different parts of  input data, enhancing their understanding of the data. This mechanism is crucial for improving  model performance across various applications. How do Attention Mechanisms improve model performance in deep learning? Attention Mechanisms address critical challenges in deep learning, including handling long  sequences effectively, ensuring contextual understanding, and achieving improved overall  model performance. They enable models to focus on relevant parts of input data, making them  highly efficient in processing complex information. What are some types of Attention Mechanisms used in deep learning? There are several types of attention mechanisms, including:    Self-Attention Mechanism: Used in tasks involving sequences like natural language  processing.    Scaled Dot-Product Attention: An efficient mechanism that’s a key component of the  Transformer architecture.    Multi-Head Attention: Allows models to focus on different parts of input data  simultaneously.    Location-Based Attention: Commonly used in image-related tasks, assigning attention  based on spatial location. How can I implement Attention Mechanisms in my deep-learning models? To implement Attention Mechanisms in your deep learning models, you can use popular  libraries like TensorFlow or PyTorch. In TensorFlow, you can add an attention layer to your  model, as shown in the provided Python code example. Depending on your specific task, you  can experiment with different types of attention mechanisms and architectures. What are some real-world applications where Attention Mechanisms have had a significant  impact? Attention Mechanisms have made a substantial impact in various domains, including:    Machine Translation: They enhance translation accuracy by focusing on relevant parts  of the source language text.    Image Captioning: They enable models to describe image content accurately, adding  contextual information.    Speech Recognition: By understanding context, they improve speech transcription,  even in noisy environments.    Question Answering: They help models focus on relevant information in the text while  generating answers. "
            }
        ],
        "images": []
    },
    {
        "file_name": "attention_mechanism.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "What are Attention Mechanisms in Deep Learning? Attention mechanism is a fundamental invention in artificial intelligence and machine learning,  redefining the capabilities of deep learning models. This mechanism, inspired by the human  mental process of selective focus, has emerged as a pillar in a variety of applications,  accelerating developments in natural language processing, computer vision, and beyond. Imagine if machines could pay attention selectively, the way we do, focusing on critical features  in a vast amount of data. This is the essence of the attention mechanism, a critical component  of today’s deep learning models. This article will take you on a journey to learn about the heart, growth, and enormous  consequences of attention mechanisms in deep learning. We’ll look at how they function, from  the fundamentals to their game-changing impact in several fields. What is an Attention Mechanism? Attention mechanism is a technique used in deep learning models that allows the model to  selectively focus on specific areas of the input data when making predictions. This is very helpful when working with extensive data sequences, like in natural language  processing or computer vision tasks. Rather than processing all inputs identically, this mechanism allows the model to pay different  levels of attention to distinct bits of data. It’s similar to how our brains prioritize particular  elements when processing information, allowing the model to focus on what’s important,  making it tremendously strong for tasks like interpreting language or identifying patterns in  photos. Attention was originally employed in neural machine translation to assist the model in focusing  on the most significant words or phrases in a sentence when translating it into another  language. Since then, attention has become widely used in a variety of deep learning  applications, including computer vision, speech recognition, and recommender systems. How Does the Attention Mechanism Work? The attention mechanism works by allowing a deep learning model to focus on different parts of  the input sequence and give varying amounts of value to distinct elements. This selective focus  enables the model to weigh and prioritize information adaptively, improving its capacity to  detect relevant patterns and connections in the data. Here’s a step-by-step breakdown of how most attention mechanisms work: 1. The model is given the input sequence, which tends to be a sequence of vectors or embeddings. This might be a natural language statement, a sequence of photos, or any  other structured input. 2. The calculation of scores that represent the relevance of each element in the input sequence begins with the calculation of attention. The scores are derived using a  similarity measure between the model’s current state or context and each element in  the input. 3. The scores are then processed through a softmax function (a mathematical function that turns an array of real numbers into a probability distribution) to produce probability- "
            },
            {
                "page_number": 2,
                "text": "like values. These are the attention weights, which indicate the relative relevance of  each element. Higher weights indicate greater relevance, whereas lower weights  indicate less importance. 4. Attention weights are used to compute a weighted sum of the components in the input sequence. Each element is multiplied by its attention weight, and the results are added  together. This generates a context vector, which represents the focused information that  the model deems most important. 5. The context vector is then combined with the model’s current state to generate an output. This output indicates the model’s prediction or decision at a specific phase in a  sequence-to-sequence job. 6. The attention mechanism is used iteratively in tasks demanding sequential processing, such as natural language translation. The context vector is recalculated at each step  based on the input sequence and the model’s previous state. 7. Backpropagation is used during training to learn the attention weights. These weights are adjusted by the model to optimize its performance on the task at hand. This learning  process trains the model to focus on the most important bits of the input. Overall, the attention mechanism operates by dynamically distributing attention weights to  various portions of the input sequence, allowing the model to focus on what is most important  for a given job. The model’s adaptability improves its ability to handle information in a more  contextually aware and efficient manner. Basic Concepts of the Attention Mechanism in Deep Learning Models Scaled-Dot-Product Attention The scaled dot product attention mechanism is a common sort of attention mechanism seen in  transformer models. It operates by computing a weighted sum of the input items, where the  weights are acquired during training and reflect the relative relevance of each input piece. Assume you’re working with computer software that must comprehend and prioritize various  portions of a story or text. In this instance, we refer to these components as “vectors” — they are  known as “keys,” “values,” and “queries.”    Query (Q): This is like a question. The program wants to know something specific.    Key (K): These are like the pieces of information it has. Each piece has its key.    Value (V): This is the actual information associated with each key. The program is attempting to determine which pieces of information are most significant to the  inquiry. This is accomplished by determining how similar the question (Q) is to each item of  information (K). To measure this resemblance, the program employs a simple method known as a “dot  product.” It multiplies and adds the corresponding portions of the query and the information  component. It’s the same as asking, “How much do they align?” We scale down the findings to keep things stable because we’re dealing with a lot of statistics.  It’s similar to ensuring that the numbers aren’t too large or too small so that the computer can  grasp them better. "
            },
            {
                "page_number": 3,
                "text": "The algorithm now wants to determine how much weight to assign to each piece of information.  This is accomplished through the use of another technique known as “softmax.” This converts  the similarities into weights – the higher the weight, the more attention that component  receives. Finally, the program takes all of the information (V) and merges it, but each component is  weighted based on how much attention it receives. This generates a new piece of information —  the “context” — which functions as a summary of the most significant elements. In basic terms, the scaled dot product attention mechanism functions similarly to a smart  technique for a computer to focus on the most important elements when attempting to  understand or summarize information. It’s similar to how we pay attention to keywords in a  phrase to better understand its meaning. Multi-Head Attention The multi-head attention mechanism is an important component of deep learning models,  particularly in designs such as the Transformer. It enables the model to attend to different parts  of the input sequence concurrently, capturing diverse characteristics or patterns. This  mechanism improves the model’s ability to learn and process data more thoroughly. Consider how you would solve a complex problem if you had a team of specialists, each  specializing in a different area. For example, if you’re working on a puzzle with several types of  components (colors, shapes, patterns), you may have one expert concentrate on colors,  another on shapes, and so on. In deep learning, when your model encounters a complex task, it needs to understand different  aspects, just like the puzzle example. Each aspect could be a different feature of the input data. Multi-head attention is equivalent to having numerous specialists, each focusing on a specific  area of the data. They collaborate as a group. Each expert (or head) poses a specific inquiry regarding the incoming data. In our puzzle  scenario, one would question, “What colors are there?” while another might ask, “What are the  shapes?” Based on their experience, each expert extracts the most relevant information. They focus on  their designated aspect while ignoring the rest. All of the experts’ information is pooled. It’s like fitting together puzzle pieces. Different views  help the model capture a more comprehensive knowledge of the input. As a whole, multi-head attention is equivalent to having a team of specialists, each focusing on  a distinct aspect of the incoming data. They provide a more extensive and nuanced  understanding, allowing the model to handle more complicated tasks. It is a collaborative  endeavor that draws on multiple viewpoints to solve problems more effectively. Applications of Attention Mechanism The attention mechanism has found applications in artificial intelligence and deep learning in a  wide range of domains. Here are some notable scenarios: 1. Machine Translation: Attention mechanisms enhanced the quality of machine translation systems dramatically. They enable models to concentrate on certain words "
            },
            {
                "page_number": 4,
                "text": "or phrases in the source language when producing the corresponding terms in the target  language, hence boosting translation accuracy. 2. Natural Language Processing (NLP): The attention mechanism aids models in understanding and extracting meaningful information from input sequences in NLP  tasks such as sentiment analysis, question answering, and text summarization,  boosting overall task performance. 3. Computer Vision: Computer vision activities that require attention include image captioning, visual question answering, and image-to-image translation. It allows the  model to focus on certain areas of an image, improving the description or translation. 4. Medical Image Analysis: In medical image processing tasks like illness identification in radiological pictures, attention mechanisms are used. They allow models to focus on  specific areas of interest, assisting in the correct identification of anomalies. 5. Autonomous Vehicles: Attention mechanisms are employed in the field of computer vision for autonomous vehicles to recognize and focus on essential objects or features  in the surroundings, resulting in superior object detection and scene perception. 6. Reinforcement Learning: In reinforcement learning cases, attention mechanisms are used to allow models to focus on essential information in the environment or state  space, resulting in better decision-making. These applications demonstrate the adaptability and usefulness of attention mechanisms in a  variety of areas, where the capacity to choose and focus on relevant information adds to  improved deep-learning model performance. These are only a handful of the many uses of the attention mechanism in deep learning. As  research advances, attention is likely to play a more significant role in addressing complicated  challenges across multiple areas. Advantages of Attention Mechanism in Deep Learning Models The attention mechanism in deep learning models has multiple benefits, including enhanced  performance and versatility across a variety of tasks. The following are some of the primary  benefits of attention mechanisms: 1. Selective Information Processing: The attention mechanism enables the model to concentrate on select parts of the input sequence, emphasizing critical information  while potentially ignoring less significant bits. This improves the model’s ability to  recognize dependencies and patterns in data, resulting in more effective learning. 2. Improved Model Interpretability: Through attention weights, the Attention Mechanism reveals which elements of the input data are considered relevant for a given prediction,  improving model interpretability and assisting practitioners and stakeholders in  understanding and believing model judgments. 3. Capturing Long-Range Dependencies: It tackles the challenge of capturing long-term dependencies in sequential data by allowing the model to connect distant pieces,  boosting the model’s ability to recognize context and relationships between elements  separated by substantial distances. "
            },
            {
                "page_number": 5,
                "text": "4. Transfer Learning Capabilities: It aids in knowledge transfer by allowing the model to focus on relevant aspects when adapting information from one task to another. This  improves the model’s adaptability and generalizability across domains. 5. Efficient Information Processing: It enables the model to process relevant information selectively, decreasing computational waste and enabling more scalable and efficient  learning, improving the model’s performance on large datasets and computationally  expensive tasks. In general, attention mechanisms benefit deep learning models significantly by facilitating  selective information processing, addressing sequence-related difficulties, enhancing  interpretability, and enabling efficient and scalable learning. These benefits lead to the  widespread use and effectiveness of attention-based models in a variety of applications. Cons Of The Attention Mechanism While the attention mechanism has transformed natural language processing and has been  effectively implemented in a variety of different disciplines, it does have some drawbacks that  should be considered: 1. Computational Complexity: Attention processes can greatly increase a model’s computational complexity, particularly when dealing with long input sequences.  Because of the increasing complexity, training and inference periods may be longer,  making attention-based models more demanding of resources. 2. Dependency on Model Architecture: The overall model design and the job at hand can influence the effectiveness of attention mechanisms. Attention mechanisms do not  benefit all models equally, and their influence varies among architectures. 3. Overfitting Risks: Overfitting can also affect attention mechanisms, especially when the number of attention heads is significant. When there are too many attention heads in  the model, it may begin to memorize the training data rather than generalize to new data.  As a result, performance on unseen data may suffer. 4. Attention to Noise: Attention mechanisms may pay attention to noisy or irrelevant sections of the input, particularly when the data contains distracting information. This  can result in inferior performance and necessitates careful model adjustment. Despite these constraints, attention methods have revolutionized natural language processing  and shown promising advances in a variety of other disciplines. Researchers are working on  improvements and ways to alleviate some of the drawbacks of attention mechanisms. Conclusion Deep learning’s attention mechanism is a game changer, altering how machines process  complex information. Attention mechanisms have become a critical tool, supercharging the  powers of artificial intelligence, whether it’s the basics or its real-world applications. In a nutshell, attention mechanisms assist machines in focusing on what is important in data,  allowing them to perform better at tasks such as language processing, image recognition, and  others. It’s more than simply a technical change – it’s a significant player in the realm of artificial  intelligence, bringing up intriguing possibilities for smarter and more efficient systems. "
            }
        ],
        "images": []
    },
    {
        "file_name": "deep_learning1.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Deep Learning Deep learning is a subset of artificial intelligence (AI)  and machine learning (ML) that uses multi- layered artificial neural networks to deliver state-of-the-art accuracy in tasks like object  detection, speech recognition, language translation, and others. What is Deep Learning? Deep learning is a subset of machine learning, with the difference that DL algorithms can  automatically learn representations from data such as images, video, or text, without  introducing human domain knowledge. The word 'deep' in deep learning represents the many  layers of algorithms, or neural networks, that are used to recognize patterns in data. DL’s highly  flexible architectures can learn directly from raw data, similar to the way the human brain  operates, and can increase their predictive accuracy when provided with more data. Further, deep learning is the primary technology that allows high precision and accuracy in  tasks such as speech recognition, language translation, and object detection. It has led to many  recent breakthroughs in AI, including Google DeepMind’s AlphaGo, self-driving cars, intelligent  voice assistants, and many others. How Deep Learning Works Deep learning uses multi-layered artificial neural networks (ANNs), which are networks  composed of several 'hidden layers' of nodes between the input and output. "
            },
            {
                "page_number": 2,
                "text": "An artificial neural network transforms input data by applying a nonlinear function to a weighted  sum of the inputs. The transformation is known as a neural layer and the function is referred to  as a neural unit. The intermediate outputs of one layer, called features, are used as the input into the next layer.  The neural network learns multiple layers of nonlinear features (like edges and shapes) through  repeated transformations, which it then combines in a final layer to create a prediction (of more  complex objects). With backpropagation inside of a process called gradient descent, the errors are sent back  through the network again and the weights are adjusted, improving the model. The neural net  learns by varying the weights or parameters of a network so as to minimize the difference  between the predictions of the neural network and the desired values. This process is repeated  thousands of times, adjusting a model's weights in response to the error it produces, until the  error can't be reduced anymore. This phase where the artificial neural network learns from the  data is called training.  During this process, the layers learn the optimal features for the model,  which has the advantage that features do not need to be predetermined. "
            },
            {
                "page_number": 3,
                "text": "GPUs: Key to Deep Learning Architecturally, the CPU is composed of just a few cores with lots of cache memory that can  handle a few software threads at a time. In contrast, a GPU is composed of hundreds of cores  that can handle thousands of threads simultaneously. State-of-the-art deep learning neural networks can have from millions to well over one billion  parameters to adjust using backpropagation. They also require a large amount of training data  to achieve high accuracy, meaning hundreds of thousands to millions of input samples will have  to be run through both a forward and backward pass. Because neural nets are created from  large numbers of identical neurons, they’re highly parallel by nature. This parallelism maps  naturally to GPUs, providing a significant computation speedup over CPU-only training and  making them the platform of choice for training large, complex neural network-based systems.  The parallel nature of inference operations also lend themselves well for execution on GPUs. Deep Learning Use Cases Deep learning is commonly used across apps in computer vision, conversational AI, and  recommendation systems. Computer vision apps use deep learning to gain knowledge from  digital images and videos. Conversational AI apps help computers understand and  communicate through natural language. And recommendation systems use images, language,  and a user’s interests to offer meaningful and relevant search results and services. Deep learning is enabling self-driving cars, smart personal assistants, and smarter web  services. Applications of deep learning, such as fraud detection and supply chain  modernization, are also being used by the world’s most advanced teams and organizations. There are different variations of deep learning algorithms, such as the following:    ANNs where information is only fed forward from one layer to the next are called  feedforward artificial neural networks.  Multilayer perceptrons (MLPs) are a type of  feedforward ANN consisting of at least three layers of nodes: an input layer, a hidden  layer and an output layer. MLPs are good at classification prediction problems using  labeled inputs. They’re flexible networks that can be applied to a variety of scenarios.    Convolutional Neural Networks are the image crunchers to identify objects. CNN image  recognition is better in some scenarios than humans, and that ranges from cats to  identifying indicators for cancer in blood and tumors in MRI scans. CNNs  are today’s  eyes of autonomous vehicles, oil exploration, and fusion energy research. In healthcare,  they can help spot diseases faster in medical imaging and save lives.     Recurrent neural networks are the mathematical engines to parse language patterns  and sequenced data. "
            },
            {
                "page_number": 4,
                "text": "o  These networks  are revving up a voice-based computing revolution and provide  the natural language processing brains that give ears and speech to Amazon’s  Alexa, Google’s Assistant, and Apple’s Siri. They also lend clairvoyant-like magic  to Google’s autocomplete feature that fills in lines of your search queries. o  RNN applications extend beyond natural language processing and speech  recognition. They’re used in language translation, stock predictions, and  algorithmic trading as well. o  To detect fraud in finance, anomalous spending patterns can be red-flagged  using RNNs, which are particularly good at guessing what comes next in a  sequence of data. American Express has deployed deep-learning-based models  optimized with NVIDIA® TensorRT™ and running on NVIDIA Triton™ Inference Server  to detect fraud. Deep Learning Benefits One benefit of deep learning is its inherent flexibility in developing approximations for diverse  sets of data. Data scientists can develop approximations for just about anything when using  deep learning and neural networks. The accuracy of the predictions and analysis of deep  learning when trained with huge amounts of data is unparalleled. A clear advantage of using deep learning over machine learning is the ability to execute feature  engineering on its own. Using deep learning, an algorithm can scan data searching for features  that correlate, then combine them to enable faster learning without any human intervention. Why Deep Learning Matters to Researchers and Data Scientists With NVIDIA GPU-accelerated deep learning frameworks, researchers and data scientists can  significantly speed up deep learning training that could otherwise take days and weeks to just  hours and days. When models are ready for deployment, developers can rely on GPU- accelerated inference platforms for the cloud, embedded device, or self-driving cars, to deliver  high-performance, low-latency inference for the most computationally-intensive deep neural  networks. NVIDIA Deep Learning for Developers GPU-accelerated deep learning frameworks offer flexibility to design and train custom deep  neural networks and provide interfaces to commonly used programming languages such as  Python and C/C++. Widely used deep learning frameworks such as MXNet, PyTorch,  TensorFlow, and others rely on NVIDIA GPU-accelerated libraries to deliver high-performance,  multi-GPU accelerated training. "
            },
            {
                "page_number": 5,
                "text": "Next Steps    NVIDIA provides optimized software stacks to accelerate training and inference phases  of the deep learning workflow. Learn more on the NVIDIA deep learning home page.    Developers, researchers, and data scientists can get easy access to NVIDIA optimized  deep learning framework containers with deep learning examples that are performance  tuned and tested for NVIDIA GPUs. This eliminates the need to manage packages and  dependencies or build deep learning frameworks from source. Visit NVIDIA NGC to learn  more and get started.    Designed specifically for deep learning, Tensor Cores on NVIDIA Volta™ and Turing™  GPUs deliver significantly higher training and inference performance. Learn more about  accessing reference implementations.    The NVIDIA Deep Learning Institute (DLI) offers hands-on training for developers, data  scientists, and researchers in AI and accelerated computing    For a more technical deep dive on deep learning check out Deep learning in a nutshell    For developer news and resources check out the  NVIDIA developers site.    Learn more about deep learning recommendation , read  NVIDIA Merlin: An Application  Framework for Deep Recommender Systems    To learn more about deep learning read Deep learning in a nutshell "
            }
        ],
        "images": [
            "Image_21",
            "Image_24",
            "Image_25",
            "Image_27",
            "Image_53",
            "Image_76"
        ]
    },
    {
        "file_name": "deep_learning2.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "What is deep learning in AI? Deep learning is an artificial intelligence (AI) method that teaches computers to process data in  a way inspired by the human brain. Deep learning models can recognize complex pictures, text,  sounds, and other data patterns to produce accurate insights and predictions. You can use  deep learning methods to automate tasks that typically require human intelligence, such as  describing images or transcribing a sound file into text. Watch our introduction to deep learning What is deep generative learning? Deep generative learning is deep learning that focuses on creating new output from learned  input. Traditionally, deep learning focused on identifying relationships between data. Deep  learning models were trained with large amounts of data to recognize patterns in the data set. Deep generative learning adds generation to pattern recognition. Such models look for data  patterns and then create their own unique patterns. For example, they can analyze the text in  several books and then use the information to generate new sentences and paragraphs not  found in the original books. Deep generative learning is the basis of modern generative AI and foundation models. These  models use deep learning technologies at scale, trained on vast data, to perform complex tasks  like answering questions, creating images from text, and writing content. Watch an introductory video to foundation models Why is deep learning important? Deep learning technology drives many artificial intelligence applications used in everyday  products, such as the following:    Chatbots and code generators    Digital assistants    Voice-activated television remotes    Fraud detection    Automatic facial recognition It is also a critical component of technologies like self-driving cars, virtual reality, and more.  Businesses use deep learning models to analyze data and make predictions in various  applications. What are deep learning use cases? Deep learning has several use cases in automotive, aerospace, manufacturing, electronics,  medical research, and other fields.    Self-driving cars use deep learning models for object detection.    Defense systems use deep learning to flag areas of interest in satellite images.    Medical image analysis uses deep learning to detect cancer cells for medical diagnosis. "
            },
            {
                "page_number": 2,
                "text": "   Factories use deep learning applications to detect when people or objects are within an  unsafe distance of machines. These various use cases of deep learning can be grouped into five broad categories: computer  vision, speech recognition, natural language processing (NLP), recommendation engines, and  generative AI. Computer vision Computer vision automatically extracts information and insights from images and videos. Deep  learning techniques to comprehend images in the same way that humans do. Computer vision  has several applications, such as the following:    Content moderation to automatically remove unsafe or inappropriate content from  image and video archives    Facial recognition to identify faces and recognize attributes like open eyes, glasses, and  facial hair    Image classification to identify brand logos, clothing, safety gear, and other image  details Speech recognition Deep learning models can analyze human speech despite varying speech patterns, pitch, tone,  language, and accent. Virtual assistants such as Amazon Alexa, text-to-speech, and speech-to- text software use speech recognition to do the following tasks:    Assist call center agents and automatically classify calls.    Convert clinical conversations into documentation in real-time.    Accurately subtitle videos and meeting recordings for a wider content reach.    Convert scripts to prompts for intelligent voice assistance. Natural language processing Computers use deep learning algorithms to gather insights and meaning from text data and  documents. This ability to process natural, human-created text has several use cases,  including:    Automated virtual agents and chatbots    Automatic summarization of documents or news articles    Business intelligence analysis of long-form documents, such as emails and forms    Indexing of key phrases that indicate sentiment, such as positive and negative  comments on social media Recommendation engines Applications can use deep learning methods to track user activity and develop personalized  recommendations. They can analyze users' behavior and help them discover new products or  services. For example, "
            },
            {
                "page_number": 3,
                "text": "   Recommend personalized videos and content.    Recommend customized products and services.    Filter search results to highlight relevant content based on user location and behavior Generative AI Generative AI applications can create new content and communicate with end users more  sophisticatedly. They can assist in automating complex workflows, brainstorming ideas, and  intelligent knowledge searches. For example, with generative AI tools like Amazon Q Business  and Amazon Q Developer, users can    Ask natural language questions and get summarized answers from multiple internal  knowledge sources.    Get code suggestions and automatic code scanning and upgrades.    Create new documents, emails, and other marketing content faster. How does deep learning work? Deep learning models are neural networks designed after the human brain. A human brain  contains millions of interconnected biological neurons that work together to learn and process  information. Similarly, artificial neurons are software modules called nodes that use  mathematical calculations to process data. Deep learning neural networks, or artificial neural  networks, comprise many layers of artificial neurons that work together to solve complex  problems. The components of a deep neural network are the following. Input layer An artificial neural network has several nodes that input data into it. These nodes make up the  system's input layer. Hidden layer The input layer processes and passes the data to layers further in the neural network. These  hidden layers process information at different levels, adapting their behavior as they receive  new information. Deep learning networks have hundreds of hidden layers that they can use to  analyze a problem from several different angles. For example, if you were given an image of an unknown animal that you had to classify, you  would compare it with animals you already know. For example, you would look at the shape of  its eyes and ears, size, number of legs, and fur pattern. You would try to identify patterns, such  as the following:    The animal has hooves, so it could be a cow or deer.    The animal has cat eyes, so it could be a wild cat. The hidden layers in deep neural networks work in the same way. If a deep learning algorithm  tries to classify an animal image, each of its hidden layers processes a different animal feature  and tries to categorize it accurately. "
            },
            {
                "page_number": 4,
                "text": "Output layer The output layer consists of the nodes that output the data. Deep learning models that output  'yes' or 'no' answers have only two nodes in the output layer. On the other hand, those that  output a wider range of answers have more nodes. Generative AI has a sophisticated output  layer to generate new data that matches patterns in its training data set. What is the difference between machine learning, deep learning, and generative AI? The terms machine learning, deep learning, and generative AI indicate a progression in neural  network technology. Machine learning Deep learning is a subset of machine learning. Deep learning algorithms emerged to make  traditional machine learning techniques more efficient. Traditional machine learning methods  require significant human effort to train the software. For example, in animal image recognition,  you need to do the following:    Manually label hundreds of thousands of animal images.    Make the machine learning algorithms process those images.    Test those algorithms on a set of unknown images.    Identify why some results are inaccurate.    Improve the dataset by labeling new images to improve result accuracy. This process is called supervised learning. In supervised learning, result accuracy improves only  with a broad and sufficiently varied dataset. For instance, the algorithm might accurately  identify black cats but not white cats because the training dataset had more images of black  cats. In that case, you would need more labeled data of white cat images to train the machine  learning models again. Benefits of deep learning over machine learning "
            },
            {
                "page_number": 5,
                "text": "A deep learning network has the following benefits over traditional machine learning. Efficient processing of unstructured data Machine learning methods find unstructured data, such as text documents, challenging to  process because the training dataset can have infinite variations. On the other hand, deep  learning models can comprehend unstructured data and make general observations without  manual feature extraction. For instance, a neural network can recognize that these two different  input sentences have the same meaning:    Can you tell me how to make the payment?    How do I transfer money? Hidden relationships and pattern discovery A deep learning application can analyze large amounts of data more deeply and reveal new  insights for which it might not have been trained. For example, consider a deep learning model  trained to analyze consumer purchases. The model has data only for the items you have already  purchased. However, the artificial neural network can suggest new items you haven't bought by  comparing your buying patterns to those of similar customers. Unsupervised learning Deep learning models can learn and improve over time based on user behavior. They do not  require large variations of labeled datasets. For example, consider a neural network that  automatically corrects or suggests words by analyzing your typing behavior. Let's assume it was  trained in English and can spell-check English words. However, if you frequently type non- English words, such as danke, the neural network automatically learns and autocorrects these  words too. Volatile data processing Volatile datasets have large variations. One example is loan repayment amounts in a bank. A  deep learning neural network can categorize and sort that data by analyzing financial  transactions and flagging some for fraud detection. Learn more about deep learning vs. machine learning Generative AI Generative AI took the neural networks of machine learning and deep learning to the next level.  While machine learning and deep learning focus on prediction and pattern recognition,  generative AI produces unique outputs based on the patterns it detects. Generative AI  technology is built on transformer architecture that combines several different neural networks  to combine data patterns in unique ways. Deep learning networks first convert text, images, and  other data into mathematical abstractions and then reconvert them into meaningful new  patterns. What are the challenges of deep learning? Challenges in implementing deep learning and generative AI are given below. Large quantities of high-quality data "
            },
            {
                "page_number": 6,
                "text": "When you train them on large amounts of high-quality data, deep learning algorithms give better  results. Outliers or mistakes in your input dataset can significantly affect the deep learning  process. For instance, in our animal image example, the deep learning model might classify an  airplane as a turtle if the dataset accidentally introduces non-animal images. To avoid such inaccuracies, you must clean and process large amounts of data before training  deep learning models. The input data preprocessing requires large amounts of data storage  capacity. Large processing power Deep learning algorithms are compute-intensive and require infrastructure with sufficient  compute capacity to function properly. Otherwise, they take a long time to process results. What are the benefits of generative AI and deep learning in the cloud? Running generative AI and deep learning on cloud infrastructure helps you design, develop, and  train applications faster. Speed You can train generative AI and deep learning models faster by using clusters of GPUs and CPUs  to perform the complex mathematical operations that your neural networks require. You can  then deploy these models to process large amounts of data and produce increasingly relevant  results. Scalability With the wide range of on-demand resources available through the cloud, you can access  virtually unlimited hardware resources to tackle AI deep learning models of any size. Your neural  networks can take advantage of multiple processors to seamlessly and efficiently distribute  workloads across different processor types and quantities. Tools You can access AI and deep learning tools like notebooks, debuggers, profilers, pipelines,  AIOps, and more. You can work with existing generative AI models from within the cloud as a  service without requiring infrastructure to host the model. Teams can start with generative AI  and deep learning applications even with limited knowledge and training. How can AWS help with your generative AI and deep learning requirements? AWS AI and deep learning services harness the power of cloud computing so that you can build  and scale the next wave of AI innovation. Reinvent customer experiences with the most  comprehensive purpose-built services, AI infrastructure, deep learning technology, and  generative AI solutions. For example,    Amazon SageMaker provides fully managed infrastructure, tools, and workflows for  machine learning and deep learning development.    Amazon Bedrock provides a single API to access and utilize various high-performing  foundation models from leading AI companies. "
            },
            {
                "page_number": 7,
                "text": "You can also use AWS AI infrastructure to access comprehensive, secure, and price-performant  computing, storage, and networking to build any AI application. Get started with AI deep  learning on AWS by creating a free AWS account today! "
            }
        ],
        "images": [
            "Image_43",
            "Image_59"
        ]
    },
    {
        "file_name": "DeploymentandScalability1.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Model Inference Explained: Turning AI Models into Real-World Solutions A detailed exploration of model inference, its importance in machine learning, and best practices for optimization. Machine learning has revolutionized the way we approach complex problems in various industries, from image and speech recognition to natural language processing and recommendation systems. But the journey from training a model to deploying it in real-world applications hinges on a critical step: model inference, the process of using a trained model to make predictions or take actions on new, unseen data. Imagine an autonomous car navigating busy city streets, making split-second decisions to avoid accidents and ensure passenger safety, or a voice assistant effortlessly understanding your commands and responding with relevant information. These remarkable capabilities are fueled by machine learning models performing inference – the process of applying learned patterns to new, unseen data. Inference directly impacts the performance, scalability, and reliability of applications, making it a crucial aspect of machine learning in real-world scenarios. An autonomous vehicle making decision slowly is dangerous and a voice assistant understanding commands erroneously can be frustrating. As AI becomes more embedded in our daily lives, from personalized recommendations on streaming platforms to voice-activated assistants, the importance of model inference continues to grow. In this post, we’ll explore the power of model inference, discuss its importance in machine learning, and provide best practices for getting the most out of your models. What is Model Inference? In the world of machine learning, models are the lifeblood of many organizations. Whether it’s predicting customer behavior,detecting anomalies in financial transactions, or classifying images, models have become an integral part of many industries. However, simply having a model is not enough – you also need to be able to use it effectively to drive business value. This is where model inference comes in. Model inference is the process of taking a trained machine learning model and using it to make predictions on new, unseen data. It’s a crucial step that enables organizations to unlock the full potential of their models and turn them into actionable insights. In essence, model inference is the bridge that connects theoretical models to real-world impact. It’s the process of using a trained machine learning model to make predictions, classify data, or generate content. Here’s how it works: 1. Input: New, unseen data (e.g., a video feed, an image, a sentence, sensor readings) is fed into the trained model. 2. Processing: The model applies the patterns and relationships it learned during training to analyze the input. 3. Output: The model generates a prediction, classification, or other form of output based on its analysis. Training vs. Inference Machine learning models go through two primary phases: training and inference. 1. Training: The model’s learning phase, where it identifies patterns and relationships from vast amounts of data. 2. Inference: The application phase, where the model makes predictions or decisions based on new data. Real-World Applications of Model Inference Model inference isn’t just theoretical; it’s actively transforming how industries operate. Here are a few compelling examples of inference in action: Healthcare "
            },
            {
                "page_number": 2,
                "text": "Disease Diagnosis: Machine learning models trained on medical images like X-rays and MRIs can analyze new patient scans. This aids in the identification of tumors or anomalies, potentially leading to faster and more accurate diagnoses than relying solely on human interpretation. Personalized Treatment Plans: By processing a patient’s medical history, genetic information, and other relevant data, models can recommend personalized treatment plans. This approach can improve patient outcomes by tailoring treatments to individual needs and reducing potential side effects. Finance Fraud Detection: Real-time model inference on financial transaction data can instantly detect suspicious activity. This rapid identification helps prevent fraudulent transactions, safeguarding both consumers and businesses from financial losses. Algorithmic Trading: Models can analyze market trends and execute trades at high speeds. This algorithmic trading approach allows for capitalizing on market opportunities and potentially maximizing profits through data-driven decision- making. Autonomous Vehicles Object Recognition: Models continuously analyze data from cameras and sensors to identify pedestrians, vehicles, and other objects on the road. This real-time object recognition is crucial for safe navigation and decision-making in autonomous driving scenarios. Traffic Prediction: By analyzing historical traffic data and real-time information, models can predict traffic patterns and suggest the most efficient routes to drivers. This helps reduce traffic congestion and travel times, benefiting both individual drivers and overall traffic flow. Other Industries Manufacturing: Machine learning models can predict equipment failures by analyzing sensor data, helping prevent costly downtime and optimize maintenance schedules. Retail: Recommendation engines powered by model inference personalize product suggestions for customers based on their browsing and purchase history, enhancing the shopping experience and potentially increasing sales. Agriculture: Models can analyze crop data to determine optimal irrigation and fertilizer use, leading to increased yields and resource conservation. Why Efficient Inference Matters Efficient model inference is the key to unlocking the full potential of AI applications. It is a key differentiator for organizations that want to stay ahead of the curve. With the increasing complexity and scale of data, traditional methods of manual analysis are no longer sufficient. Models need to be able to process large amounts of data quickly and accurately, and model inference enables this. It’s especially crucial for scenarios demanding real-time or near-real-time responses. In healthcare, rapid analysis of medical scans can aid in swift diagnosis. In finance, instantaneous fraud detection can prevent significant losses and in autonomous vehicles, real-time object recognition is paramount for safe navigation. Moreover, model inference is not just about making predictions – it’s also about gaining a deeper understanding of your data and identifying patterns that might not be immediately apparent. By using models to analyze complex datasets, organizations can uncover new insights, identify areas for improvement, and make data-driven decisions. The speed and accuracy of model inference directly impacts user experiences, business outcomes, and even life-saving decisions. As AI continues to permeate various aspects of our lives, the importance of optimizing model inference cannot be overstated. Types of Model Inference "
            },
            {
                "page_number": 3,
                "text": "Model inference is not a one-size-fits-all solution. Depending on your specific use case, data distribution, and performance requirements, you may need to choose from different types of model inference. There are several types of model inference, including: Batch Inference: Batch inference, also known as offline inference, involves processing large datasets all at once. This approach is commonly used for: Image classification Natural language processing Recommendation systems Batch inference typically requires less computational resources than online inference but can be slower and more memory-intensive due to the need to process entire datasets. Online Inference: Online inference refers to the process of making predictions in real-time as new data arrives. This is particularly useful for applications that require immediate feedback, such as: Personalized product recommendations in e-commerce Sentiment analysis for customer feedback Real-time fraud detection Online inference typically involves streaming data into a model and generating predictions on- the-fly. This approach can be challenging due to the need for high-throughput processing and handling varying data distributions. Streaming Inference: Processing continuous streams of data, often used for applications like video analytics and natural language processing Overcoming the Hurdles: Challenges in Model Inference While model inference empowers AI applications, it’s not without its hurdles. As models grow in complexity and the demand for real-time insights surges, developers and organizations face a range of critical challenges: The Need for Speed: Scalability and Throughput: Handling a flood of inference requests without sacrificing performance is a constant balancing act. Scaling up infrastructure to meet demand while maintaining low latency is essential. Latency and Response Time: In time-sensitive applications like fraud detection or autonomous vehicles, split-second decision-making requires lightning-fast inference. Meeting strict latency requirements is non-negotiable. Taming Model Complexity: Model Size and Resources: Large, intricate models often demand significant computational power and memory, straining resources and potentially leading to bottlenecks. Optimization Trade-offs: Striking the right balance between model accuracy and computational efficiency is a delicate dance. Techniques like model compression can help, but require careful consideration. Navigating Hardware and Software: Hardware Diversity: Optimizing inference for diverse hardware platforms (CPUs, GPUs, TPUs, etc.) necessitates careful tailoring and expertise. Software Selection: Choosing the right inference framework and tools can significantly impact performance. Compatibility and ease of use are also key factors. Model Management Maze: Deployment and Updates: Seamlessly deploying, updating, and maintaining a multitude of models across various frameworks and environments can be a logistical nightmare. Consistency and Accuracy: Ensuring models perform consistently and accurately across different deployments and over time is a critical concern. Safeguarding Sensitive Data: "
            },
            {
                "page_number": 4,
                "text": "Security and Privacy: Protecting data used for inference is paramount, especially in industries handling sensitive information like healthcare or finance. Robust security measures are essential to prevent unauthorized access and potential breaches. These challenges underscore the need for robust, scalable, and efficient model inference solutions. Fortunately, powerful tools like Triton Inference Server have emerged to address these complexities, paving the way for a smoother and more effective deployment of AI models in the real world. Triton offers a robust, scalable, and efficient platform for deploying and managing diverse AI models in various environments. You can learn more about it here. Understanding Model Inference: From Training to Action Now that we’ve established what is model inference and its significance, let’s dive deeper into the mechanics behind it. Think of the inference pipeline as a well-orchestrated sequence of steps that transform raw input data into insightful predictions or classifications. Here’s a breakdown of this journey: 1. DataPreprocessing: Preparing the Input: Before your model can work its magic, the incoming data needs to be prepared. This involves tasks such as cleaning, formatting, scaling, or converting the data into a suitable format for your model’s consumption. Proper preprocessing ensures that the data is in the right shape and quality for accurate predictions. 2. ModelLoading: Loading the Trained Model: Once your data is prepped, the trained model is loaded into memory. This step is crucial for efficient inference, as the model must be readily available to process incoming data. Platforms like Triton Inference Server excel at this stage, seamlessly managing models from various frameworks and ensuring they are optimally loaded for performance. 3. InferenceExecution: Generating Predictions: This is where the core action happens. The loaded model takes the preprocessed data and performs the necessary computations, applying the patterns and relationships it learned during training. The outcome is the model’s prediction or classification. This step involves the heavy lifting of the inference process, where the model’s intelligence is applied to new data. 4. Postprocessing: Refining the Output: In some cases, the model’s output might require further refinement or interpretation. Postprocessing steps can include formatting the results, applying thresholds, or translating numerical values into meaningful labels. This final step ensures that the output is in a usable form for decision-making or further analysis. By understanding these steps, we gain insight into how raw data is transformed into actionable intelligence through model inference. Each step is critical in ensuring that the inference process is efficient, accurate, and scalable, allowing machine learning models to make a tangible impact in real-world applications. Where Does Inference Reside? Deployment Options Unveiled The location where you deploy your model for inference plays a crucial role in its performance and accessibility. Let’s explore some common deployment strategies On-Premises and Hybrid Inference: Inference can be performed on-premises or in a hybrid manner, combining both on- premises and cloud resources. Solutions such as VMware Private AI offer these capabilities. The benefits include: Control and Security: Maintain full control over your data and infrastructure, ensuring compliance with security policies. Customization: Tailor the infrastructure to meet specific performance and operational requirements. "
            },
            {
                "page_number": 5,
                "text": "Hybrid Flexibility: Combine the benefits of on-premises and cloud-based deployments to optimize performance, cost, and resource utilization. Cloud-Based Inference: In this scenario, your model resides in the cloud, accessible via APIs. Cloud platforms like AWS, Azure, or Google Cloud provide the infrastructure and scalability needed to handle high volumes of inference requests. The advantages are Scalability: Effortlessly scale your inference capacity to match demand. Managed Infrastructure: Focus on your ML tasks while the cloud provider handles server management. Global Accessibility: Serve predictions to users worldwide. Edge Inference: Sometimes, it’s advantageous to run inference directly on devices closer to where data is generated (e.g., smartphones, IoT devices). This approach is known as edge inference. The benefits are Low Latency: Reduced response time due to proximity to data. Offline Capability: Inference can continue even without internet connectivity. Privacy: Sensitive data can be processed locally without leaving the device. Choosing the right deployment strategy depends on your specific use case and requirements. Consider factors such as privacy, confidentiality, scalability, latency, security, and infrastructure management when deciding where to deploy your model for inference. Inference Servers To efficiently manage and optimize model inference across these deployment options, specialized platforms known as inference servers can be employed. Inference servers are specialized platforms designed to handle the complexities of model inference, providing optimized performance, scalability, and manageability. A Inference Server handles incoming inference requests, processes these requests using pre-trained models, and returns predictions. Inference servers optimize resource utilization, support scalability, and ensure reliable model deployment in production environments. Benefits of Using an Inference Server Optimized Performance: Inference servers often include features like dynamic batching, which combines multiple inference requests to maximize GPU or CPU utilization and improve throughput. Integration with hardware acceleration tools, such as NVIDIA TensorRT, to enhance performance. Scalability: Easily scalable by adding more instances to handle increased inference loads. Load balancing capabilities to distribute the inference workload evenly across multiple server instances. Multi-Framework Support: Supports a variety of machine learning frameworks (e.g., TensorFlow, PyTorch, ONNX), providing flexibility in deploying models from different environments. Model Management: Advanced model management features, including model versioning, dynamic loading, and configuration management, to streamline the deployment and updating of models. Monitoring and Logging: Provides detailed metrics and logging for monitoring the performance and health of deployed models. Enables setting up alerts and tracking system performance to proactively address potential issues. "
            },
            {
                "page_number": 6,
                "text": "While cloud platforms offer managed services that abstract away many complexities, inference servers are particularly valuable in environments where control over infrastructure and customization are paramount. They allow organizations to deploy AI solutions effectively, ensuring high performance and reliability. There are several leading inference servers in the market, each with its strengths and specific use cases. Here’s an overview of the most prominent ones: 1. NVIDIA Triton Inference Server 2. TensorFlow Serving 3. TorchServe 4. ONNX Runtime 5. AWS SageMaker Inference Leading Inference Servers: A Closer Look Let’s delve into a few of the most popular inference servers on the market: NVIDIA Triton Inference Server: NVIDIA Triton Inference Server is developed by NVIDIA and designed to maximize the performance and efficiency of deploying AI models at scale, leveraging NVIDIA GPUs.It provides exceptional hardware acceleration for NVIDIA GPUs, an extensive model repository, broad framework support, and a focus on high-performance inference. It is ideal for applications requiring top-tier GPU performance, real-time inference, and the ability to serve a diverse array of models. KServe: KServe, formerly known as KFServing, is an open-source model inference platform designed to provide serverless inference capabilities on Kubernetes. It is developed as part of the Kubeflow project and aims to simplify the deployment and management of machine learning models in production. It provides seamless integration with Kubernetes, support for multiple frameworks (e.g., TensorFlow, PyTorch, XGBoost, SKLearn), autoscaling, and canary rollouts for model updates. It is ideal for organizations using Kubernetes for their infrastructure, those seeking a serverless approach to model deployment, and environments that require flexible and scalable inference solutions. TensorFlow Serving: TensorFlow Serving is developed by Google and is a flexible, high-performance serving system for machine learning models, designed for production environments.It provides seamless integration with the TensorFlow ecosystem, strong community support, and mature model management capabilities.It is ideal for organizations that are heavily invested in TensorFlow, particularly those seeking a reliable and well-supported inference solution. TorchServe: TorchServe is an open-source model serving framework for PyTorch, developed by AWS and Facebook, offering easy-to-use APIs and scalable serving capabilities. It is designed specifically for PyTorch models, has a user-friendly interface, and a growing community. It is ideal for PyTorch users seeking a straightforward and efficient way to deploy their models. ONNX Runtime: ONNX Runtime is a cross-platform, high-performance scoring engine for Open Neural Network Exchange (ONNX) models, developed by Microsoft and optimized for various hardware. It provides extensive framework support (including TensorFlow, PyTorch, Scikit-learn), portable across various platforms, and is optimized for diverse hardware (CPUs, GPUs, etc.). It is ideal for applications that require flexibility in terms of model frameworks and deployment environments. AWS SageMaker Inference: AWS SageMaker Inference is developed by Amazon Web Services (AWS) and is a fully managed service that makes it easy to deploy machine learning models for inference at scale. It provides deep integration with the AWS ecosystem, easy scalability, and serverless options for cost optimization. It is ideal for organizations that "
            },
            {
                "page_number": 7,
                "text": "are already using AWS services, those seeking managed infrastructure, or those needing the flexibility of serverless architectures. Choosing the Right Inference Server Selecting the best inference server depends on your specific requirements: Framework(s): Ensure the server supports the frameworks you use for model development. Hardware: Consider the hardware you have available (CPUs, GPUs, TPUs) and the server’s optimization capabilities for those platforms. Scalability Needs: Estimate your inference load and choose a server that can scale accordingly. Ease of Use: Evaluate the server’s interface, documentation, and community support to determine how easy it is to learn and use. The Future of Model Inference: Where We’re Heading Model inference is not a static field; it’s constantly evolving to meet the growing demands of AI applications. Here’s a glimpse into some of the most exciting trends shaping the future of model inference: Inference at the Edge: Empowering Devices Inference at the edge allows for near-instantaneous predictions and responses, crucial for applications like autonomous vehicles, robotics, and real-time analytics. Less data is transmitted to the cloud, leading to cost savings and improved reliability in areas with limited connectivity. Sensitive data can be processed locally, minimizing the risk of exposure during transmission. Decentralized Inference: As devices become more powerful, there is a shift towards performing inference directly on edge devices such as smartphones, IoT devices, and autonomous vehicles. This reduces latency and bandwidth usage while enhancing privacy by keeping data local. Enhanced Hardware Capabilities: Innovations in edge hardware, like AI accelerators and specialized inference chips, are making it possible to run complex models efficiently on edge devices.Edge inference brings the power of AI directly to devices like smartphones, wearables, and industrial sensors. Reduced Latency: By processing data locally, edge inference minimizes the delay in receiving predictions, making it ideal for real-time applications like autonomous vehicles and robotics. Enhanced Privacy: Sensitive data can be analyzed on the device itself, reducing the need to transmit it to the cloud and minimizing potential privacy risks. AI Accelerators Specialized chips (e.g., TPUs, ASICs) are being developed to accelerate inference tasks, boosting performance and efficiency. Specialized Hardware: AI accelerators, such as NVIDIA’s TensorRT, Google’s Coral TPU, and custom AI chips, are designed to accelerate inference workloads. These accelerators optimize performance and energy efficiency, making it feasible to deploy high-complexity models in real-time applications. Integration with Inference Servers: Inference servers are increasingly integrating with AI accelerators to leverage their capabilities, providing seamless and optimized inference experiences. AI accelerators are finding applications in diverse fields, from speeding up natural language processing in chatbots to accelerating image recognition in surveillance systems. Model Compression and Optimization "
            },
            {
                "page_number": 8,
                "text": "Techniques like quantization and pruning are shrinking model sizes, enabling faster inference on resource-constrained devices. Efficient Models: Techniques such as model pruning, quantization, and knowledge distillation are being used to reduce model size and computational requirements, enabling faster and more efficient inference. AutoML for Inference: Automated Machine Learning (AutoML) tools are evolving to include model optimization for inference, allowing developers to automatically generate and deploy highly efficient models. These optimization techniques are crucial for running AI models on mobile devices, embedded systems, and other resource- constrained environments. Serverless Inference Architectures: Scaling on Demand Serverless inference leverages cloud-based functions to execute model inference tasks on a per-request basis. This eliminates the need to manage infrastructure, allowing for Automatic Scaling: Effortlessly handles fluctuating inference workloads without manual intervention. Cost Efficiency: Pay only for the actual compute time used, reducing operational costs. Simplified Deployment: Streamlines the process of deploying and updating models without complex infrastructure management. Serverless inference is becoming popular for applications with unpredictable workloads, such as chatbots or e-commerce recommendation engines, where demand can fluctuate significantly. Explainable AI (XAI): Building Trust and Transparency As AI models become more integrated into decision-making processes, the need for transparency and understanding grows. Explainable AI (XAI) techniques aim to shed light on how models arrive at their predictions, enabling users to: Understand Reasoning: XAI methods can provide insights into the factors that influence a model’s decision, helping build trust and identify potential biases. Improve Debugging: By understanding why a model made a particular prediction, developers can more effectively debug and refine their models. The Road Ahead The future of model inference is bright, with ongoing research and development aimed at making AI more accessible, efficient, and powerful. As these trends mature, we can expect even faster, more efficient, and privacy-conscious AI applications that seamlessly integrate into our daily lives. "
            }
        ],
        "images": []
    },
    {
        "file_name": "DeploymentandScalability2.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Reducing AI Model Latency for Real-Time AI Inference Learn techniques to minimize latency in AI models for efficient real-time inference, enhancing performance and responsiveness. Post-Training Quantization Techniques Post-training quantization is a pivotal technique in optimizing AI models, particularly in reducing AI model latency and enhancing inference speed. This method allows for the conversion of floating-point weights and activations to lower precision formats, which significantly decreases memory usage while maintaining model accuracy. Types of Post-Training Quantization There are two primary types of post-training quantization available in Intel® Neural Compressor: 1. Post-Training Static Quantization: This method quantizes both weights and activations, leading to a more significant reduction in model size and improved inference speed. It requires calibration datasets to determine the optimal scaling factors for quantization. 2. Post-Training Dynamic Quantization: Recommended for its ease of use, this approach quantizes only the weights at conversion time, while activations remain in floating-point format. This results in reduced memory usage and faster computation without the need for additional calibration datasets. Although the speed of dynamic-quantized operations is slightly less than that of static-quantized computations, it still offers substantial benefits in terms of latency reduction. Benefits of Quantization Quantization techniques provide several advantages:  Reduced Memory Footprint: By using lower precision (e.g., 8-bit integers), models occupy less memory, making them more efficient for deployment on resource-constrained devices.  Increased Inference Speed: Lower precision calculations can be executed faster, leading to quicker response times in real-time applications.  Robustness: Neural networks (NNs) are generally robust to quantization, allowing them to maintain accuracy even when significant reductions in precision are applied. Implementation Example To illustrate the implementation of post-training quantization, consider the following code snippet that utilizes the Intel® Neural Compressor API: from neural_compressor import Quantization # Load your trained model model = load_model('your_model.pth') # Apply post-training dynamic quantization quantized_model = Quantization(model, quantization_type='dynamic') # Save the quantized model save_model(quantized_model, 'quantized_model.pth') "
            },
            {
                "page_number": 2,
                "text": "This simple code demonstrates how easily a fine-tuned model can be quantized by integrating a few lines of code into the existing workflow. Conclusion As the demand for large language models (LLMs) continues to grow, the importance of efficient quantization methods becomes increasingly critical for their scalable and cost-effective deployment in real-world applications. By leveraging post-training quantization, developers can ensure that their models are optimized for performance without sacrificing accuracy. Optimizing Inference with INT8 Precision To effectively reduce AI model latency, leveraging INT8 precision is a critical strategy in optimizing inference performance. This approach utilizes quantization techniques that convert model weights and activations from higher precision formats to 8-bit integers, significantly enhancing computational efficiency while maintaining acceptable accuracy levels. Understanding INT8 Quantization Quantization is the process of mapping a large set of input values to output values in a smaller set, which is particularly useful in AI models where the size and complexity can lead to increased latency. By employing post-training integer-bit quantization (PTQ), models can achieve a W8A8 configuration, which is widely recognized for its effectiveness in real-time applications.  Benefits of INT8 Quantization:  Reduced Memory Footprint: Lower precision formats require less memory, allowing for larger models to fit into memory-constrained environments.  Increased Throughput: INT8 operations can be executed faster on modern hardware, leading to improved inference times.  Energy Efficiency: Using lower precision reduces the energy consumption of computations, which is crucial for deploying models in edge devices. Hardware Support for INT8 Recent advancements in GPU architectures have made it easier to implement INT8 quantization. For instance, NVIDIA's latest GPUs, such as the Blackwell architecture, support a range of precision formats including INT8, enabling peak performance metrics of up to 9 PFLOPS under FP4 quantization. This flexibility allows developers to optimize their models based on the specific requirements of their applications. Best Practices for Implementing INT8 When transitioning to INT8 precision, consider the following best practices:  Delayed Scaling: This technique involves selecting scaling factors based on the maximum absolute values observed in previous iterations, ensuring that the model maintains its performance while utilizing INT8 computations.  Selective Weight Bypassing: Recent research suggests that not all weights contribute equally during inference. By selectively bypassing less important weights, models can further reduce inference costs without sacrificing accuracy.  Memory Optimization Techniques: Implementing strategies such as QST (Zhang et al., 2024g) can help manage memory usage effectively by quantizing model weights and optimizing the storage of gradients and intermediate activations. "
            },
            {
                "page_number": 3,
                "text": "Conclusion Incorporating INT8 precision into AI model inference is a powerful method for reducing latency and improving overall performance. By understanding the benefits and best practices associated with quantization, developers can enhance their models' efficiency and scalability in real-world applications. Adaptive Weight Quantization for Generative Models Adaptive weight quantization is a pivotal technique in enhancing the efficiency of generative models, particularly in reducing AI model latency. This section delves into the intricacies of post-training integer-bit quantization (PTQ), focusing on the widely adopted 8-bit weights and 8-bit activations (W8A8) configuration. By leveraging this quantization method, we can achieve a balance between model performance and resource utilization, which is essential for real-time AI inference. Context-Dependent Weight Importance In generative AI models, the significance of weights can vary significantly based on the context. To optimize inference, it is crucial to implement a weight quantization strategy that adapts to these variations. The proposed design suggests a coarse granularity approach, where weights associated with similar contextual importance are grouped together. This allows for a streamlined process of fetching model weights from memory, enhancing both speed and efficiency.  Chunked Quantization Configuration: By organizing weights into chunks, each with a uniform quantization configuration, we simplify the runtime evaluation of weight importance. This method not only accelerates the loading of model weights from DRAM but also aligns with the capabilities of modern GPUs and AI accelerators that support variable-precision arithmetic. Efficient Memory Utilization To address the challenges posed by the large size of generative AI models, an innovative solution involves storing only the full-precision model in memory. During inference, reduced-precision weights can be generated on-the-fly through quantization format conversion. This approach minimizes memory overhead while maintaining the flexibility to adapt to different quantization formats as needed.  CXL Memory Devices: The integration of CXL memory devices plays a crucial role in this process. These devices can dynamically convert full-precision weights into the desired quantization format, thereby optimizing memory usage and enhancing inference speed. The controller chip within the CXL memory facilitates this conversion seamlessly, ensuring that inference computing devices can specify the required quantization format at runtime. Performance Scaling with Variable-Precision Arithmetic Recognizing the importance of quantization, the chip design industry has made significant strides in supporting variable-precision arithmetic. For instance, the latest Nvidia GPU architectures, such as Blackwell, offer extensive support for various precision formats, including FP64, FP32, FP16, FP8, FP6, FP4, and INT8. This capability allows for a scalable "
            },
            {
                "page_number": 4,
                "text": "approach to AI inference, where computational throughput and energy efficiency can be adjusted based on the data quantization precision.  Dynamic Trade-offs: By dynamically adjusting the quantization precision, algorithms can navigate the trade-off space between inference quality and implementation cost. This flexibility is particularly beneficial for generative AI models, which continue to grow in size and complexity, necessitating innovative solutions to enhance their training and inference efficiency. In conclusion, adaptive weight quantization represents a critical advancement in the deployment of generative models, enabling them to operate efficiently in real-world applications while maintaining high performance. "
            }
        ],
        "images": []
    },
    {
        "file_name": "DeploymentandScalability3.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Efficient Deployment of Large-Scale Transformer Models: Strategies for Scalable and Low-Latency Inference Scaling Transformer-based models to over 100 billion parameters has led to groundbreaking results in natural language processing. These large language models excel in various applications, but deploying them efficiently poses challenges due to the sequential nature of generative inference, where each token’s computation relies on the preceding tokens. This necessitates meticulous parallel layouts and memory optimizations. The study highlights crucial engineering principles for efficiently serving large-scale Transformer models in diverse production settings, ensuring scalability and low-latency inference. Google researchers investigate efficient generative inference for large Transformer models, focusing on tight latency targets and long sequence lengths. They developed an analytical model to optimize multi-dimensional partitioning techniques for TPU v4 slices and implemented low-level optimizations. This achieved superior latency and Model FLOPS Utilization (MFU) tradeoffs for 500B+ parameter models, outperforming FasterTransformer benchmarks. Using multi-query attention, they scaled context lengths up to 32× larger. Their PaLM 540B model attained 29ms latency per token with int8 quantization and a 76% MFU, supporting a 2048-token context length, highlighting practical applications in chatbots and high-throughput offline inference. Prior works on efficient partitioning for training large models include NeMo Megatron, GSPMD, and Alpa, which utilize tensor and pipeline parallelism with memory optimizations. FasterTransformer sets benchmarks for multi-GPU multi-node inference, while DeepSpeed Inference uses ZeRO offload to leverage CPU and NVMe memory. EffectiveTransformer reduces padding by packing sequences. Unlike these, this study develops partitioning strategies based on analytical tradeoffs. For improving inference efficiency, approaches include efficient attention layers, distillation, pruning, and quantization. The study incorporates model quantization for inference speedups and suggests its techniques could complement other compression methods. Scaling model sizes improves capabilities but increases latency, throughput, and MFU inference costs. Key metrics include latency (prefill and decode times), throughput (tokens processed/generated per second), and MFU (observed vs. theoretical throughput). Larger models face memory and compute challenges, with small batch sizes dominated by weight loading "
            },
            {
                "page_number": 2,
                "text": "times and larger ones by KV cache. Efficient inference requires balancing low latency and high throughput through strategies like 1D/2D weight-stationary and weight-gathered partitioning. Attention mechanisms impact memory use, with multi-query attention reducing KV cache size but adding communication costs. In a study of PaLM models, techniques like multi-query attention and parallel attention/feedforward layers were evaluated using JAX and XLA on TPU v4 chips. For the PaLM 540B model, padding attention heads enhanced partitioning efficiency. Different partitioning strategies were tested: 1D and 2D weight-stationary layouts and weight-gathered layouts, with 2D performing better at higher chip counts. Multi-query attention allowed larger context lengths with less memory use than multihead. The study demonstrated that optimizing partitioning layouts based on batch size and phase (prefill vs. generation) is crucial for balancing efficiency and latency. 🚨🚨FREE AI WEBINAR: 'Fast-Track Your LLM Apps with deepset & Haystack'(Promoted) Large Transformer models are revolutionizing various domains, but democratizing their access requires significant advancements. This study explores scaling Transformer inference workloads and suggests practical partitioning methods to meet stringent latency demands, especially for 500B+ parameter models. Optimal latencies were achieved by scaling inference across 64+ chips. Multiquery attention with effective partitioning reduces memory costs for long-context inference. Although scaling improves performance, FLOP count and communication volume remain limiting factors. Techniques like sparse architectures and adaptive computation, which reduce FLOPs per token and chip-to-chip communication, promise further cost and latency improvements. "
            }
        ],
        "images": [
            "Image_10"
        ]
    },
    {
        "file_name": "DeploymentandScalability4.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Build an AI Inference Service That Scales Like a Pro If your AI-powered application is gaining traction, users are flooding in, and everything seems to be going great—until your system starts to buckle under the pressure. Latency spikes, costs spiral, and suddenly your once-promising service feels unsustainable. Sound familiar? Whether you’re a developer, startup founder, or tech enthusiast, scaling AI inference services to meet unpredictable user demand is a common challenge. But what if there were a way to handle these fluctuations seamlessly without breaking the bank or sacrificing performance? This guide by Trelis Research is designed to help you tackle that exact problem explaining how to build an AI inference service that not only survives under pressure but thrives. From setting up a simple APIendpoint to implementing advanced autoscaling techniques, the guide breaks down the process step by step. Whether you’re working with off-the-shelf models or fine-tuning custom ones, you’ll discover practical strategies to balance cost, performance, and flexibility. By the end, you’ll have a clear roadmap to create a scalable, efficient system that adapts to your users’ needs—no matter how unpredictable they may be. Choosing the Right Inference Approach TL;DR Key Takeaways : Choose the right inference approach based on workload and budget: fixed GPUrentals for stability, autoscaling for flexibility, or shared services for cost efficiency. Balance cost and performance by optimizing GPU utilization, batch sizes, and throughput to meet workload demands effectively. Set up a scalable foundation using single GPU API endpoints, Docker containers, and proper environment configurations for deployment flexibility. Implement autoscaling using third-party platforms or custom solutions to dynamically adjust resources based on token generation speed (TPS) and demand fluctuations. Test and optimize performance through load testing, GPU selection, and batch size adjustments, while integrating advanced features like custom Docker images and fine-tuned models for complex use cases. "
            },
            {
                "page_number": 2,
                "text": "Creating a scalable AI inference service involves more than deploying a machine learning model. It requires a system capable of delivering accurate, real-time predictions while efficiently managing computational resources. Selecting the appropriate inference approach is the foundation of building a scalable AI service. Your choice should align with your workload characteristics, budget, and the level of customization needed. Below are three primary methods to consider:  Manual Approach: Renting fixed GPUs provides consistent capacity, making it ideal for predictable workloads. However, this method lacks flexibility during demand fluctuations, potentially leading to underutilized resources or service bottlenecks.  Autoscaling Approach: Dynamically scaling GPU usage based on demand ensures you only pay for the resources you use. While this approach offers flexibility, it typically incurs higher hourly GPU rental costs.  Shared Services: Using shared GPU infrastructure minimizes costs by maximizing resource utilization. However, this option may not support custom or fine-tuned models, limiting its applicability for specialized use cases. Each method has trade-offs. Fixed rentals provide stability, autoscaling offers adaptability, and shared services prioritize cost efficiency. Evaluate your workload’s predictability and customization needs to determine the best fit. Balancing Cost and Performance Achieving a balance between cost and performance is critical when deploying AI inference services. The cost per token, or the cost of processing a single unit of data, is influenced by several factors:  GPU Utilization: Higher utilization rates reduce costs but may increase latency if resources are stretched too thin.  Batch Size: Larger batches improve throughput but require more memory, which could limit scalability.  Throughput: The number of tokens processed per second directly impacts resource efficiency and overall system responsiveness. Shared services often achieve the lowest costs due to their high utilization rates, but they may not meet the needs of custom deployments. Autoscaling, while flexible, "
            },
            {
                "page_number": 3,
                "text": "comes with higher operational costs. Striking the right balance involves careful tuning of these variables to optimize both performance and cost. Steps to Build and Optimize Your Inference Service To create a robust and scalable AI inference service, follow these key steps: 1. Set Up a Single GPU API Endpoint Begin with a single GPU endpoint to process requests. This setup is ideal for testing and small-scale deployments. Use FastAPI or similar frameworks to create a responsive and efficient API. 2. Deploy Models Using Docker Containers Docker containers simplify deployment by encapsulating your model and its dependencies. Use pre-built Docker images for common models or create custom images for fine-tuned or unsupported models. Configure container parameters, such as disk size and environment variables, to ensure compatibility. 3. Implement Autoscaling for Dynamic Resource Allocation Autoscaling is essential for handling fluctuating workloads. You can choose between third-party platforms like RunPod or Fireworks, which offer pre-built solutions, or develop a custom autoscaling system. A custom solution involves monitoring token generation speed (TPS) and using APIs to rent or release GPUs based on predefined thresholds. 4. Test and Optimize Performance Conduct load tests to establish performance benchmarks. Focus on key metrics such as: – Token Generation Speed (TPS): Determine the maximum TPS your system can handle without compromising latency or accuracy. – GPU Preferences: Experiment with different GPU types and configurations to find the optimal balance between cost and performance. – Batch Sizes: Adjust batch sizes to maximize throughput while staying within memory constraints. 5. Streamline API Integration and Load Balancing Wrap your autoscaling service into a single API endpoint to simplify user interactions. Implement load balancing to distribute requests across available GPUs, making sure consistent performance even during high traffic periods. "
            },
            {
                "page_number": 4,
                "text": "Enhancing Functionality with Advanced Features For more complex use cases, consider integrating advanced features to improve the flexibility and functionality of your AI inference service:  Custom Docker Images: Build tailored images for unsupported or fine-tuned models, allowing greater control over model deployment.  Fine-Tuning Models: Adapt pre-trained models to meet specific requirements, enhancing their relevance and accuracy for your application.  Scaling Cooldown Management: Configure cooldown periods and monitoring intervals to maintain system stability during scaling events, avoiding unnecessary resource churn. These enhancements allow your service to cater to diverse user needs while maintaining efficiency and reliability. Best Practices for a Scalable AI Inference Service To ensure the success of your AI inference service, adhere to the following best practices:  Use shared services for cost-sensitive applications that use standard models.  Opt for autoscaling or fixed GPU rentals when deploying custom or fine-tuned models that require dedicated resources.  Carefully configure scaling parameters, such as TPS thresholds and cooldown periods, to balance cost and performance effectively.  Regularly monitor and optimize key metrics, including GPU utilization, batch sizes, and throughput, to maintain peak efficiency. By tailoring your approach to your specific workload and budget, you can build a robust, scalable AI inference service that adapts seamlessly to user demand "
            }
        ],
        "images": []
    },
    {
        "file_name": "DeploymentandScalability5.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Improving LLM Inference Speeds on CPUs with Model Quantization Discover how to significantly improve inference latency on CPUs using quantization techniques for bf16, int8, and int4 precisions One of the most significant challenges the AI space faces is the need for computing resources to host large-scale production-grade LLM-based applications. At scale, LLM applications require redundancy, scalability, and reliability, which have historically been only possible on general computing platforms like CPUs. Still, the prevailing narrative today is that CPUs cannot handle LLM inference at latencies comparable with high-end GPUs. One open-source tool in the ecosystem that can help address inference latency challenges on CPUs is the Intel® Extension for PyTorch* (IPEX), which provides up-to-date feature optimizations for an extra performance boost on Intel hardware. IPEX delivers a variety of easy-to-implement optimizations that make use of hardware-level instructions. This tutorial will dive into the theory of model compression and the out-of-the-box model compression techniques IPEX provides. These compression techniques directly impact LLM inference performance on general computing platforms, like Intel 4th and 5th-generation CPUs. Inference Latency in Application Development Second only to application safety and security, inference latency is one of the most critical parameters of an AI application in production. Regarding LLM-based applications, latency or throughput is often measured in tokens/second. As illustrated in the simplified inference processing sequence below, tokens are processed by the language model and then de-tokenized into natural language. GIF 1. of inference processing sequence — Image by Author Interpreting inference this way can sometimes lead us astray because we analyze this component of AI applications in abstraction of the traditional production software paradigm. Yes, AI apps have their nuances, but at the end of the day, we are still talking about transactions per unit of time. If we start to think about inference as a transaction, like any other, from an application design point of view, the problem becomes less complex. For example, let’s say we have a chat application that has the following requirements:  Average of 300 user sessions per hour  Average of 5 transactions (LLM inference requests) per user per session  Average 100 tokens generated per transaction "
            },
            {
                "page_number": 2,
                "text": " Each session has an average of 10,000ms (10s) overhead for user authentication, guardrailing, network latency, and pre/post-processing.  Users take an average of 30,000ms (30s) to respond when actively engaged with the chatbot.  The average total active session time goal is 3 minutes or less. Below, you can see that with some simple napkin math, we can get some approximate calculations for the required latency of our LLM inference engine. Figure 1. A simple equation to calculate the required transaction and token latency based on various application requirements. — Image by Author Achieving required latency thresholds in production is a challenge, especially if you need to do it without incurring additional compute infrastructure costs. In the remainder of this article, we will explore one way that we can significantly improve inference latency through model compression. Model Compression Model compression is a loaded term because it addresses a variety of techniques, such as model quantization, distillation, pruning, and more. At their core, the chief aim of these techniques is to reduce the computational complexity of neural networks. GIF 2. Illustration of inference processing sequence — Image by Author The method we will focus on today is model quantization, which involves reducing the byte precision of the weights and, at times, the activations, reducing the computational load of matrix operations and the memory burden of moving around larger, higher precision values. The figure below illustrates the process of quantifying fp32 weights to int8. "
            },
            {
                "page_number": 3,
                "text": "Fig 2. Visual representation of model quantization going from full precision at FP32 down to quarter precision at INT8, theoretically reducing the model complexity by a factor of 4. — Image by Author It is worth mentioning that the reduction of complexity by a factor of 4 that results from quantizing from fp32 (full precision) to int8 (quarter precision) does not result in a 4x latency reduction during inference because inference latency involves more factors beyond just model-centric properties. Like with many things, there is no one-size-fits-all approach, and in this article, we will explore three of my favorite techniques for quantizing models using IPEX: Inference at bf16 or fp32 This technique quantizes the weights in the neural network down to a user defined precision. This technique is ideal for smaller models, like the <1B LLMs of the world. Fig 3. Simple illustration of bf16/fp32, showing FP32 weights in orange and half-precision quantized bf16 weights in green. — Image by Author The implementation is quite straightforward: using hugging face transformers, a model can be loaded into memory and optimized using the IPEX llm-specific optimization function "
            },
            {
                "page_number": 4,
                "text": "ipex.llm.optimize(model, dtype=dtype) by setting dtype = torch.bfloat16, we can activate the half-prevision inference capability, which improves the inference latency over full-precision (fp32) and stock. import sys import os import torch import intel_extension_for_pytorch as ipex from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline # PART 1: Model and tokenizer loading using transformers tokenizer = AutoTokenizer.from_pretrained('Intel/neural-chat-7b-v3-3') model = AutoModelForCausalLM.from_pretrained('Intel/neural-chat-7b-v3-3') # PART 2: Use IPEX to optimize the model #dtype = torch.float # use for full precision FP32 dtype = torch.bfloat16 # use for half precision inference model = ipex.llm.optimize(model, dtype=dtype) # PART 3: Create a hugging face inference pipeline and generate results pipe = pipeline('text-generation', model=model, tokenizer=tokenizer) st = time.time() results = pipe('A fisherman at sea...', max_length=250) end = time.time() generation_latency = end-st print('generation latency: ', generation_latency) print(results[0]['generated_text']) Of the three compression techniques we will explore, this is the easiest to implement (measured by unique lines of code) and offers the smallest net improvement over a non-quantized baseline. SmoothQuant (int8) This technique addresses the core challenges of quantizing LLMs, which include handling large-magnitude outliers in activation channels across all layers and tokens, a common issue that traditional quantization techniques struggle to manage effectively. This technique employs a joint mathematical transformation on both weights and activations within the model. The transformation strategically reduces the disparity between outlier and non-outlier values for activations, albeit at the cost of increasing this ratio for weights. This adjustment renders the Transformer layers “quantization-friendly,” enabling the successful application of int8 quantization without degrading model quality. "
            },
            {
                "page_number": 5,
                "text": "Fig 4. Simple illustration of SmoothQuant showing weights as circles and activations as triangles. The diagram depicts the two main steps: (1) the application of scaler for smoothing and (2) the quantization to int8 — Image by Author Below, you’ll find a simple SmoothQuant implementation — omitting the code for creating the DataLoader, which is a common and well-documented PyTorch principle. SmoothQuant is an accuracy-aware post-training quantization recipe, meaning that by providing a calibration dataset and model you will be able to provide a baseline and limit the language modeling degradation. The calibration model generates a quantization configuration, which is then passed to ipex.llm.optimize() along with the SmoothQuant mapping. Upon execution, the SmoothQuant is applied, and the model can be tested using the .generate() method. import torch import intel_extension_for_pytorch as ipex from intel_extension_for_pytorch.quantization import prepare import transformers # PART 1: Load model and tokenizer from Hugging Face + Load SmoothQuant config mapping tokenizer = AutoTokenizer.from_pretrained('Intel/neural-chat-7b-v3-3') model = AutoModelForCausalLM.from_pretrained('Intel/neural-chat-7b-v3-3') qconfig = ipex.quantization.get_smooth_quant_qconfig_mapping() # PART 2: Configure calibration # prepare your calibration dataset samples calib_dataset = DataLoader({Your dataloader parameters}) example_inputs = # provide a sample input from your calib_dataset "
            },
            {
                "page_number": 6,
                "text": "calibration_model = ipex.llm.optimize( model.eval(), quantization_config=qconfig, ) prepared_model = prepare( calibration_model.eval(), qconfig, example_inputs=example_inputs ) with torch.no_grad(): for calib_samples in enumerate(calib_dataset): prepared_model(calib_samples) prepared_model.save_qconf_summary(qconf_summary=qconfig_summary_file_path) # PART 3: Model Quantization using SmoothQuant model = ipex.llm.optimize( model.eval(), quantization_config=qconfig, qconfig_summary_file=qconfig_summary_file_path, ) # generation inference loop with torch.inference_mode(): model.generate({your generate parameters}) SmoothQuant is a powerful model compression technique and helps significantly improve inference latency over full-precision models. Still, it requires a little upfront work to prepare a calibration dataset and model. Weight-Only Quantization (int8 and int4) Compared to traditional int8 quantization applied to both activation and weight, weight-only quantization (WOQ) offers a better balance between performance and accuracy. It is worth noting that int4 WOQ requires dequantizing to bf16/fp16 before computation (Figure 4), which introduces an overhead in compute. A basic WOQ technique, tensor-wise asymmetric Round To Nearest (RTN) quantization, presents challenges and often leads to reduced accuracy (source). However, literature (Zhewei Yao, 2022) suggests that groupwise quantizing the model’s weights helps maintain accuracy. Since the weights are only dequantized for computation, a significant memory advantage remains despite this extra step. "
            },
            {
                "page_number": 7,
                "text": "Fig 5. Simple illustration of weight-only quantization, with pre-quantized weights in orange and the quantized weights in green. Note that this depicts the initial quantization to int4/int8 and dequantization to fp16/bf16 for the computation step. — Image by Author The WOQ implementation below showcases the few lines of code required to quantize a model from Hugging Face with this technique. As with the previous implementations, we start by loading a model and tokenizer from Hugging Face. We can use the get_weight_only_quant_qconfig_mapping()method to configure the WOQ recipe. The recipe is then passed to the ipex.llm.optimize() function along with the model for optimization and quantization. The quantized model can then be used for inference with the .generate() method. # requirements #intel-extension-for-pytorch==2.2 #transformers==4.35.2 #torch==2.2.0 import torch import intel_extension_for_pytorch as ipex from transformers import AutoTokenizer, AutoModelForCausalLM # PART 1: Model and tokenizer loading tokenizer = AutoTokenizer.from_pretrained('Intel/neural-chat-7b-v3-3') model = AutoModelForCausalLM.from_pretrained('Intel/neural-chat-7b-v3-3') # PART 2: Preparation of quantization config "
            },
            {
                "page_number": 8,
                "text": "qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping( weight_dtype=torch.qint8, # or torch.quint4x2 lowp_mode=ipex.quantization.WoqLowpMode.NONE, # or FP16, BF16, INT8 ) checkpoint = None # optionally load int4 or int8 checkpoint # PART 3: Model optimization and quantization model = ipex.llm.optimize(model, quantization_config=qconfig, low_precision_checkpoint=checkpoint) inputs = tokenizer('I love learning to code...', return_tensors='pt').input_ids # PART 4: Inference output generation with torch.inference_mode(): tokens = model.generate( inputs, max_new_tokens=64, ) print(tokenizer.decode(tokens[0], skip_special_tokens=True)) As you can see, WOQ provides a powerful way to compress models down to a fraction of their original size with limited impact on language modeling capabilities. Conclusion and Discussion As an engineer at Intel, I’ve worked closely with the IPEX engineering team at Intel. This has afforded me a unique insight into its advantages and development roadmap, making IPEX a preferred tool. However, for developers seeking simplicity without the need to manage an extra dependency, PyTorch offers three quantization recipes: Eager Mode, FX Graph Mode (under maintenance), and PyTorch 2 Export Quantization, providing strong, less specialized alternatives. No matter what technique you choose, model compression techniques will result in some degree of language modeling performance loss, albeit in <1% in many cases. For this reason, it’s essential to evaluate the application’s fault tolerance and establish a baseline for model performance at full (FP32) and/or half-precision (BF16/FP16) before pursuing quantization. In applications that leverage some degree of in-context learning, like Retrieval Augmented Generation (RAG), model compression might be an excellent choice. In these cases, the mission-critical knowledge is spoon-fed to the model at the time of inference, so the risk is heavily reduced even with low-fault-tolerant applications. Quantization is an excellent way to address LLM inference latency concerns without upgrading or expanding compute infrastructure. It is worth exploring regardless of your use case, and IPEX provides a good option to start with just a few lines of code. A few exciting things to try would be:  Test the sample code in this tutorial on the Intel Developer Cloud’s free Jupyter Environment.  Take an existing model that you’re running on an accelerator at complete precision and test it out on a CPU at int4/int8  Explore all three techniques and determine which works best for your use case. Make sure to compare the loss of language modeling performance, not just latency. "
            },
            {
                "page_number": 9,
                "text": " Upload your quantized model to the Hugging Face Model Hub! If you do, let me know — I’d love to check it out! "
            }
        ],
        "images": [
            "Image_8",
            "Image_13",
            "Image_14",
            "Image_17",
            "Image_18",
            "Image_23",
            "Image_30"
        ]
    },
    {
        "file_name": "evaluation.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "12 Important Model Evaluation Metrics for Machine Learning Everyone Should Know  (Updated 2025) The idea of building machine learning models or artificial intelligence or deep learning  models works on a constructive feedback principle. You build a model, get feedback from  metrics, make improvements, and continue until you achieve a desirable classification  accuracy. Evaluation metrics explain the performance of the model. An important aspect of  evaluation metrics is their capability to discriminate among model results The idea of building machine learning models or artificial intelligence or deep learning  models works on a constructive feedback principle. You build a model, get feedback from  metrics, make improvements, and continue until you achieve a desirable classification  accuracy. Evaluation metrics explain the performance of the model. An important aspect of  evaluation metrics is their capability to discriminate among model results. New Feature Beta Make This Article Fun and Interactive with Flash Cards and Quizzes! This article explains 12 important evaluation metrics you must know to use as a data science  professional. You will learn their uses, advantages, and disadvantages, which will help you  choose and implement each of them accordingly. Also, in this article, we explore understanding ML metrics, evaluation metrics in machine  learning, and performance metrics in machine learning, highlighting the importance of out-of- sample evaluation metrics in data science for robust model assessment. Learning Objectives    In this tutorial, you will learn about several evaluation metrics in machine learning, like  confusion matrix, cross-validation, AUC-ROC curve, and many more classification  metrics.    You will also learn about the different metrics used for logistic regression for different  problems.    Lastly, you will learn about cross-validation. 1. What Are Evaluation Metrics? 2. Types of Predictive Models 3. Here are 12 important model evaluation metrics commonly used in machine learning: 4. Confusion Matrix 5. F1 Score 6. Gain and Lift Charts 7. Kolomogorov Smirnov Chart 8. Area Under the ROC Curve (AUC – ROC) 9. Log Loss 10. Gini Coefficient "
            },
            {
                "page_number": 2,
                "text": "11. Concordant – Discordant Ratio 12. Root Mean Squared Error (RMSE) 13. Root Mean Squared Logarithmic Error 14. R-Squared/Adjusted R-Squared 15. Cross Validation 16. Conclusion o  Key Takeaways 17. Frequently Asked Questions If you’re starting out your machine learning journey, you should check out the  comprehensive and popular Applied Machine Learning course that covers this concept in a  lot of detail along with the various algorithms and components of machine learning. What Are Evaluation Metrics? Evaluation metrics are quantitative measures used to assess the performance and  effectiveness of a statistical or machine learning model. These metrics provide insights into how  well the model is performing and help in comparing different models or algorithms. When evaluating a machine learning model, it is crucial to assess its predictive ability,  generalization capability, and overall quality. Evaluation metrics provide objective criteria to  measure these aspects. The choice of evaluation metrics depends on the specific problem  domain, the type of data, and the desired outcome. I have seen plenty of analysts and aspiring data scientists not even bothering to check how  robust their model is. Once they are finished building a model, they hurriedly map predicted  values on unseen data. This is an incorrect approach. The ground truth is building a predictive  model is not your motive. It’s about creating and selecting a model which gives a high  accuracy_score on out-of-sample data. Hence, it is crucial to check the accuracy of your model  prior to computing predicted values. In our industry, we consider different kinds of metrics to evaluate our ml models. The choice of  evaluation metric completely depends on the type of model and the implementation plan of the  model. After you are finished building your model, these 12 metrics will help you in evaluating  your model’s accuracy. Considering the rising popularity and importance of cross-validation,  I’ve also mentioned its principles in this article. Types of Predictive Models When we talk about predictive models, we are talking either about a regression model  (continuous output) or a classification model (nominal or binary output). The evaluation metrics  used in each of these models are different. In classification problems, we use two types of algorithms (dependent on the kind of output it  creates): 1. Class output: Algorithms like SVM and KNN create a class output. For instance, in a binary classification problem, the outputs will be either 0 or 1. However, today we have "
            },
            {
                "page_number": 3,
                "text": "algorithms that can convert these class outputs to probability. But these algorithms are  not well accepted by the statistics community. 2. Probability output: Algorithms like Logistic Regression, Random Forest, Gradient Boosting, Adaboost, etc., give probability outputs. Converting probability outputs to  class output is just a matter of creating a threshold probability. In regression problems, we do not have such inconsistencies in output. The output is always  continuous in nature and requires no further treatment. Illustrative Example For a classification model evaluation metrics discussion, I have used my predictions for the  problem BCI challenge on Kaggle. The solution to the problem is out of the scope of our  discussion here. However, this article uses the final predictions on the training set. The model  produced probability outputs for this problem, which have been converted to class outputs  assuming a threshold of 0.5. Here are 12 important model evaluation metrics commonly used in machine learning: Confusion Matrix A confusion matrix is an N X N matrix, where N is the number of predicted classes. For the  problem in hand, we have N=2, and hence we get a 2 X 2 matrix. It is a performance  measurement for machine learning classification problems where the output can be two or  more classes. Confusion matrix is a table with 4 different combinations of predicted and actual  values. It is extremely useful for measuring precision-recall, Specificity, Accuracy, and most  importantly, AUC-ROC curves. Here are a few definitions you need to remember for a confusion matrix:    True Positive: You predicted positive, and it’s true.    True Negative: You predicted negative, and it’s true.    False Positive: (Type 1 Error): You predicted positive, and it’s false.    False Negative: (Type 2 Error): You predicted negative, and it’s false.    Accuracy: the proportion of the total number of correct predictions that were correct.    Positive Predictive Value or Precision: the proportion of positive cases that were  correctly identified.    Negative Predictive Value: the proportion of negative cases that were correctly  identified.    Sensitivity or Recall: the proportion of actual positive cases which are correctly  identified.    Specificity: the proportion of actual negative cases which are correctly identified.    Rate: It is a measuring factor in a confusion matrix. It has also 4 types TPR, FPR, TNR,  and FNR. "
            },
            {
                "page_number": 4,
                "text": "The accuracy for the problem in hand comes out to be 88%. As you can see from the above two  tables, the Positive Predictive Value is high, but the negative predictive value is quite low. The  same holds for Sensitivity and Specificity. This is primarily driven by the threshold value we have  chosen. If we decrease our threshold value, the two pairs of starkly different numbers will come  closer. In general, we are concerned with one of the above-defined metrics. For instance, in a  pharmaceutical company, they will be more concerned with a minimal wrong positive  diagnosis. Hence, they will be more concerned about high Specificity. On the other hand, an  attrition model will be more concerned with Sensitivity. Confusion matrices are generally used  only with class output models. F1 Score In the last section, we discussed precision and recall for classification problems and also  highlighted the importance of choosing a precision/recall basis for our use case. What if, for a  use case, we are trying to get the best precision and recall at the same time? F1-Score is the  harmonic mean of precision and recall values for a classification problem. The formula for F1- Score is as follows: Now, an obvious question that comes to mind is why you are taking a harmonic mean and not  an arithmetic mean. This is because HM punishes extreme values more. Let us understand this  with an example. We have a binary classification model with the following results: Precision: 0, Recall: 1 Here, if we take the arithmetic mean, we get 0.5. It is clear that the above result comes from a  dumb classifier that ignores the input and predicts one of the classes as output. Now, if we were  to take HM, we would get 0 which is accurate as this model is useless for all purposes. This seems simple. There are situations, however, for which a data scientist would like to give a  percentage more importance/weight to either precision or recall. Altering the above expression  a bit such that we can include an adjustable parameter beta for this purpose, we get: Fbeta measures the effectiveness of a model with respect to a user who attaches β times as  much importance to recall as precision. Gain and Lift Charts Gain and Lift charts are mainly concerned with checking the rank ordering of the probabilities.  Here are the steps to build a Lift/Gain chart:    Step 1: Calculate the probability for each observation    Step 2: Rank these probabilities in decreasing order.    Step 3: Build deciles with each group having almost 10% of the observations. "
            },
            {
                "page_number": 5,
                "text": "   Step 4: Calculate the response rate at each decile for Good (Responders), Bad (Non- responders), and total. You will get the following table from which you need to plot Gain/Lift charts: This is a very informative table. The cumulative Gain chart is the graph between Cumulative  %Right and Cumulative %Population. For the case in hand, here is the graph: This graph tells you how well is your model segregating responders from non-responders. For  example, the first decile, however, has 10% of the population, has 14% of the responders. This  means we have a 140% lift at the first decile. What is the maximum lift we could have reached in the first decile? From the first table of this  article, we know that the total number of responders is 3850. Also, the first decile will contain  543 observations. Hence, the maximum lift at the first decile could have been 543/3850 ~  14.1%. Hence, we are quite close to perfection with this model. Let’s now plot the lift curve. The lift curve is the plot between total lift and %population. Note  that for a random model, this always stays flat at 100%. Here is the plot for the case in hand: You can also plot decile-wise lift with decile number: What does this graph tell you? It tells you that our model does well till the 7th decile. Post which  every decile will be skewed towards non-responders. Any model with lift @ decile above 100%  till minimum 3rd decile and maximum 7th decile is a good model. Else you might consider  oversampling first. Lift / Gain charts are widely used in campaign targeting problems. This tells us to which decile  we can target customers for a specific campaign. Also, it tells you how much response you  expect from the new target base. Kolomogorov Smirnov Chart K-S or Kolmogorov-Smirnov chart measures the performance of classification models. More  accurately, K-S is a measure of the degree of separation between the positive and negative  distributions. The K-S is 100 if the scores partition the population into two separate groups in  which one group contains all the positives and the other all the negatives. On the other hand, If the model cannot differentiate between positives and negatives, then it is  as if the model selects cases randomly from the population. The K-S would be 0. In most  classification models, the K-S will fall between 0 and 100, and the higher the value, the better  the model is at separating the positive from negative cases. For the case in hand, the following is the table: We can also plot the %Cumulative Good and Bad to see the maximum separation. Following is a  sample plot: The evaluation metrics covered here are mostly used in classification problems. So far, we’ve  learned about the confusion matrix, lift and gain chart, and kolmogorov-smirnov chart. Let’s  proceed and learn a few more important metrics. "
            },
            {
                "page_number": 6,
                "text": "Area Under the ROC Curve (AUC – ROC) This is again one of the popular evaluation metrics used in the industry. The biggest advantage  of using the ROC curve is that it is independent of the change in the proportion of responders.  This statement will get clearer in the following sections. Let’s first try to understand what the ROC (Receiver operating characteristic) curve is. If we look  at the confusion matrix below, we observe that for a probabilistic model, we get different values  for each metric. Hence, for each sensitivity, we get a different specificity. The two vary as follows: The ROC curve is the plot between sensitivity and (1- specificity). (1- specificity) is also known as  the false positive rate, and sensitivity is also known as the True Positive rate. Following is the  ROC curve for the case in hand. Let’s take an example of threshold = 0.5 (refer to confusion matrix). Here is the confusion  matrix: As you can see, the sensitivity at this threshold is 99.6%, and the (1-specificity) is ~60%. This  coordinate becomes on point in our ROC curve. To bring this curve down to a single number, we  find the area under this curve (AUC). Note that the area of the entire square is 1*1 = 1. Hence AUC itself is the ratio under the curve  and the total area. For the case in hand, we get AUC ROC as 96.4%. Following are a few thumb  rules:    .90-1 = excellent (A)    .80-.90 = good (B)    .70-.80 = fair (C)    .60-.70 = poor (D)    .50-.60 = fail (F) We see that we fall under the excellent band for the current model. But this might simply be  over-fitting. In such cases, it becomes very important to do in-time and out-of-time validations. Points to Remember 1. A model that gives class as output will be represented as a single point in the ROC plot. 2. You cannot compare such models directly because the judgment relies on a single metric  rather than multiple metrics. For instance, a model with parameters (0.2,0.8) and a model with  parameters (0.8,0.2) can be coming out of the same model; hence these metrics should not be  directly compared. "
            },
            {
                "page_number": 7,
                "text": "3. In the case of the probabilistic model, we were fortunate enough to get a single number which  was AUC-ROC. But still, we need to look at the entire curve to make conclusive decisions. It is  also possible that one model performs better in some regions and other performs better in  others. Advantages of Using ROC Why should you use ROC and not metrics like the lift curve? Lift is dependent on the total response rate of the population. Hence, if the response rate of the  population changes, the same model will give a different lift chart. A solution to this concern  can be a true lift chart (finding the ratio of lift and perfect model lift at each decile). But such a  ratio rarely makes sense for the business. The ROC curve, on the other hand, is almost independent of the response rate. This is because  it has the two axes coming out from columnar calculations of the confusion matrix. The  numerator and denominator of both the x and y axis will change on a similar scale in case of a  response rate shift. Log Loss AUC ROC considers the predicted probabilities for determining our model’s performance.  However, there is an issue with AUC ROC, it only takes into account the order of probabilities,  and hence it does not take into account the model’s capability to predict a higher probability for  samples more likely to be positive. In that case, we could use the log loss, which is nothing but a  negative average of the log of corrected predicted probabilities for each instance.    p(yi) is the predicted probability of a positive class    1-p(yi) is the predicted probability of a negative class    yi = 1 for the positive class and 0 for the negative class (actual values) Let us calculate log loss for a few random values to get the gist of the above mathematical  function:    Log loss(1, 0.1) = 2.303    Log loss(1, 0.5) = 0.693    Log loss(1, 0.9) = 0.105 If we plot this relationship, we will get a curve as follows: It’s apparent from the gentle downward slope towards the right that the Log Loss gradually  declines as the predicted probability improves. Moving in the opposite direction, though, the Log  Loss ramps up very rapidly as the predicted probability approaches 0. "
            },
            {
                "page_number": 8,
                "text": "So, the lower the log loss, the better the model. However, there is no absolute measure of a  good log loss, and it is use-case/application dependent. Whereas the AUC is computed with regards to binary classification with a varying decision  threshold, log loss actually takes the “certainty” of classification into account. Gini Coefficient The Gini coefficient is sometimes used in classification problems. The Gini coefficient can be  derived straight away from the AUC ROC number. Gini is nothing but the ratio between the area  between the ROC curve and the diagonal line & the area of the above triangle. Following are the  formulae used: Gini = 2*AUC – 1 Gini above 60% is a good model. For the case in hand, we get Gini as 92.7%. Concordant – Discordant Ratio This is, again, one of the most important evaluation metrics for any classification prediction  problem. To understand this, let’s assume we have 3 students who have some likelihood of  passing this year. Following are our predictions: A – 0.9  B – 0.5  C – 0.3 Now picture this. if we were to fetch pairs of two from these three students, how many pairs  would we have? We will have 3 pairs: AB, BC, and CA. Now, after the year ends, we see that A  and C passed this year while B failed. No, we choose all the pairs where we will find one  responder and another non-responder. How many such pairs do we have? We have two pairs AB and BC. Now for each of the 2 pairs, the concordant pair is where the  probability of the responder was higher than the non-responder. Whereas discordant pair is  where the vice-versa holds true. In case both the probabilities were equal, we say it’s a tie. Let’s  see what happens in our case : AB – Concordant  BC – Discordant Hence, we have 50% of concordant cases in this example. A concordant ratio of more than 60%  is considered to be a good model. This metric generally is not used when deciding how many  customers to target etc. It is primarily used to access the model’s predictive power. Decisions  like how many to target are again taken by KS / Lift charts. Root Mean Squared Error (RMSE) RMSE is the most popular evaluation metric used in regression problems. It follows an  assumption that errors are unbiased and follow a normal distribution. Here are the key points to  consider on RMSE: 1. The power of ‘square root’ empowers this metric to show large number deviations. "
            },
            {
                "page_number": 9,
                "text": "2. The ‘squared’ nature of this metric helps to deliver more robust results, which prevent canceling the positive and negative error values. In other words, this metric aptly  displays the plausible magnitude of the error term. 3. It avoids the use of absolute error values, which is highly undesirable in mathematical calculations. 4. When we have more samples, reconstructing the error distribution using RMSE is considered to be more reliable. 5. RMSE is highly affected by outlier values. Hence, make sure you’ve removed outliers from your data set prior to using this metric. 6. As compared to mean absolute error, RMSE gives higher weightage and punishes large errors. RMSE metric is given by: where N is the Total Number of Observations. Root Mean Squared Logarithmic Error In the case of Root mean squared logarithmic error, we take the log of the predictions and  actual values. So basically, what changes are the variance that we are measuring? RMSLE is  usually used when we don’t want to penalize huge differences in the predicted and the actual  values when both predicted, and true values are huge numbers. 1. If both predicted and actual values are small: RMSE and RMSLE are the same. 2. If either predicted or the actual value is big: RMSE > RMSLE 3. If both predicted and actual values are big: RMSE > RMSLE (RMSLE becomes almost negligible) R-Squared/Adjusted R-Squared We learned that when the RMSE decreases, the model’s performance will improve. But these  values alone are not intuitive. In the case of a classification problem, if the model has an accuracy of 0.8, we could gauge how  good our model is against a random model, which has an accuracy of 0.5. So the random model  can be treated as a benchmark. But when we talk about the RMSE metrics, we do not have a  benchmark to compare. This is where we can use the R-Squared metric. The formula for R-Squared is as follows: "
            },
            {
                "page_number": 10,
                "text": "MSE(model): Mean Squared Error of the predictions against the actual values MSE(baseline): Mean Squared Error of mean prediction against the actual values In other words, how good is our regression model as compared to a very simple model that just  predicts the mean value of the target from the train set as predictions? Adjusted R-Squared A model performing equal to the baseline would give R-Squared as 0. Better the model, the  higher the r2 value. The best model with all correct predictions would give R-Squared of 1.  However, on adding new features to the model, the R-Squared value either increases or remains  the same. R-Squared does not penalize for adding features that add no value to the model. So  an improved version of the R-Squared is the adjusted R-Squared. The formula for adjusted R- Squared is given by: k: number of features n: number of samples As you can see, this metric takes the number of features into account. When we add more  features, the term in the denominator n-(k +1) decreases, so the whole expression increases. If R-Squared does not increase, that means the feature added isn’t valuable for our model. So  overall, we subtract a greater value from 1 and adjusted r2, in turn, would decrease. Beyond these 12 evaluation metrics, there is another method to check the model performance.  These 7 methods are statistically prominent in data science. But, with the arrival of machine  learning, we are now blessed with more robust methods of model selection. Yes! I’m talking  about Cross Validation. Though cross-validation isn’t really an evaluation metric that is used openly to communicate  model accuracy, the result of cross-validation provides a good enough intuitive result to  generalize the performance of a model. Let’s now understand cross-validation in detail. Cross Validation Let’s first understand the importance of cross-validation. Due to my busy schedule these days, I  don’t get much time to participate in data science competitions. A long time back, I participated  in TFI Competition on Kaggle. Without delving into my competition performance, I would like to  show you the dissimilarity between my public and private leaderboard scores. Here Is an Example of Scoring on Kaggle! For the TFI competition, the following were three of my solution and scores (the lesser, the  better): You will notice that the third entry which has the worst Public score turned out to be the best  model on Private ranking. There were more than 20 models above the “submission_all.csv”, but  I still chose “submission_all.csv” as my final entry (which really worked out well). What caused "
            },
            {
                "page_number": 11,
                "text": "this phenomenon? The dissimilarity in my public and private leaderboards is caused by over- fitting. Over-fitting is nothing, but when your model becomes highly complex that it starts capturing  noise, also. This ‘noise’ adds no value to the model but only inaccuracy. In the following section, I will discuss how you can know if a solution is an over-fit or not before  we actually know the test set results. The Concept of Cross-Validation Cross Validation is one of the most important concepts in any type of data modeling. It simply  says, try to leave a sample on which you do not train the model and test the model on this  sample before finalizing the model. The above diagram shows how to validate the model with the in-time sample. We simply divide  the population into 2 samples and build a model on one sample. The rest of the population is  used for in-time validation. Could there be a negative side to the above approach? I believe a negative side of this approach is that we lose a good amount of data from training the  model. Hence, the model is very high bias. And this won’t give the best estimate for the  coefficients. So what’s the next best option? What if we make a 50:50 split of the training population and the train on the first 50 and validate  on the rest 50? Then, we train on the other 50 and test on the first 50. This way, we train the  model on the entire population, however, on 50% in one go. This reduces bias because of  sample selection to some extent but gives a smaller sample to train the model on. This  approach is known as 2-fold cross-validation. K-Fold Cross-Validation Let’s extrapolate the last example to k-fold from 2-fold cross-validation. Now, we will try to  visualize how a k-fold validation work. This is a 7-fold cross-validation. Here’s what goes on behind the scene: we divide the entire population into 7 equal samples.  Now we train models on 6 samples (Green boxes) and validate on 1 sample (grey box). Then, at  the second iteration, we train the model with a different sample held as validation. In 7  iterations, we have basically built a model on each sample and held each of them as validation.  This is a way to reduce the selection bias and reduce the variance in prediction power. Once we  have all 7 models, we take an average of the error terms to find which of the models is best. How does this help to find the best (non-over-fit) model? k-fold cross-validation is widely used to check whether a model is an overfit or not. If the  performance metrics at each of the k times modeling are close to each other and the mean of  the metric is highest. In a Kaggle competition, you might rely more on the cross-validation score "
            },
            {
                "page_number": 12,
                "text": "than the Kaggle public score. This way, you will be sure that the Public score is not just by  chance. How do we implement k-fold with any model? Coding k-fold in R and Python are very similar. Here is how you code a k-fold in Python: Try out the code for KFold in the live coding window below: # importing the required libraries import numpy as np from sklearn.model_selection import KFold # create a sample dataset X = np.array([[1, 2, 3, 2], [3, 4, 1, 1], [1, 2, 1, 1], [3, 4, 1, 1]]) y = np.array([1, 2, 3, 4]) # create the object of the KFold kf = KFold(n_splits=2) print(kf) KFold(n_splits=2, random_state=None, shuffle=False) for train_index, test_index in kf.split(X): print('TRAIN:', train_index, 'TEST:', test_index) X_train, X_test = X[train_index], X[test_index] y_train, y_test = y[train_index], y[test_index] kf = KFold(n_splits=3) print(kf) KFold(n_splits=3, random_state=None, shuffle=False) "
            },
            {
                "page_number": 13,
                "text": "for train_index, test_index in kf.split(X): print('TRAIN:', train_index, 'TEST:', test_index) X_train, X_test = X[train_index], X[test_index] y_train, y_test = y[train_index], y[test_index] But how do we choose k? This is the tricky part. We have a trade-off to choosing k.  For a small k, we have a higher selection bias but a low variance in the performances.  For a large k, we have a small selection bias but a high variance in the performances. Think of extreme cases: k = 2: We have only 2 samples similar to our 50-50 example. Here we build the model only on  50% of the population each time. But as the validation is a significant population, the variance  of validation performance is minimal.  k = a number of observations (n): This is also known as “Leave one out.” We have n samples and  modeling repeated n number of times, leaving only one observation out for cross-validation.  Hence, the selection bias is minimal, but the variance of validation performance is very large. Generally, a value of k = 10 is recommended for most purposes. Conclusion Measuring the performance of the training sample is pointless. And leaving an in-time validation  batch aside is a waste of data. K-Fold gives us a way to use every single data point, which can  reduce this selection bias to a good extent. In addition, the metrics covered in this article are some of the most used metrics of evaluation  in classification and regression problems. Which metric do you often use in classification and regression problems? Have you used k-fold  cross-validation before for any kind of analysis? Did you see any significant benefits against  using batch validation? Do let us know your thoughts about this guide in the comments section  below. Hope you find this information on ML metrics and evaluation metrics in machine learning helpful  for your projects! Understanding performance metrics and out-of-sample evaluation metrics in  data science is essential for success. Key Takeaways    Evaluation metrics measure the quality of the machine learning model.    For any project evaluating machine learning models or algorithms is essential "
            }
        ],
        "images": []
    },
    {
        "file_name": "explained_rag.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Retrieval Augmented Generation (RAG): Explained Retrieval-Augmented Generation (RAG) is a prevalent technique in generative AI for  building applications using custom data and pre-trained models. It has gained popularity  due to its effectiveness and relative ease of implementation. If you’re developing tools or  products with Large Language Models (LLMs) like GPT-4 or Claude 3, it’s important to  familairaize yourself with RAG so you know how and when to use it. This blog explains what RAG is, how it works, and why it’s important. What is Retrieval Augmented Generation? Retrieval-Augmented Generation (RAG) is a method in Large Language Model (LLM)  application development that dynamically integrates data from external sources into an  LLM's context window to enhance the generation process. As this data can be external to the model’s training data, it can include proprietary, time- sensitive, or domain-specific data, which may contain more accurate, relevant and/or  improved content for a wide range of LLM applications. RAG is most commonly used in LLM applications which require access to new context or  specific data in order to perform a task. We see RAG being used everyday in applications  like:   ChatGPT web-search   Internal enterprise chatbots   “Chat to PDF” services How Does Retrieval Augmented Generation Work? A RAG pipeline is designed to give additional context to an LLM which can be used to  augment its completions. Below is a breakdown of the anatomy of a RAG application: When  a user query is submitted, the content of the query is passed through a retrieval process  which pulls relevant information from a specified data source (knowledge base). This  information, along with the user query, is then incorporated into a prompt template which  contains respective placeholders and instructions for the LLM. The LLM then processes the  enriched template and generates a contextually enhanced response which is returned to  the user. "
            },
            {
                "page_number": 2,
                "text": "The anatomy of a Retrieval Augmented Generation (RAG) application When developing a RAG application, the objective is to engineer the various components  such that for a given user query, relevant information is retrieved and the LLM has clear  guidance, enabling it to generate desirable output. Here are the important steps necessary  to achieve this: 1. Build a knowledge base To get started with RAG, an external database is required. Typically, this database should  contain information not already present in the LLM's training data. For instance, in an  enterprise chatbot, this may be internal company information from documentation or  proprietary databases. The most common approach in RAG for building a knowledge base is using text data,  embedding models and vector databases like Pinecone or ChromaDB. In this approach,  the text data is converted into vectors (numerical representations) using an embedding  model like OpenAI’s text-embedding-3-large, and then stored on a vector database. Vector  storage is designed to capture complex, high-dimensional data. This provides a  representation of semantics, contextual relevance, conceptual similarity and more within  the text data. 2. Information Retrieval The retrieval component takes the user query as a basis for its search. For example, if your  external data source/knowledge base was Google search, and a user query was “What is  the weather like today in London?”, this would be run on Google and the top search results  would be given to the LLM as context to respond to the query. This example can easily be  implemented using Humanloop Tools. In a RAG pipeline built on a vector database, when a user inputs a query, the text is  embedded into vectors in real-time (using the same embedding model as before). These "
            },
            {
                "page_number": 3,
                "text": "vectors are then matched with those stored in the database, and the ones most similar to  the query are retrieved. This process is called vector search, and since it enables the  capture of a broader context within the embedded text, the relevance of the information  retrieved should be significantly enhanced. Getting the retrieval process working effectively is crucial as it will determine the reliability  of the RAG pipeline. There is room for experimentation here between how you prepare your  data, choose the database and select a retrieval method. Our guide to optimizing LLM  applications covers when to use RAG and how you should set it up. 3. Augment the Prompt Template The prompt template contains the instructions you provide to the LLM. Here, you describe  the task step-by-step and create dynamic inputs for the user’s query and retrieved context  (as known as chunks). The LLM will see something which resembles the following: Basic RAG Prompt Template for an enterpire chatbot using the Humanloop Editor Based on the combined information in the prompt template, the LLM generates a response.  The prompt template is by far the most powerful component of the RAG framework, and it’s  imperative that the model understands its task and consistently performs. At this step, it’s crucial to ensure that your prompt engineering approach is applied  appropriately, enabling the model to discern when (and when not) to utilize the retrieved  data. For this purpose, using a larger model, such as GPT-4 or Claude 3 is advisable, as  their enhanced reasoning capabilities will significantly improve performance. 4. Evaluate performance Comprehensive testing and evaluation of the RAG system should be consistently  undertaken during development and whilst in production. Humanloop’s evaluation  features support this process for each stage of the development lifecycle. "
            },
            {
                "page_number": 4,
                "text": "By using Humanloop, you can develop test sets of inputs and automatically run  evaluations of prompts and tools (in this case, your retriever), whose performance can  then be scored based on the evaluation metrics you set. Humanloop offers flexibility to  accommodate the full spectrum of complexity in the judgments required for providing  evaluation scores, enabling you to create evaluators using AI, code (Python), or human  review. To learn more, read about our approach to evaluating LLM applications. Evaluators supported on Humanloop Why is Retrieval Augmented Generation Important? RAG is crucial for applications that require awareness of recent developments or specific  domain knowledge. It represents a straightforward, cost-effective approach to provide an  LLM with new context without the need for retraining or fine-tuning, enabling teams to  quickly iterate during the development of AI applications built on their own data. Benefits of Retrieval Augmented Generation Speed "
            },
            {
                "page_number": 5,
                "text": "RAG frameworks can be relatively quick to build, allowing teams to promptly develop  custom AI applications. Building a RAG pipeline on a pre-trained model significantly  outpaces the time required to train a custom model. It is also quicker and easier to make  real-time interventions and updates to RAG pipelines, compared to that of custom-trained  or fine-tuned models. Cost Effective Compared to the cost of training an LLM for an enterprise application, RAG is a vastly  cheaper alternative. The set-up cost is minimal and pricing is based on token usage per API  call to the models being used. Information is Current Popular LLMs like GPT-4 and Claude 3 are static, meaning their knowledge cuts off after  their training date, which trails a few months behind their release date. RAG can be used to  overcome this by containing current information in the knowledge base. Increased User Trust Developers can label chunks with their original source before storing them in the  knowledge base. This way when they’re retrieved, the LLM has the context of the data  source and can reference this its completions, allowing a user to verify the source  themselves. Known as grounding the LLM, this enhances data visibility and is useful in LLM  applications in legal, healthcare, financial and other critical settings. Retrieval Augmented Generation with Humanloop Humanloop is a collaborative enterprise platform for developing and evaluating LLM  applications. We solve the most critical workflows around prompt engineering and model  evaluation, which allows product managers and engineers to work together to find,  manage, and measure prompts. Coupled with this are tools for rigorously monitoring and  evaluating performance, both from user feedback and automated evaluations. Using  Humanloop, you can develop the different components of your RAG system with a  dedicated environment for prompts and tools (retrievers) where you can test, evaluate, and  track performance. This makes it easier for your teams to work together to get AI from  playground to production. To learn more, you can book a demo. "
            }
        ],
        "images": [
            "Image_22",
            "Image_31",
            "Image_35"
        ]
    },
    {
        "file_name": "feature_data1.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Feature Data What is Feature Data? Feature data, often referred to simply as features, is simply the data that is passed as input to  machine learning (ML) models. You use feature data both when training a model and when  making predictions with a model (inference) - in both cases the input to the model is feature  data.  Feature data could be diverse, ranging from numerical attributes like age and price to  categorical variables such as hair color or genre preference. Feature data act as the inputs to  algorithms, providing crucial information for training ML models. It serves as the cornerstone for  building robust models capable of making accurate predictions and classifications across  various domains. Example of feature data in the Iris flower data set - the features are the sepal length, sepal width,  petal length, and petal width. The other columns are not features - the leftmost column is an ID  for each row (an index in Pandas) and the rightmost column is the label. In tabular data, feature data is the sequence of columns that are used as input to the model.  Feature data can be encoded or, in the above table, unencoded. Depending on the model that "
            },
            {
                "page_number": 2,
                "text": "will use the feature data, it may need to be encoded into a numerical format and/or  scaled/normalized. Features Types In the table above, each feature contains either quantitative or qualitative information.  Quantitative data is numerical. For example, the amount of rainfall or the  temperature.  Qualitative data is discrete, with a finite number of well-known values, like  purpose  in the above table. A taxonomy of feature types is shown in the figure below, where a feature can be a discrete  variable (categorical),  a continuous variable (numerical), or an array (many values). Just like a  data type, each feature type has a set of valid transformations. For example, you can normalize  a numerical variable (but not a categorical variable), and you can one-hot encode a categorical  variable (but not a numerical variable). For arrays, you can return the maximum, minimum, or  average value from a list of values (assuming those values are numerical). The feature type  helps you understand what aggregations and transformations you can apply to a variable. Engineering Feature Data The essence of feature data lies in its ability to encapsulate relevant information about the  problem at hand, enabling algorithms to learn patterns and make informed decisions. However,  not all data can be directly used by ML models. Real-world raw data can be from different data  sources, unstructured and contain redundant information. Including irrelevant features can  confuse the model and hinder its ability to identify the true patterns. Feature engineering helps  bridge this gap by transforming raw data into meaningful features the model can understand. It  allows us to select the most informative and discriminating features, focusing the model's  learning process on what truly matters. Feature engineering emerges as a crucial process in the ML pipeline, where raw data is  transformed to extract meaningful features. This process involves tasks like data cleaning,  feature selection, feature extraction, feature scaling, feature encoding, and so on. Selection of Feature Data Not all features are equally important. Depending on specific tasks, some features may carry  more predictive power than others. Using irrelevant features can confuse the model and hinder  its ability to learn. Feature selection identifies the most relevant features from the pool of "
            },
            {
                "page_number": 3,
                "text": "available data, eliminating noise and redundancy. It can help ML models improve performance  from the following perspectives:    Curse of dimensionality: In high-dimensional spaces, the number of features can  sometimes outnumber the number of data points or samples. This imbalance often  leads to overfitting, where models learn noise in the data rather than true patterns.  Feature selection mitigates this risk by focusing on only the most informative features.    Improved generalization: Models trained on only the relevant features are more likely to  generalize well to unseen data. By selecting informative features, feature selection  enhances the model's ability to capture underlying true patterns and make accurate  predictions on new samples.    Computational efficiency: Including irrelevant or redundant features increases the  complexity of the model and the computational cost of model training and inference. By  excluding unnecessary features, execution times can be faster and the cost of  computational resources can be reduced. Extraction of Feature Data Feature extraction is to derive new features from the existing ones to capture complex  relationships or reduce dimensionality. Feature extraction can be thought of as a distillation  process where the most salient aspects of the data are extracted and distilled into a more  manageable form. This distilled representation retains the key information required for the  intended task while discarding less relevant details. The following techniques help distill  essential information.    Principal Component Analysis (PCA): PCA is a popular technique for dimensionality  reduction that identifies principal components along which the data exhibits the most  variation. By retaining a subset of these components, PCA effectively compresses the  data while preserving its variance.    Statistical methods: Techniques like calculating means, medians, and variances can  extract features that summarize the central tendency or variability within the data.    Autoencoders: Autoencoders are neural network architectures used for unsupervised  feature learning. They learn to encode the input data into a lower-dimensional  representation (encoding) and then decode it back to the original input space.  Autoencoders can capture complex relationships in the data and are capable of  nonlinear feature extraction. Scaling of Feature Data Feature scaling, or data normalization, involves transforming the values of features within a  dataset to a common range. This is particularly important when dealing with datasets  containing features that have different units or varying ranges. For example, a dataset with  features like 'income' (in dollars) and 'age' (in years). The raw values would have vastly  different scales, with income values potentially reaching thousands or millions, while age often  stays below 100. Ensuring uniformity in feature magnitudes can prevent certain features from dominating others  during model training. On the other hand, it can improve the gradient descent convergence of "
            },
            {
                "page_number": 4,
                "text": "ML algorithms during gradient descent optimization, leading to smoother convergence towards  the optimal solution. There are several popular techniques for feature scaling:    Min-max scaling (normalization): This technique scales each feature to a specific range,  typically between 0 and 1 (or -1 and 1). It preserves the relative relationships between  values in a feature but can be sensitive to outliers. It is suitable if the distribution of your  data is unknown.    Standardization: This technique transforms features by subtracting the mean value from  each data point and then dividing by the standard deviation. It results in features with a  mean of 0 and a standard deviation of 1. This method is robust to outliers. It is suitable if  your data has a Gaussian distribution and you want to emphasize features with higher  standard deviations. Encoding of Feature Data ML algorithms typically work best with numerical data. Feature encoding transforms non- numerical variables (e.g., categorical data, text data) into numerical representations to feed into  ML models. There are several ways to encode categorical features:    Label encoding: This method assigns a unique integer value to each category. It's simple  and efficient but may lead ML models to misunderstand that the numerical orders are  meaningful, for example, 3 is more important than 1. This encoding method is typically  used for simple categorical data with few categories.    One-Hot encoding: This technique creates a new binary feature for each category. Each  new feature indicates the presence (value of 1) or absence (value of 0) of that category.  This method avoids the ordering issue but can lead to a significant increase in feature  dimensionality (number of features) for datasets with many categories.    Word embedding: This is particularly used in large language models to represent words  or phrases into vector representations while capturing semantic relationships and  contextual information. Popular word embedding techniques include Word2Vec, GloVe,  and FastText. Storage of Feature Data Feature data is often managed by a feature store that acts as a centralized repository for feature  data. It serves as the backbone for feature engineering pipelines, facilitating the creation,  transformation, and extraction of features from raw data sources. By providing a unified location  for storing and managing features, it streamlines the feature engineering process, ensuring  consistency and reusability across different ML projects and teams. In dynamic environments where data can change frequently, a feature store becomes even  more crucial as it provides a centralized and up-to-date repository for managing evolving feature  data. This ensures that ML models can adapt to changing data patterns and maintain accuracy  over time. Feature stores also incorporate versioning capabilities to track changes made to  feature data over time, enabling reproducibility and auditability in model development. Summary Feature data consists of numerical, categorical, or textual attributes that encode valuable  information, forming the foundation for constructing ML models. By effective selection, "
            },
            {
                "page_number": 5,
                "text": "extraction, and encoding of features, raw data becomes meaningful representations, which is  paramount for enhancing model accuracy, generalization, and efficiency, ultimately enabling  intelligent systems to make informed decisions across various domains. "
            }
        ],
        "images": [
            "Image_14",
            "Image_15",
            "Image_22"
        ]
    },
    {
        "file_name": "feature_data2.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "What are Features in Machine Learning and Why it is Important? In machine learning, features are individual independent variables that act like a input in your  system. Actually, while making the predictions, models use such features to make the  predictions. And using the feature engineering process, new features can also be obtained from  old features in machine learning. To understand in more simple way, lets take an example, where you can consider one column of  your data set to be one feature which is also know as “variables or attributes” and the more  number of features are known as dimensions. And depending on what you are trying to analyze  the features you include in your dataset can vary widely. What is Feature Engineering in Machine Learning? Feature engineering is the process of using the domain knowledge of the data to create features  that makes machine learning algorithms work properly. If feature engineering is performed  properly, it helps to improve the power of prediction of machine learning algorithms by creating  the features using the raw data that facilitate the machine learning process. Why Feature is Important in Machine Learning? Features in machine learning is very important, being building a blocks of datasets, the quality  of the features in your dataset has major impact on the quality of the insights you will get while  using the dataset for machine learning. However, depending on the different business problems in different industries it is not  necessary the features should be same features, so here you need to strongly understand the  business goal of your data science project. Where on the other hand, using the “feature selection” and “feature engineering” process you  can improve the quality of your datasets’ features, which a very tedious and difficult process. It  these techniques are working well, you will get optimal dataset with all of the important "
            },
            {
                "page_number": 2,
                "text": "features, that bearing on your specific business problem leads to the best possible model  development and the most beneficial visual perception. Tops Methods of Feature Selection in ML:    Universal Selection    Feature Importance    Correlation Matrix with Heatmap Feature engineering is the most important part of machine leaning that makes difference  between and good and bad model. And there are several steps involved in feature engineering  and most preferred steps are given below. Steps To Do Feature Engineering in ML: 1. Gathering Data 2. Cleaning DATA 3. Feature Engineering 4. Defining Model 5. Training & Testing of model prediction To perform the feature engineering in machine learning you need data experts like data  scientists or hire machine learning engineer who can understand and perform the feature  engineering process with right instructions. Cogito is one the companies providing the hiring  and recruitment services with outsourcing of data scientists and machine learning engineers for  in-house AI development or for remote locations as per the requirements of various companies. Training & Testing of model prediction "
            },
            {
                "page_number": 3,
                "text": "To perform the feature engineering in machine learning you need data experts like data  scientists or hire machine learning engineer who can understand and perform the feature  engineering process with right instructions. Cogtio is one the companies providing the hiring  and recruitment services with outsourcing of data scientists and machine learning engineers for  in-house AI development or for remote locations as per the requirements of various companies.  Originally published at Source "
            }
        ],
        "images": [
            "Image_16",
            "Image_27"
        ]
    },
    {
        "file_name": "fine_tunning.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "What is fine-tuning? Fine-tuning could be considered a subset of the broader technique of transfer learning: the  practice of leveraging knowledge an existing model has already learned as the starting point for  learning new tasks. The intuition behind fine-tuning is that, essentially, it’s easier and cheaper to hone the  capabilities of a pre-trained base model that has already acquired broad learnings relevant to  the task at hand than it is to train a new model from scratch for that specific purpose. This is  especially true for deep learning models with millions or even billions of parameters, like the  large language models (LLMs) that have risen to prominence in the field of natural language  processing (NLP) or the complex convolutional neural networks (CNNs) and vision transformers  (ViTs) used for computer vision tasks like image classification, object detection or image  segmentation. By leveraging prior model training through transfer learning, fine-tuning can reduce the amount  of expensive computing power and labeled data needed to obtain large models tailored to niche  use cases and business needs. For example, fine-tuning can be used to simply adjust the  conversational tone of a pre-trained LLM or the illustration style of a pre-trained image  generation model; it could also be used to supplement learnings from a model’s original training  dataset with proprietary data or specialized, domain-specific knowledge. Fine-tuning thus plays an important role in the real-world application of machine learning  models, helping democratize access to and customization of sophisticated models. Fine-tuning vs. Training While fine-tuning is ostensibly a technique used in model training, it’s a process distinct from  what is conventionally called “training.” For the sake of disambiguation, data scientists typically  refer to the latter as pre-training in this context. (Pre-)Training At the onset of training (or, in this context, pre-training), the model has not yet “learned”  anything. Training begins with a random initialization of model parameters—the varying weights  and biases applied to the mathematical operations occurring at each node in the neural  network. Training occurs iteratively in two phases: in a forward pass, the model makes predictions for a  batch of sample inputs from the training dataset, and a loss function measures the difference  (or loss) between the model’s predictions for each input and the “correct” answers (or ground  truth); during backpropagation, an optimization algorithm—typically gradient descent—is used  to adjust model weights across the network to reduce loss. These adjustments to model  weights are how the model “learns.” The process is repeated across multiple training epochs  until the model is deemed to be sufficiently trained. Conventional supervised learning, which is typically used to pre-train models for computer  vision tasks like image classification, object detection or image segmentation, uses labeled  data: labels (or annotations) provide both the range of possible answers and the ground truth  output for each sample. LLMs are typically pre-trained through self-supervised learning (SSL), in which models learn  through pretext tasks that are designed to derive ground truth from the inherent structure of "
            },
            {
                "page_number": 2,
                "text": "unlabeled data. These pretext tasks impart knowledge useful for downstream tasks. They  typically take one of two approaches:    Self-prediction: masking some part of the original input and tasking the model with  reconstructing it. This is the dominant mode of training for LLMs.    Contrastive learning: training models to learn similar embeddings for related inputs and  different embeddings for unrelated inputs. This is used prominently in computer vision  models designed for few-shot or zero-shot learning, like Contrasting Language-Image  Pretraining (CLIP). SSL thus allows for the use of massively large datasets in training without the burden of having  to annotate millions or billions of data points. This saves a tremendous amount of labor, but  nevertheless requires huge computational resources. Fine-tuning Conversely, fine-tuning entails techniques to further train a model whose weights have already  been updated through prior training. Using the base model’s previous knowledge as a starting  point, fine-tuning tailors the model by training it on a smaller, task-specific dataset. While that task-specific dataset could theoretically have been used for the initial training,  training a large model from scratch on a small dataset risks overfitting: the model might learn to  perform well on the training examples, but generalize poorly to new data. This would render the  model ill-suited to its given task and defeat the purpose of model training. Fine-tuning thus provides the best of both worlds: leveraging the broad knowledge and stability  gained from pre-training on a massive set of data and honing the model’s understanding of more  detailed, specific concepts. Given the increasing prowess of open source foundation models,  the benefits can often be enjoyed without any of the financial, computational or logistical  burden of pre-training. How does fine-tuning work? Fine-tuning uses the weights of a pre-trained model as a starting point for further training on a  smaller dataset of examples that more directly reflect the specific tasks and use cases the  model will be utilized for. It typically entails supervised learning, but can also involve  reinforcement learning, self-supervised learning or semi-supervised learning. The datasets used for fine-tuning convey the specific domain knowledge, style, tasks or use  cases for which the pre-trained model is being fine-tuned. For example:    An LLM pre-trained for general language might be fine-tuned for coding with a new  dataset containing relevant programming requests and corresponding code snippets for  each.    An image classification model used to identify certain species of birds can learn new  species through additional labeled training samples. "
            },
            {
                "page_number": 3,
                "text": "   An LLM can learn to emulate a specific writing style through self-supervised learning on  sample texts representing that style. Semi-supervised learning, a subset of machine learning that incorporates both labeled and  unlabeled data, is advantageous when the scenario calls for supervised learning but suitable  labeled examples are scarce. Semi-supervised fine-tuning has yielded promising results for  both computer vision1 and NLP2 tasks and helps reduce the burden of acquiring a sufficient  amount of labeled data. Fine-tuning can be used to update the weights of the entire network, but for practical reasons  this is not always the case. There exist a wide variety of alternate fine-tuning methods, often  referred to under the umbrella term of parameter-efficient fine-tuning (PEFT), that update only a  select subset of model parameters. PEFT methods, which are explored later in this section, can  decrease computational demands and reduces catastrophic forgetting—the phenomenon in  which fine-tuning causes the loss or destabilization of the model’s core knowledge—often  without meaningful compromises in performance. Given the wide variety of fine-tuning techniques and the many variables inherent to each,  achieving ideal model performance often requires multiple iterations of training strategies and  setups, adjusting datasets and hyperparameters like batch size, learning rate and regularization  terms until a satisfactory outcome—per whichever metrics are most relevant to your use case— has been reached. Full fine-tuning The most conceptually straightforward means of fine-tuning is to simply update the entire  neural network. This simple methodology essentially resembles the pre-training process: the  only fundamental differences between the full fine-tuning and pre-training processes are the  dataset being used and the initial state of the model’s parameters. To avoid destabilizing changes from the fine-tuning process, certain hyperparameters—model  attributes that influence the learning process but are not themselves learnable parameters— might be adjusted relative to their specifications during pre-training: for example, a smaller  learning rate (which reduces the magnitude of each update to model weights) is less likely to  lead to catastrophic forgetting. Parameter efficient fine-tuning (PEFT) Full fine-tuning, like the pre-training process it resembles, is very computationally demanding.  For modern deep learning models with hundreds of millions or even many billions of  parameters, it’s often prohibitively costly and impractical. Parameter efficient fine-tuning (PEFT) encompasses a range of methods to reduce the number  of trainable parameters that need to be updated in order to effectively adapt a large pre-trained  model to specific downstream applications. In doing so, PEFT significantly decreases the  computational resources and memory storage needed to yield an effectively fine-tuned model.  PEFT methods have often been demonstrated to be more stable than full fine-tuning methods,  particularly for NLP use cases.3 Partial fine-tuning  Also called selective fine-tuning, partial fine-tuning methods aim to reduce computational  demands by updating only the select subset of pre-trained parameters most critical to model "
            },
            {
                "page_number": 4,
                "text": "performance on relevant downstream tasks. The remaining parameters are “frozen,” ensuring  that they will not be changed. The most intuitive partial fine-tuning approach is to update only the outer layers of the neural  network. In most model architectures, the inner layers of the model (closest to the input layer)  capture only broad, generic features: for example, in a CNN used for image classification, early  layers typically discern edges and textures; each subsequent layer discerns progressively finer  features until final classification is predicted at the outermost layer. Generally speaking, the  more similar the new task (for which the model is being fine-tuned) is to the original task, the  more useful the pre-trained weights of the inner layers will already be for this new, related  task—and thus the fewer layers need to be updated). Other partial fine-tuning methods including updating only the layer-wide bias terms of the  model (rather than the node-specific weights)4 and “sparse” fine-tuning methods that update  only a select subset of overall weights throughout the model.5 Additive fine-tuning  Rather than fine-tuning the existing parameters of a pre-trained model, additive methods add  extra parameters or layers to the model, freeze the existing pre-trained weights, and train only  those new components. This approach helps retain stability of the model by ensuring that the  original pre-trained weights remain unchanged. While this can increase training time, it significantly reduces memory requirements because  there are far fewer gradients and optimization states to store: according to Lialin, et al, training  all of a model’s parameters requires 12–20 times more GPU memory than the model weights  alone.6 Further memory savings can be achieved through quantization of the frozen model  weights: a reduction in the precision used to represent model parameters, conceptually similar  to lowering the bitrate of an audio file. One sub-branch of additive methods is prompt tuning. Conceptually, it’s similar to prompt  engineering, which refers to tailoring “hard prompts”—that is, prompts written by a human in  natural language—to guide the model toward the desired output, such as by specifying a certain  tone or by providing examples that facilitate few-shot learning. Prompt tuning introduces AI- authored soft prompts: learnable vector embeddings that are concatenated to the user’s hard  prompt. Rather than retraining the model, prompt tuning entails freezing model weights and  instead trains the soft prompt itself. Fast and efficient, prompt tuning allows for models to more  easily switch between specific tasks, albeit with a tradeoff in interpretability. Reparameterization  Reparameterization-based methods like Low Rank Adaptation (LoRA) leverage low-rank  transformation of high-dimensional matrices (like the massive matrix of pre-trained model  weights in a transformer model). These low-rank representations omit inconsequential higher- dimensional information in order to capture the underlying low-dimensional structure of model  weights, greatly reducing the number of trainable parameters. This dramatically speeds up fine- tuning and reduces memory needed to store model updates. LoRA eschews direct optimization of the matrix of model weights and instead optimizes a matrix  of updates to model weights (or delta weights), which is inserted into the model. That matrix of  weight updates is, in turn, represented as two smaller (i.e., lower rank) matrices, greatly  reducing the number of parameters to be updated—which, in turn, dramatically speeds up fine- "
            },
            {
                "page_number": 5,
                "text": "tuning and reduces memory needed to store model updates. The pre-trained model weights  themselves remain frozen. An added benefit of LoRA is that, since what’s being optimized and stored are not new model  weights but rather the difference (or delta) between the original pre-trained weights and fine- tuned weights, different task-specific LoRAs can be “swapped in” as needed to adapt the pre- trained model—whose actual parameters remain unchanged—to a given use case. A variety of LoRA derivatives has been developed, such as QLoRA, which further reduces  computational complexity by quantizing the transformer model prior to LoRA. Fine-tuning large language models Fine-tuning is an essential part of the LLM development cycle, allowing the raw linguistic  capabilities of base foundation models to be adapted for a variety of use cases, from chatbots  to coding to other domains both creative and technical. LLMs are pre-trained using self-supervised learning on a massive corpus of unlabeled data.  Autoregressive language models, like OpenAI’s GPT, Google’s Gemini or Meta’s Llama models,  are trained to simply predict the next word(s) in a sequence until it’s complete. In pre-training,  models are provided the beginning of a sample sentence drawn from the training data and  repeatedly tasked with predicting the next word in the sequence until the end of the sample. For  each prediction, the actual next word of the original sample sentence serves as ground truth. While this pre-training yields powerful text generation capabilities, it does not yield any  actual understanding of a user’s intent. On a fundamental level, autoregressive LLMs do  not actually answer a prompt; they only append text to it. Without very specific guidance in  the form of prompt engineering, a pre-trained LLM (that has not been fine-tuned) simply  predicts, in a grammatically coherent way, what might be the next word(s) in a given sequence  that is initiated by the prompt. If prompted with “teach me how to make a resumé,” an LLM  might respond with “using Microsoft Word.” It’s a valid way to complete the sentence, but not  aligned with user’s goal. The model might already have a substantial knowledge of resumé  writing gleaned from relevant content included in its pre-training corpus, but without fine-tuning  this knowledge might not be accessed. The fine-tuning process thus serves a crucial role in not only tailoring foundation models for  your or your business’s unique tone and use cases, but in making them altogether suitable for  practical usage. Instruction tuning Instruction tuning is a subset of supervised fine-tuning (SFT), often used to fine-tune LLMs for  chatbot usage, that primes the LLM to generate responses that more directly address user  needs: in other words, to better follow instructions. Labeled examples, following the format  (prompt, response)—in which the prompt examples comprise instruction-oriented tasks, like  “translate the following sentence from English to Spanish” or “classify the following sentence as  Positive or Negative”—demonstrate how to respond to prompts representing a variety of use  cases, like question answering, summarization or translation. In updating model weights to  minimize the loss between model outputs and the labeled samples, the LLM learns to append  text to prompts in a more useful way and better follow instructions in general. "
            },
            {
                "page_number": 6,
                "text": "Continuing the earlier prompt example of “teach me how to write a resumé,” the dataset used  for SFT could contain a number of (prompt, response) pairs demonstrating that the desired way  to respond to prompts beginning with “teach me how to” is to provide step by step suggestions,  rather than merely complete the sentence. Reinforcement learning from human feedback (RLHF) While instruction tuning can teach the model tangible, straightforward behaviors like how to  structure its responses, it can be prohibitively laborious and difficult to teach abstract human  qualities like helpfulness, factual accuracy, humor or empathy through labeled examples. To better align model outputs with ideal human behavior, especially for conversational use  cases like chatbots, SFT may be supplemented with reinforcement learning—more specifically,  reinforcement learning from human feedback (RLHF). RLHF, also called reinforcement learning  from human preferences, helps fine-tune models for qualities that are complex, ill-defined or  difficult to specify through discrete examples. Consider comedy: to teach a model to be “funny” with SFT not only requires the cost and labor  of writing (or acquiring) enough jokes to constitute a learnable pattern, but also requires that  what a given data scientist thinks is funny aligns with what the user base would find funny. RLHF  essentially provides a mathematically crowdsourced alternative: prompt the LLM to generate  jokes and have human testers rate their quality. These ratings can be used to train a reward  model to predict the kinds of jokes that will receive positive feedback, and in turn that reward  model can be use to train the LLM through reinforcement learning. More practically, RLHF aims to address existential challenges of LLMs, like hallucinations,  reflecting societal biases inherent in training data or dealing with rude or adversarial user inputs. Common fine-tuning use cases Fine-tuning can be used for a wide range of purposes, from customizing to supplementing the  model’s core knowledge to extending the model to entirely new tasks and domains.    Customizing style: Models can be fine-tuned to reflect a brand’s desired tone, from  implementing complex behavioral patterns and idiosyncratic illustration styles to simple  modifications like beginning each exchange with a polite salutation.    Specialization: The general linguistic abilities of LLMs can be honed for specific tasks.  For example, Meta’s Llama 2 models were released as base foundation models,  chatbot-tuned variants (Llama-2-chat) and code-tuned variants (Code Llama).    Adding domain-specific knowledge: While LLMs are pre-trained on a massive corpus  of data, they are not omniscient. Using additional training samples to supplement the  base model’s knowledge is particularly relevant in legal, financial or medical settings,  which typically entail use of specialized, esoteric vocabulary that may not have been  sufficiently represented in pre-training.    Few-shot learning: Models that already have strong generalized knowledge can often be  fine-tuned for more specific classification texts using comparatively few demonstrative  examples.    Addressing edge cases: You may want your model to handle certain situations that are  unlikely to have been covered in pre-training in a specific way. Fine-tuning a model on "
            },
            {
                "page_number": 7,
                "text": "labeled examples of such situations is an effective way to ensure they are dealt with  appropriately.    Incorporating proprietary data: Your company may have its own proprietary data  pipeline, highly relevant to your specific use case. Fine-tuning allows this knowledge to  be incorporated into the model without having to train it from scratch. "
            }
        ],
        "images": []
    },
    {
        "file_name": "fine_tunning_openai.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Fine-tuning =========== Fine-tune models for better results and efficiency. Fine-tuning lets you get more out of the models available through the API by providing: *   Higher quality results than prompting *   Ability to train on more examples than can fit in a prompt *   Token savings due to shorter prompts *   Lower latency requests OpenAI's text generation models have been pre-trained on a vast amount of text. To use the  models effectively, we include instructions and sometimes several examples in a prompt. Using  demonstrations to show how to perform a task is often called 'few-shot learning.' Fine-tuning improves on few-shot learning by training on many more examples than can fit in the  prompt, letting you achieve better results on a wide number of tasks. **Once a model has been  fine-tuned, you won't need to provide as many examples in the prompt.** This saves costs and  enables lower-latency requests. At a high level, fine-tuning involves the following steps: 1.  Prepare and upload training data 2.  Train a new fine-tuned model 3.  Evaluate results and go back to step 1 if needed 4.  Use your fine-tuned model Visit our [pricing page](https://openai.com/api/pricing) to learn more about how fine-tuned  model training and usage are billed. ### Which models can be fine-tuned? "
            },
            {
                "page_number": 2,
                "text": "Fine-tuning is currently available for the following models: *   `gpt-4o-2024-08-06` *   `gpt-4o-mini-2024-07-18` *   `gpt-4-0613` *   `gpt-3.5-turbo-0125` *   `gpt-3.5-turbo-1106` *   `gpt-3.5-turbo-0613` You can also fine-tune a fine-tuned model, which is useful if you acquire additional data and  don't want to repeat the previous training steps. We expect `gpt-4o-mini` to be the right model for most users in terms of performance, cost,  and ease of use. When to use fine-tuning ----------------------- Fine-tuning OpenAI text generation models can make them better for specific applications, but  it requires a careful investment of time and effort. We recommend first attempting to get good  results with prompt engineering, prompt chaining (breaking complex tasks into multiple  prompts), and [function calling](/docs/guides/function-calling), with the key reasons being: *   There are many tasks at which our models may not initially appear to perform well, but results  can be improved with the right prompts - thus fine-tuning may not be necessary *   Iterating over prompts and other tactics has a much faster feedback loop than iterating with  fine-tuning, which requires creating datasets and running training jobs *   In cases where fine-tuning is still necessary, initial prompt engineering work is not wasted -  we typically see best results when using a good prompt in the fine-tuning data (or combining  prompt chaining / tool use with fine-tuning) Our [prompt engineering guide](/docs/guides/prompt-engineering) provides a background on  some of the most effective strategies and tactics for getting better performance without fine- tuning. You may find it helpful to iterate quickly on prompts in our [playground](/playground). "
            },
            {
                "page_number": 3,
                "text": "### Common use cases Some common use cases where fine-tuning can improve results: *   Setting the style, tone, format, or other qualitative aspects *   Improving reliability at producing a desired output *   Correcting failures to follow complex prompts *   Handling many edge cases in specific ways *   Performing a new skill or task that’s hard to articulate in a prompt One high-level way to think about these cases is when it’s easier to 'show, not tell'. In the  sections to come, we will explore how to set up data for fine-tuning and various examples where  fine-tuning improves the performance over the baseline model. Another scenario where fine-tuning is effective is reducing cost and/or latency by replacing a  more expensive model like `gpt-4o` with a fine-tuned `gpt-4o-mini` model. If you can achieve  good results with `gpt-4o`, you can often reach similar quality with a fine-tuned `gpt-4o-mini`  model by fine-tuning on the `gpt-4o` completions, possibly with a shortened instruction  prompt. Preparing your dataset ---------------------- Once you have determined that fine-tuning is the right solution (i.e. you’ve optimized your  prompt as far as it can take you and identified problems that the model still has), you’ll need to  prepare data for training the model. You should create a diverse set of demonstration  conversations that are similar to the conversations you will ask the model to respond to at  inference time in production. Each example in the dataset should be a conversation in the same format as our [Chat  Completions API](/docs/api-reference/chat/create), specifically a list of messages where each  message has a role, content, and [optional name](/docs/api- reference/chat/create#chat/create-chat/create-messages-name). At least some of the training  examples should directly target cases where the prompted model is not behaving as desired,  and the provided assistant messages in the data should be the ideal responses you want the  model to provide. "
            },
            {
                "page_number": 4,
                "text": "### Example format In this example, our goal is to create a chatbot that occasionally gives sarcastic responses,  these are three training examples (conversations) we could create for a dataset: ```jsonl {'messages': [{'role': 'system', 'content': 'Marv is a factual chatbot that is also sarcastic.'},  {'role': 'user', 'content': 'What's the capital of France?'}, {'role': 'assistant', 'content': 'Paris,  as if everyone doesn't know that already.'}]} {'messages': [{'role': 'system', 'content': 'Marv is a factual chatbot that is also sarcastic.'},  {'role': 'user', 'content': 'Who wrote 'Romeo and Juliet'?'}, {'role': 'assistant', 'content': 'Oh,  just some guy named William Shakespeare. Ever heard of him?'}]} {'messages': [{'role': 'system', 'content': 'Marv is a factual chatbot that is also sarcastic.'},  {'role': 'user', 'content': 'How far is the Moon from Earth?'}, {'role': 'assistant', 'content':  'Around 384,400 kilometers. Give or take a few, like that really matters.'}]} ``` ### Multi-turn chat examples Examples in the chat format can have multiple messages with the assistant role. The default  behavior during fine-tuning is to train on all assistant messages within a single example. To skip  fine-tuning on specific assistant messages, a `weight` key can be added disable fine-tuning on  that message, allowing you to control which assistant messages are learned. The allowed  values for `weight` are currently 0 or 1. Some examples using `weight` for the chat format are  below. ```jsonl {'messages': [{'role': 'system', 'content': 'Marv is a factual chatbot that is also sarcastic.'},  {'role': 'user', 'content': 'What's the capital of France?'}, {'role': 'assistant', 'content': 'Paris',  'weight': 0}, {'role': 'user', 'content': 'Can you be more sarcastic?'}, {'role': 'assistant',  'content': 'Paris, as if everyone doesn't know that already.', 'weight': 1}]} {'messages': [{'role': 'system', 'content': 'Marv is a factual chatbot that is also sarcastic.'},  {'role': 'user', 'content': 'Who wrote 'Romeo and Juliet'?'}, {'role': 'assistant', 'content':  'William Shakespeare', 'weight': 0}, {'role': 'user', 'content': 'Can you be more sarcastic?'},  {'role': 'assistant', 'content': 'Oh, just some guy named William Shakespeare. Ever heard of  him?', 'weight': 1}]} {'messages': [{'role': 'system', 'content': 'Marv is a factual chatbot that is also sarcastic.'},  {'role': 'user', 'content': 'How far is the Moon from Earth?'}, {'role': 'assistant', 'content':  '384,400 kilometers', 'weight': 0}, {'role': 'user', 'content': 'Can you be more sarcastic?'}, "
            },
            {
                "page_number": 5,
                "text": "{'role': 'assistant', 'content': 'Around 384,400 kilometers. Give or take a few, like that really  matters.', 'weight': 1}]} ``` ### Crafting prompts We generally recommend taking the set of instructions and prompts that you found worked best  for the model prior to fine-tuning, and including them in every training example. This should let  you reach the best and most general results, especially if you have relatively few (e.g. under a  hundred) training examples. If you would like to shorten the instructions or prompts that are repeated in every example to  save costs, keep in mind that the model will likely behave as if those instructions were included,  and it may be hard to get the model to ignore those 'baked-in' instructions at inference time. It may take more training examples to arrive at good results, as the model has to learn entirely  through demonstration and without guided instructions. ### Example count recommendations To fine-tune a model, you are required to provide at least 10 examples. We typically see clear  improvements from fine-tuning on 50 to 100 training examples with `gpt-4o-mini` and `gpt-3.5- turbo`, but the right number varies greatly based on the exact use case. We recommend starting with 50 well-crafted demonstrations and seeing if the model shows  signs of improvement after fine-tuning. In some cases that may be sufficient, but even if the  model is not yet production quality, clear improvements are a good sign that providing more  data will continue to improve the model. No improvement suggests that you may need to rethink  how to set up the task for the model or restructure the data before scaling beyond a limited  example set. ### Train and test splits After collecting the initial dataset, we recommend splitting it into a training and test portion.  When submitting a fine-tuning job with both training and test files, we will provide statistics on  both during the course of training. These statistics will be your initial signal of how much the "
            },
            {
                "page_number": 6,
                "text": "model is improving. Additionally, constructing a test set early on will be useful in making sure  you are able to evaluate the model after training, by generating samples on the test set. ### Token limits Token limits depend on the model you select. Here is an overview of the maximum inference  context length and training examples context length for `gpt-4o-mini` and `gpt-3.5-turbo`  models: |Model|Inference context length|Training examples context length| |---|---|---| |gpt-4o-2024-08-06|128,000 tokens|65,536 tokens (128k coming soon)| |gpt-4o-mini-2024-07-18|128,000 tokens|65,536 tokens (128k coming soon)| |gpt-3.5-turbo-0125|16,385 tokens|16,385 tokens| |gpt-3.5-turbo-1106|16,385 tokens|16,385 tokens| |gpt-3.5-turbo-0613|16,385 tokens|4,096 tokens| Examples longer than the default will be truncated to the maximum context length which  removes tokens from the end of the training example(s). To be sure that your entire training  example fits in context, consider checking that the total token counts in the message contents  are under the limit. You can compute token counts using our [counting tokens  notebook](https://cookbook.openai.com/examples/How_to_count_tokens_with_tiktoken.ipynb ) from the OpenAI cookbook. ### Estimate costs For detailed pricing on training costs, as well as input and output costs for a deployed fine- tuned model, visit our [pricing page](https://openai.com/api/pricing). Note that we don't charge  for tokens used for training validation. To estimate the cost of a specific fine-tuning training job,  use the following formula: > (base training cost per 1M input tokens ÷ 1M) × number of tokens in the input file × number of  epochs trained "
            },
            {
                "page_number": 7,
                "text": "For a training file with 100,000 tokens trained over 3 epochs, the expected cost would be: *   ~$0.90 USD with `gpt-4o-mini-2024-07-18` after the free period ends on October 31, 2024. *   ~$2.40 USD with `gpt-3.5-turbo-0125`. ### Check data formatting Once you have compiled a dataset and before you create a fine-tuning job, it is important to  check the data formatting. To do this, we created a simple Python script which you can use to  find potential errors, review token counts, and estimate the cost of a fine-tuning job. [ Fine-tuning data format validation Learn about fine-tuning data formatting ](https://cookbook.openai.com/examples/chat_finetuning_data_prep) ### Upload a training file Once you have the data validated, the file needs to be uploaded using the [Files API](/docs/api- reference/files/create) in order to be used with a fine-tuning jobs: ```python from openai import OpenAI client = OpenAI() client.files.create( file=open('mydata.jsonl', 'rb'), purpose='fine-tune' "
            },
            {
                "page_number": 8,
                "text": ") ``` ```javascript import fs from 'fs'; import fetch from 'node-fetch'; import OpenAI, { toFile } from 'openai'; const openai = new OpenAI(); // If you have access to Node fs we recommend using fs.createReadStream(): await openai.files.create({ file: fs.createReadStream('mydata.jsonl'), purpose: 'fine-tune' }); // Or if you have the web File API you can pass a File instance: await openai.files.create({ file: new File(['my bytes'], 'mydata.jsonl'), purpose: 'fine-tune' }); // You can also pass a fetch Response: await openai.files.create({ file: await fetch('https://somesite/mydata.jsonl'), purpose: 'fine-tune'  }); ``` ```bash curl https://api.openai.com/v1/files -H 'Authorization: Bearer $OPENAI_API_KEY' -F purpose='fine-tune' -F file='@mydata.jsonl' ``` After you upload the file, it may take some time to process. While the file is processing, you can  still create a fine-tuning job but it will not start until the file processing has completed. "
            },
            {
                "page_number": 9,
                "text": "The maximum file upload size is 1 GB, though we do not suggest fine-tuning with that amount of  data since you are unlikely to need that large of an amount to see improvements. Vision fine-tuning ---------------------- Fine-tuning is also possible with images in your JSONL files. Just as you can [send one or many  image inputs to chat completions](/docs/guides/vision), you can include those same message  types within your training data. Images can be provided either as HTTP URLs or data URLs  containing [base64 encoded images](/docs/guides/vision#uploading-base-64-encoded- images). Here's an example of an image message on a line of your JSONL file. Below, the JSON object is  expanded for readibility, but typically this JSON would appear on a single line in your data file: ```json { 'messages': [ { 'role': 'system', 'content': 'You are an assistant that identifies uncommon cheeses.' }, { 'role': 'user', 'content': 'What is this cheese?' }, { 'role': 'user', 'content': [ { 'type': 'image_url', 'image_url': { 'url': 'https://upload.wikimedia.org/wikipedia/commons/3/36/Danbo_Cheese.jpg' } } ] }, { 'role': 'assistant', 'content': 'Danbo' } ] } "
            },
            {
                "page_number": 10,
                "text": "``` ### Image dataset requirements #### Size *   Your training file can contain a maximum of 50,000 examples that contain images (not  including text examples). *   Each example can have at most 10 images. *   Each image can be at most 10 MB. #### Format *   Images must be JPEG, PNG, or WEBP format. *   Your images must be in the RGB or RGBA image mode. *   You cannot include images as output from messages with the `assistant` role. #### Content moderation policy We scan your images before training to ensure that they comply with our usage policy. This may  introduce latency in file validation before fine tuning begins. Images containing the following will be excluded from your dataset and not used for training: *   People *   Faces *   Children *   CAPTCHAs ### Help #### What to do if your images get skipped "
            },
            {
                "page_number": 11,
                "text": "Your images can get skipped for the following reasons: *   **contains CAPTCHAs**, **contains people**, **contains faces**, **contains children** *   Remove the image. For now, we cannot fine-tune models with images containing these  entities. *   **inaccessible URL** *   Ensure that the image URL is publicly accessible. *   **image too large** *   Please ensure that your images fall within our [dataset size limits](#size). *   **invalid image format** *   Please ensure that your images fall within our [dataset format](#format). #### How to upload large files *   Your training files might get quite large. You can upload files up to 8 GB in multiple parts using  the [Uploads API](/docs/api-reference/uploads) as opposed to the [Files API](/docs/api- reference/files), which only allows file uploads of up to 512 MB. #### Reducing training cost If you set the `detail` parameter for an image to `low`, the image is resized to 512 by 512 pixels  and is only represented by 85 tokens regardless of its size. This will reduce the cost of training.  [See here for more information.](/docs/guides/vision#low-or-high-fidelity-image-understanding) ```text { 'type': 'image_url', 'image_url': { 'url': 'https://upload.wikimedia.org/wikipedia/commons/3/36/Danbo_Cheese.jpg', 'detail': 'low' } } "
            },
            {
                "page_number": 12,
                "text": "``` #### Other considerations for vision fine-tuning *   To control the fidelity of image understanding, set the `detail` parameter of `image_url` to  `low`, `high`, or `auto` for each image. This will also affect the number of tokens per image  that the model sees during training time, and will affect the cost of training. [See here for more  information.](/docs/guides/vision#low-or-high-fidelity-image-understanding) Create a fine-tuned model ------------------------- After ensuring you have the right amount and structure for your dataset, and have uploaded the  file, the next step is to create a fine-tuning job. We support creating fine-tuning jobs via the [fine- tuning UI](/finetune) or programmatically. To start a fine-tuning job using the OpenAI SDK: ```python from openai import OpenAI client = OpenAI() client.fine_tuning.jobs.create( training_file='file-abc123', model='gpt-4o-mini-2024-07-18' ) ``` ```javascript const fineTune = await openai.fineTuning.jobs.create({ training_file: 'file-abc123', model: 'gpt-4o-mini-2024-07-18' }); "
            },
            {
                "page_number": 13,
                "text": "``` In this example, `model` is the name of the model you want to fine-tune. Note that only specific  model snapshots (like `gpt-4o-mini-2024-07-18` in this case) can be used for this parameter,  as listed in our [supported models](#which-models-can-be-fine-tuned). The `training_file`  parameter is the file ID that was returned when the training file was uploaded to the OpenAI API.  You can customize your fine-tuned model's name using the [suffix parameter](/docs/api- reference/fine-tuning/create#fine-tuning/create-suffix). To set additional fine-tuning parameters like the `validation_file` or `hyperparameters`, please  refer to the [API specification for fine-tuning](/docs/api-reference/fine-tuning/create). After you've started a fine-tuning job, it may take some time to complete. Your job may be  queued behind other jobs in our system, and training a model can take minutes or hours  depending on the model and dataset size. After the model training is completed, the user who  created the fine-tuning job will receive an email confirmation. In addition to creating a fine-tuning job, you can also list existing jobs, retrieve the status of a  job, or cancel a job. ```python from openai import OpenAI client = OpenAI() # List 10 fine-tuning jobs client.fine_tuning.jobs.list(limit=10) # Retrieve the state of a fine-tune client.fine_tuning.jobs.retrieve('ftjob-abc123') # Cancel a job client.fine_tuning.jobs.cancel('ftjob-abc123') # List up to 10 events from a fine-tuning job client.fine_tuning.jobs.list_events(fine_tuning_job_id='ftjob-abc123', limit=10) "
            },
            {
                "page_number": 14,
                "text": "# Delete a fine-tuned model (must be an owner of the org the model was created in) client.models.delete('ft:gpt-3.5-turbo:acemeco:suffix:abc123') ``` ```javascript // List 10 fine-tuning jobs let page = await openai.fineTuning.jobs.list({ limit: 10 }); // Retrieve the state of a fine-tune let fineTune = await openai.fineTuning.jobs.retrieve('ftjob-abc123'); // Cancel a job let status = await openai.fineTuning.jobs.cancel('ftjob-abc123'); // List up to 10 events from a fine-tuning job let events = await openai.fineTuning.jobs.listEvents(fineTune.id, { limit: 10 }); // Delete a fine-tuned model (must be an owner of the org the model was created in) let model = await openai.models.delete('ft:gpt-3.5-turbo:acemeco:suffix:abc123'); ``` Use a fine-tuned model ---------------------- When a job has succeeded, you will see the `fine_tuned_model` field populated with the name  of the model when you retrieve the job details. You may now specify this model as a parameter  to in the [Chat Completions](/docs/api-reference/chat) API, and make requests to it using the  [Playground](/playground). After your job is completed, the model should be available right away for inference use. In some  cases, it may take several minutes for your model to become ready to handle requests. If "
            },
            {
                "page_number": 15,
                "text": "requests to your model time out or the model name cannot be found, it is likely because your  model is still being loaded. If this happens, try again in a few minutes. ```python from openai import OpenAI client = OpenAI() completion = client.chat.completions.create( model='ft:gpt-4o-mini:my-org:custom_suffix:id', messages=[ {'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Hello!'} ] ) print(completion.choices[0].message) ``` ```javascript async function main() { const completion = await openai.chat.completions.create({ messages: [{ role: 'system', content: 'You are a helpful assistant.' }], model: 'ft:gpt-4o-mini:my-org:custom_suffix:id', }); console.log(completion.choices[0]); } main(); ``` You can start making requests by passing the model name as shown above and in our [GPT  guide](/docs/guides/text-generation). Use a checkpointed model "
            },
            {
                "page_number": 16,
                "text": "------------------------ In addition to creating a final fine-tuned model at the end of each fine-tuning job, OpenAI will  create one full model checkpoint for you at the end of each training epoch. These checkpoints  are themselves full models that can be used within our completions and chat-completions  endpoints. Checkpoints are useful as they potentially provide a version of your fine-tuned model  from before it experienced overfitting. To access these checkpoints, 1.  Wait until a job succeeds, which you can verify by [querying the status of a job.](/docs/api- reference/fine-tuning/retrieve) 2.  [Query the checkpoints endpoint](/docs/api-reference/fine-tuning/list-checkpoints) with  your fine-tuning job ID to access a list of model checkpoints for the fine-tuning job. For each checkpoint object, you will see the `fine_tuned_model_checkpoint` field populated  with the name of the model checkpoint. You may now use this model just like you would with  the [final fine-tuned model](#use-a-fine-tuned-model). ```json { 'object': 'fine_tuning.job.checkpoint', 'id': 'ftckpt_zc4Q7MP6XxulcVzj4MZdwsAB', 'created_at': 1519129973, 'fine_tuned_model_checkpoint': 'ft:gpt-3.5-turbo-0125:my-org:custom-suffix:96olL566:ckpt- step-2000', 'metrics': { 'full_valid_loss': 0.134, 'full_valid_mean_token_accuracy': 0.874 }, 'fine_tuning_job_id': 'ftjob-abc123', 'step_number': 2000 } ``` "
            },
            {
                "page_number": 17,
                "text": "Each checkpoint will specify its: *   `step_number`: The step at which the checkpoint was created (where each epoch is number  of steps in the training set divided by the batch size) *   `metrics`: an object containing the metrics for your fine-tuning job at the step when the  checkpoint was created. Currently, only the checkpoints for the last 3 epochs of the job are saved and available for use.  We plan to release more complex and flexible checkpointing strategies in the near future. Analyzing your fine-tuned model ------------------------------- We provide the following training metrics computed over the course of training: *   training loss *   training token accuracy *   valid loss *   valid token accuracy Valid loss and valid token accuracy are computed in two different ways - on a small batch of the  data during each step, and on the full valid split at the end of each epoch. The full valid loss and  full valid token accuracy metrics are the most accurate metric tracking the overall performance  of your model. These statistics are meant to provide a sanity check that training went smoothly  (loss should decrease, token accuracy should increase). While an active fine-tuning jobs is  running, you can view an event object which contains some useful metrics: ```json { 'object': 'fine_tuning.job.event', 'id': 'ftevent-abc-123', 'created_at': 1693582679, 'level': 'info', "
            },
            {
                "page_number": 18,
                "text": "'message': 'Step 300/300: training loss=0.15, validation loss=0.27, full validation loss=0.40', 'data': { 'step': 300, 'train_loss': 0.14991648495197296, 'valid_loss': 0.26569826706596045, 'total_steps': 300, 'full_valid_loss': 0.4032616495084362, 'train_mean_token_accuracy': 0.9444444179534912, 'valid_mean_token_accuracy': 0.9565217391304348, 'full_valid_mean_token_accuracy': 0.9089635854341737 }, 'type': 'metrics' } ``` After a fine-tuning job has finished, you can also see metrics around how the training process  went by [querying a fine-tuning job](/docs/api-reference/fine-tuning/retrieve), extracting a file ID  from the `result_files`, and then [retrieving that files content](/docs/api- reference/files/retrieve-contents). Each results CSV file has the following columns: `step`,  `train_loss`, `train_accuracy`, `valid_loss`, and `valid_mean_token_accuracy`. ```csv step,train_loss,train_accuracy,valid_loss,valid_mean_token_accuracy 1,1.52347,0.0,, 2,0.57719,0.0,, 3,3.63525,0.0,, 4,1.72257,0.0,, 5,1.52379,0.0,, ``` While metrics can be helpful, evaluating samples from the fine-tuned model provides the most  relevant sense of model quality. We recommend generating samples from both the base model  and the fine-tuned model on a test set, and comparing the samples side by side. The test set  should ideally include the full distribution of inputs that you might send to the model in a "
            },
            {
                "page_number": 19,
                "text": "production use case. If manual evaluation is too time-consuming, consider using our [Evals  library](https://github.com/openai/evals) to automate future evaluations. ### Iterating on data quality If the results from a fine-tuning job are not as good as you expected, consider the following ways  to adjust the training dataset: *   Collect examples to target remaining issues *   If the model still isn’t good at certain aspects, add training examples that directly show the  model how to do these aspects correctly *   Scrutinize existing examples for issues *   If your model has grammar, logic, or style issues, check if your data has any of the same  issues. For instance, if the model now says 'I will schedule this meeting for you' (when it  shouldn’t), see if existing examples teach the model to say it can do new things that it can’t do *   Consider the balance and diversity of data *   If 60% of the assistant responses in the data says 'I cannot answer this', but at inference  time only 5% of responses should say that, you will likely get an overabundance of refusals *   Make sure your training examples contain all of the information needed for the response *   If we want the model to compliment a user based on their personal traits and a training  example includes assistant compliments for traits not found in the preceding conversation, the  model may learn to hallucinate information *   Look at the agreement / consistency in the training examples *   If multiple people created the training data, it’s likely that model performance will be  limited by the level of agreement / consistency between people. For instance, in a text  extraction task, if people only agreed on 70% of extracted snippets, the model would likely not  be able to do better than this *   Make sure your all of your training examples are in the same format, as expected for inference ### Iterating on data quantity Once you’re satisfied with the quality and distribution of the examples, you can consider scaling  up the number of training examples. This tends to help the model learn the task better,  especially around possible 'edge cases'. We expect a similar amount of improvement every  time you double the number of training examples. You can loosely estimate the expected quality  gain from increasing the training data size by: "
            },
            {
                "page_number": 20,
                "text": "*   Fine-tuning on your current dataset *   Fine-tuning on half of your current dataset *   Observing the quality gap between the two In general, if you have to make a trade-off, a smaller amount of high-quality data is generally  more effective than a larger amount of low-quality data. ### Iterating on hyperparameters We allow you to specify the following hyperparameters: *   epochs *   learning rate multiplier *   batch size We recommend initially training without specifying any of these, allowing us to pick a default for  you based on dataset size, then adjusting if you observe the following: *   If the model does not follow the training data as much as expected increase the number of  epochs by 1 or 2 *   This is more common for tasks for which there is a single ideal completion (or a small set of  ideal completions which are similar). Some examples include classification, entity extraction, or  structured parsing. These are often tasks for which you can compute a final accuracy metric  against a reference answer. *   If the model becomes less diverse than expected decrease the number of epochs by 1 or 2 *   This is more common for tasks for which there are a wide range of possible good  completions *   If the model does not appear to be converging, increase the learning rate multiplier You can set the hyperparameters as is shown below: ```python from openai import OpenAI "
            },
            {
                "page_number": 21,
                "text": "client = OpenAI() client.fine_tuning.jobs.create( training_file='file-abc123', model='gpt-4o-mini-2024-07-18', hyperparameters={ 'n_epochs':2 } ) ``` ```javascript const fineTune = await openai.fineTuning.jobs.create({ training_file: 'file-abc123', model: 'gpt-4o-mini-2024-07-18', hyperparameters: { n_epochs: 2 } }); ``` Fine-tuning examples -------------------- Now that we have explored the basics of the fine-tuning API, let’s look at going through the fine- tuning lifecycle for a few different use cases. Style and tone In this example, we will explore how to build a fine-tuned model which gets the model follow  specific style and tone guidance beyond what is possible with prompting alone. To begin, we create a sample set of messages showing what the model should which in this  case is misspelled words. "
            },
            {
                "page_number": 22,
                "text": "```jsonl {'messages': [{'role': 'system', 'content': 'Marv is a factual chatbot that is also sarcastic.'},  {'role': 'user', 'content': 'What's the capital of France?'}, {'role': 'assistant', 'content': 'Paris,  as if everyone doesn't know that already.'}]} {'messages': [{'role': 'system', 'content': 'Marv is a factual chatbot that is also sarcastic.'},  {'role': 'user', 'content': 'Who wrote 'Romeo and Juliet'?'}, {'role': 'assistant', 'content': 'Oh,  just some guy named William Shakespeare. Ever heard of him?'}]} {'messages': [{'role': 'system', 'content': 'Marv is a factual chatbot that is also sarcastic.'},  {'role': 'user', 'content': 'How far is the Moon from Earth?'}, {'role': 'assistant', 'content':  'Around 384,400 kilometers. Give or take a few, like that really matters.'}]} ``` If you want to follow along and create a fine-tuned model yourself, you will need at least 10  examples. After getting the data that will potentially improve the model, the next step is to check if the data  meets all the [formatting requirements](#check-data-formatting). Now that we have the data formatted and validated, the final training step is to kick off a job to  create the fine-tuned model. You can do this via the OpenAI CLI or one of our SDKs as shown  below: ```python from openai import OpenAI client = OpenAI() file = client.files.create( file=open('marv.jsonl', 'rb'), purpose='fine-tune' ) client.fine_tuning.jobs.create( training_file=file.id, model='gpt-4o-mini-2024-07-18' "
            },
            {
                "page_number": 23,
                "text": ") ``` Once the training job is done, you will be able to [use your fine-tuned model](#use-a-fine-tuned- model). Structured output Another type of use case which works really well with fine-tuning is getting the model to provide  structured information, in this case about sports headlines: ```jsonl {'messages': [{'role': 'system', 'content': 'Given a sports headline, provide the following fields  in a JSON dict, where applicable: 'player' (full name), 'team', 'sport', and 'gender'.'},  {'role': 'user', 'content': 'Sources: Colts grant RB Taylor OK to seek trade'}, {'role': 'assistant',  'content': '{'player': 'Jonathan Taylor', 'team': 'Colts', 'sport': 'football', 'gender':  'male' }'}]} {'messages': [{'role': 'system', 'content': 'Given a sports headline, provide the following fields  in a JSON dict, where applicable: 'player' (full name), 'team', 'sport', and 'gender'.'},  {'role': 'user', 'content': 'OSU 'split down middle' on starting QB battle'}, {'role': 'assistant',  'content': '{'player': null, 'team': 'OSU', 'sport': 'football', 'gender': null }'}]} ``` If you want to follow along and create a fine-tuned model yourself, you will need at least 10  examples. After getting the data that will potentially improve the model, the next step is to check if the data  meets all the [formatting requirements](#check-data-formatting). Now that we have the data formatted and validated, the final training step is to kick off a job to  create the fine-tuned model. You can do this via the OpenAI CLI or one of our SDKs as shown  below: ```python from openai import OpenAI client = OpenAI() "
            },
            {
                "page_number": 24,
                "text": "file = client.files.create( file=open('sports-context.jsonl', 'rb'), purpose='fine-tune' ) client.fine_tuning.jobs.create( training_file=file.id, model='gpt-4o-mini-2024-07-18' ) ``` Once the training job is done, you will be able to [use your fine-tuned model](#use-a-fine-tuned- model) and make a request that looks like the following: ```python completion = client.chat.completions.create( model='ft:gpt-4o-mini:my-org:custom_suffix:id', messages=[ {'role': 'system', 'content': 'Given a sports headline, provide the following fields in a JSON  dict, where applicable: player (full name), team, sport, and gender'}, {'role': 'user', 'content': 'Richardson wins 100m at worlds to cap comeback'} ] ) print(completion.choices[0].message) ``` Based on the formatted training data, the response should look like the following: ```json { "
            },
            {
                "page_number": 25,
                "text": "'player': 'Sha'Carri Richardson', 'team': null, 'sport': 'track and field', 'gender': 'female' } ``` Tool calling The chat completions API supports [tool calling](/docs/guides/function-calling). Including a long  list of tools in the completions API can consume a considerable number of prompt tokens and  sometimes the model hallucinates or does not provide valid JSON output. Fine-tuning a model with tool calling examples can allow you to: *   Get similarly formatted responses even when the full tool definition isn't present *   Get more accurate and consistent outputs Format your examples as shown, with each line including a list of 'messages' and an optional  list of 'tools': ```json { 'messages': [ { 'role': 'user', 'content': 'What is the weather in San Francisco?' }, { 'role': 'assistant', 'tool_calls': [ { 'id': 'call_id', 'type': 'function', 'function': { "
            },
            {
                "page_number": 26,
                "text": "'name': 'get_current_weather', 'arguments': '{'location': 'San Francisco, USA', 'format': 'celsius'}' } } ] } ], 'tools': [ { 'type': 'function', 'function': { 'name': 'get_current_weather', 'description': 'Get the current weather', 'parameters': { 'type': 'object', 'properties': { 'location': { 'type': 'string', 'description': 'The city and country, eg. San Francisco, USA' }, 'format': { 'type': 'string', 'enum': ['celsius', 'fahrenheit'] } }, 'required': ['location', 'format'] } } } ] } ``` "
            },
            {
                "page_number": 27,
                "text": "If you want to follow along and create a fine-tuned model yourself, you will need at least 10  examples. If your goal is to use less tokens, some useful techniques are: *   Omit function and parameter descriptions: remove the description field from function and  parameters *   Omit parameters: remove the entire properties field from the parameters object *   Omit function entirely: remove the entire function object from the functions array If your goal is to maximize the correctness of the function calling output, we recommend using  the same tool definitions for both training and querying the fine-tuned model. Fine-tuning on function calling can also be used to customize the model's response to function  outputs. To do this you can include a function response message and an assistant message  interpreting that response: ```json { 'messages': [ {'role': 'user', 'content': 'What is the weather in San Francisco?'}, {'role': 'assistant', 'tool_calls': [{'id': 'call_id', 'type': 'function', 'function': {'name':  'get_current_weather', 'arguments': '{'location': 'San Francisco, USA', 'format':  'celsius'}'}}]} {'role': 'tool', 'tool_call_id': 'call_id', 'content': '21.0'}, {'role': 'assistant', 'content': 'It is 21 degrees celsius in San Francisco, CA'} ], 'tools': [...] // same as before } ``` [Parallel function calling](/docs/guides/function-calling) is enabled by default and can be  disabled by using `parallel_tool_calls: false` in the training example. "
            },
            {
                "page_number": 28,
                "text": "Function calling `function_call` and `functions` have been deprecated in favor of `tools` it is recommended to  use the `tools` parameter instead. The chat completions API supports [function calling](/docs/guides/function-calling). Including a  long list of functions in the completions API can consume a considerable number of prompt  tokens and sometimes the model hallucinates or does not provide valid JSON output. Fine-tuning a model with function calling examples can allow you to: *   Get similarly formatted responses even when the full function definition isn't present *   Get more accurate and consistent outputs Format your examples as shown, with each line including a list of 'messages' and an optional  list of 'functions': ```json { 'messages': [ { 'role': 'user', 'content': 'What is the weather in San Francisco?' }, { 'role': 'assistant', 'function_call': { 'name': 'get_current_weather', 'arguments': '{'location': 'San Francisco, USA', 'format': 'celsius'}' } } ], 'functions': [ { 'name': 'get_current_weather', "
            },
            {
                "page_number": 29,
                "text": "'description': 'Get the current weather', 'parameters': { 'type': 'object', 'properties': { 'location': { 'type': 'string', 'description': 'The city and country, eg. San Francisco, USA' }, 'format': { 'type': 'string', 'enum': ['celsius', 'fahrenheit'] } }, 'required': ['location', 'format'] } } ] } ``` If you want to follow along and create a fine-tuned model yourself, you will need at least 10  examples. If your goal is to use less tokens, some useful techniques are: *   Omit function and parameter descriptions: remove the description field from function and  parameters *   Omit parameters: remove the entire properties field from the parameters object *   Omit function entirely: remove the entire function object from the functions array If your goal is to maximize the correctness of the function calling output, we recommend using  the same function definitions for both training and querying the fine-tuned model. Fine-tuning on function calling can also be used to customize the model's response to function  outputs. To do this you can include a function response message and an assistant message  interpreting that response: "
            },
            {
                "page_number": 30,
                "text": "```json { 'messages': [ {'role': 'user', 'content': 'What is the weather in San Francisco?'}, {'role': 'assistant', 'function_call': {'name': 'get_current_weather', 'arguments':  '{'location': 'San Francisco, USA', 'format': 'celsius'}'}} {'role': 'function', 'name': 'get_current_weather', 'content': '21.0'}, {'role': 'assistant', 'content': 'It is 21 degrees celsius in San Francisco, CA'} ], 'functions': [...] // same as before } ``` Fine-tuning integrations ======================== OpenAI provides the ability for you to integrate your fine-tuning jobs with 3rd parties via our  integration framework. Integrations generally allow you to track job state, status, metrics,  hyperparameters, and other job-related information in a 3rd party system. You can also use  integrations to trigger actions in a 3rd party system based on job state changes. Currently, the  only supported integration is with [Weights and Biases](https://wandb.ai), but more are coming  soon. Weights and Biases Integration ------------------------------ [Weights and Biases (W&B)](https://wandb.ai) is a popular tool for tracking machine learning  experiments. You can use the OpenAI integration with W&B to track your fine-tuning jobs in  W&B. This integration will automatically log metrics, hyperparameters, and other job-related  information to the W&B project you specify. To integrate your fine-tuning jobs with W&B, you'll need to "
            },
            {
                "page_number": 31,
                "text": "1.  Provide authentication credentials for your Weights and Biases account to OpenAI 2.  Configure the W&B integration when creating new fine-tuning jobs ### Authenticate your Weights and Biases account with OpenAI Authentication is done by submitting a valid W&B API key to OpenAI. Currently, this can only be  done via the [Account Dashboard](/settings/organization/organization), and only by account  administrators. Your W&B API key will be stored encrypted within OpenAI and will allow OpenAI  to post metrics and metadata on your behalf to W&B when your fine-tuning jobs are running.  Attempting to enable a W&B integration on a fine-tuning job without first authenticating your  OpenAI organization with WandB will result in an error. ![](https://cdn.openai.com/API/images/guides/WandB_Integration.png) ### Enable the Weights and Biases integration When creating a new fine-tuning job, you can enable the W&B integration by including a new  `'wandb'` integration under the `integrations` field in the job creation request. This integration  allows you to specify the W&B Project that you wish the newly created W&B Run to show up  under. Here's an example of how to enable the W&B integration when creating a new fine-tuning job: ```bash curl -X POST H 'Content-Type: application/json' H 'Authorization: Bearer $OPENAI_API_KEY' d '{ 'model': 'gpt-4o-mini-2024-07-18', 'training_file': 'file-ABC123', 'validation_file': 'file-DEF456', 'integrations': [ { 'type': 'wandb', "
            },
            {
                "page_number": 32,
                "text": "'wandb': { 'project': 'custom-wandb-project', 'tags': ['project:tag', 'lineage'] } } ] }' https://api.openai.com/v1/fine_tuning/jobs ``` By default, the Run ID and Run display name are the ID of your fine-tuning job (e.g. `ftjob- abc123`). You can customize the display name of the run by including a `'name'` field in the  `wandb` object. You can also include a `'tags'` field in the `wandb` object to add tags to the  W&B Run (tags must be <= 64 character strings and there is a maximum of 50 tags). Sometimes it is convenient to explicitly set the [W&B  Entity](https://docs.wandb.ai/guides/runs/manage-runs#send-new-runs-to-a-team) to be  associated with the run. You can do this by including an `'entity'` field in the `wandb` object. If  you do not include an `'entity'` field, the W&B entity will default to the default W&B entity  associated with the API key you registered previously. The full specification for the integration can be found in our [fine-tuning job creation](/docs/api- reference/fine-tuning/create) documentation. ### View your fine-tuning job in Weights and Biases Once you've created a fine-tuning job with the W&B integration enabled, you can view the job in  W&B by navigating to the W&B project you specified in the job creation request. Your run should  be located at the URL: `https://wandb.ai/<WANDB-ENTITY>/<WANDB-PROJECT>/runs/ftjob- ABCDEF`. You should see a new run with the name and tags you specified in the job creation request. The  Run Config will contain relevant job metadata such as: *   `model`: The model you are fine-tuning *   `training_file`: The ID of the training file "
            },
            {
                "page_number": 33,
                "text": "*   `validation_file`: The ID of the validation file *   `hyperparameters`: The hyperparameters used for the job (e.g. `n_epochs`,  `learning_rate_multiplier`, `batch_size`) *   `seed`: The random seed used for the job Likewise, OpenAI will set some default tags on the run to make it easier for your to search and  filter. These tags will be prefixed with `'openai/'` and will include: *   `openai/fine-tuning`: Tag to let you know this run is a fine-tuning job *   `openai/ft-abc123`: The ID of the fine-tuning job *   `openai/gpt-4o-mini`: The model you are fine-tuning An example W&B run generated from an OpenAI fine-tuning job is shown below: ![](https://cdn.openai.com/API/images/guides/WandB_Integration_Dashboard1.png) Metrics for each step of the fine-tuning job will be logged to the W&B run. These metrics are the  same metrics provided in the [fine-tuning job event](/docs/api-reference/fine-tuning/list-events)  object and are the same metrics your can view via the [OpenAI fine-tuning  Dashboard](https://platform.openai.com/finetune). You can use W&B's visualization tools to  track the progress of your fine-tuning job and compare it to other fine-tuning jobs you've run. An example of the metrics logged to a W&B run is shown below: ![](https://cdn.openai.com/API/images/guides/WandB_Integration_Dashboard2.png) FAQ --- ### When should I use fine-tuning vs embeddings / retrieval augmented generation? Embeddings with retrieval is best suited for cases when you need to have a large database of  documents with relevant context and information. "
            },
            {
                "page_number": 34,
                "text": "By default OpenAI’s models are trained to be helpful generalist assistants. Fine-tuning can be  used to make a model which is narrowly focused, and exhibits specific ingrained behavior  patterns. Retrieval strategies can be used to make new information available to a model by  providing it with relevant context before generating its response. Retrieval strategies are not an  alternative to fine-tuning and can in fact be complementary to it. You can explore the differences between these options further in this Developer Day talk: ### How do I know if my fine-tuned model is actually better than the base model? We recommend generating samples from both the base model and the fine-tuned model on a  test set of chat conversations, and comparing the samples side by side. For more  comprehensive evaluations, consider using the [OpenAI evals  framework](https://github.com/openai/evals) to create an eval specific to your use case. ### Can I continue fine-tuning a model that has already been fine-tuned? Yes, you can pass the name of a fine-tuned model into the `model` parameter when creating a  fine-tuning job. This will start a new fine-tuning job using the fine-tuned model as the starting  point. ### How can I estimate the cost of fine-tuning a model? Please refer to the [estimate cost](#estimate-costs) section above. ### How many fine-tuning jobs can I have running at once? Please refer to our [rate limit page](/docs/guides/rate-limits#what-are-the-rate-limits-for-our- api) for the most up to date information on the limits. ### How do rate limits work on fine-tuned models? A fine-tuned model pulls from the same shared rate limit as the model it is based off of. For  example, if you use half your TPM rate limit in a given time period with the standard `gpt-4o- mini` model, any model(s) you fine-tuned from `gpt-4o-mini` would only have the remaining "
            },
            {
                "page_number": 35,
                "text": "half of the TPM rate limit accessible since the capacity is shared across all models of the same  type. Put another way, having fine-tuned models does not give you more capacity to use our models  from a total throughput perspective. "
            }
        ],
        "images": []
    },
    {
        "file_name": "knowledge_graph.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Introduction Knowledge graphs (KGs) organise data from multiple sources, capture information about  entities of interest in a given domain or task (like people, places or events), and forge  connections between them. In data science and AI, knowledge graphs are commonly used to:    Facilitate access to and integration of data sources;    Add context and depth to other, more data-driven AI techniques such as machine  learning; and    Serve as bridges between humans and systems, such as generating human-readable  explanations, or, on a bigger scale, enabling intelligent systems for scientists and  engineers. This interest group will facilitate research and innovation in a critical area of data science and  AI. Explaining the science The term ‘knowledge graph’ has been introduced by Google in 2012 to refer to its general- purpose knowledge base, though similar approaches have been around since the beginning of  modern AI in areas such as knowledge representation, knowledge acquisition, natural language  processing, ontology engineering and the semantic web. Today, KGs are used extensively in  anything from search engines and chatbots to product recommenders and autonomous  systems. In data science, common use cases are around adding identifiers and descriptions to  data of various modalities to enable sense-making, integration, and explainable analysis.  In AI,  knowledge graphs complement machine learning techniques to:    reduce the need of large, labelled datasets;    facilitate transfer learning and explainability;    encode domain, task and application knowledge that would be costly to learn from data  alone. A knowledge graph organises and integrates data according to an ontology, which is called the  schema of the knowledge graph, and applies a reasoner to derive new knowledge. Knowledge  graphs can be created from scratch, e.g., by domain experts, learned from unstructured or  semi-structured data sources, or assembled from existing knowledge graphs, typically aided by  various semi-automatic or automated data validation and integration mechanisms. While there are many definitions of knowledge graphs around, most of them agree that  knowledge graphs are:    Graphs: unlike knowledge bases, the content of KGs is organised as a graph, where  nodes (entities of interest and their types), relationships between and attributes of the  nodes are equally important. This makes it easy to integrate new datasets and formats  and supports exploration by navigating from one part of the graph to the other through  links.    Semantic: the meaning of the data is encoded for programmatic use in an ontology,  which describes the types of entities in the graph and their characteristics and can be "
            },
            {
                "page_number": 2,
                "text": "represented as a schema sub-graph. This means that the graph is both a place to  organise and store data, and to reason what it is about and derive new information.    Alive: knowledge graphs are flexible in terms of the types of data and schemas they can  support. They, including their schemas, evolve to reflect changes in the domain and new  data is added to the graph as it becomes available. Some knowledge graphs are used primarily within the organisation that created them. The most  common example is the Google knowledge graph, which is used in web search, or Amazon’s  product graph. Other knowledge graphs are openly available. These include DBpedia, Wikidata,  WordNet, Geonames, etc. Aims The UK has been at the forefront of several strands of work that have made KGs as successful as  they are today, in areas like: knowledge representation formalisms; natural language  processing; machine learning; methodologies to construct, learn and manage ontologies and  knowledge bases; de facto standard ontologies and vocabularies; scalable reasoning, linked  data etc. The main objectives of the interest group are:    To strengthen this national community of scholars and innovators to continue to pursue  world-leading research and explore novel technologies related to and applications of  KGs,    To reinforce the systematic use of KGs in practical data science and AI applications.    To identify a portfolio of joint research projects for current and future PG students. The group will encourage members to:    Present latest ideas and achievements;    Share ideas, knowledge and experiences;    Forge collaborations;    Align and expand existing education and training activities;    Reflect upon and raise awareness of specific challenges in equality, diversity and  inclusion in the field; and    Pool resources and expertise to collectively unlock new funding and partnership  opportunities that are not accessible to members in isolation. Talking points Constructing and maintaining large-scale, yet high-quality knowledge graphs Modern knowledge graphs are the result of complex assemblies of manual and automatic  modelling and data ingestion pipelines. Staying on top of these processes while ensuring that  the information remains up to date, consistent and trustworthy requires specialised socio- technical methods ranging from knowledge acquisition to natural language processing to  machine learning and human-computer interaction. "
            },
            {
                "page_number": 3,
                "text": "Knowledge-enhanced data-driven technologies Knowledge graphs are never used in isolation. They co-exist and complement data-driven  solutions, including natural language processing (text understanding, knowledge extraction, text  generation) and data management (semantic labelling, record linking, data cleansing etc.).  Recent research looks, for instance, at trade-offs between semantic representations and deep  learning, and, more broadly at designing symbolic/connectionist AI architectures. New forms of knowledge graphs Knowledge graphs need to be as rich as the AIs they serve. We need new ideas and formalisms  to go beyond the types of knowledge captured in current open and enterprise KGs, which tend to  focus on entities and their properties and on factual knowledge. New areas include common- sense knowledge, multiple viewpoints, as well as events and other types of temporal  information, and cause-effect chains. Knowledge graphs in human-AI systems Knowledge graphs are at the core of many human-facing technologies, such as search, question  answering, dialogue and recommenders. It is important to acknowledge the requirements each  of these scenarios pose to how graphs are created, maintained and used. Applications of knowledge graphs Many organisations, such as healthcare and financial service providers, are faced with data  silos across their organisational units. Knowledge graphs can help with, but not limited to, data  governance, fraud detection, knowledge management, search, chatbot, recommendation, as  well as intelligent systems across different organisational units. "
            }
        ],
        "images": []
    },
    {
        "file_name": "knowledge_graph2.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "What is a knowledge graph? A knowledge graph, also known as a semantic network, represents a network of real-world  entities—such as objects, events, situations or concepts—and illustrates the relationship  between them. This information is usually stored in a graph database and visualized as a graph  structure, prompting the term knowledge “graph.” A knowledge graph is made up of three main components: nodes, edges, and labels. Any object,  place, or person can be a node. An edge defines the relationship between the nodes. For  example, a node could be a client, like IBM, and an agency like, Ogilvy. An edge would be  categorize the relationship as a customer relationship between IBM and Ogilvy. A represents the subject, B represents the predicate, C represents the object It is also worth noting that the definitions of knowledge graphs vary and there is research (link  resides outside ibm.com), which suggests that a knowledge graph is no different than a  knowledge base or an ontology. Instead, it argues that the term was popularized by the Google’s  Knowledge Graph in 2012. Ontologies Ontologies are also frequently mentioned in the context of knowledge graphs, but again, there is  still debate around how they differ from knowledge graphs. Ultimately, ontologies serve to  create a formal representation of the entities in the graph. They are usually based on a  taxonomy, but since they can contain multiple taxonomies, it maintains its own separate  definition. Since knowledge graphs and ontologies are represented in a similar manner—i.e.  through nodes and edges—and are based on the Resource Description Framework (RDF)  triples, they tend to resemble each other in visualizations. An example of an ontology might be if we examine a particular venue, like Madison Square  Garden. An ontology distinguishes between the events at that location using a variable such as  time. A sports team, like the New York Rangers, has a series of games within a season that will  be hosted in that arena. They are all hockey games, and they are all located in the same venue.  However, each event is distinguished by their date and time. The Web Ontology Language (OWL) is an example of a widely adopted ontology, that is  supported by the World Wide Web Consortium (W3C), an international community that  champions open standards for the longevity of the internet.  Ultimately, this organization of  knowledge is supported by technological infrastructure such as databases, APIs, and machine  learning algorithms, which exist to help people and services to access and process information  more efficiently. How a knowledge graph works Knowledge graphs are typically made up of datasets from various sources, which frequently  differ in structure. Schemas, identities and context work together to provide structure to diverse  data. Schemas provide the framework for the knowledge graph, identities classify the underlying  nodes appropriately, and the context determines the setting in which that knowledge exists.  These components help distinguish words with multiple meanings. This allows products, like  Google’s search engine algorithm, to determine the difference between Apple, the brand, and  apple, the fruit. "
            },
            {
                "page_number": 2,
                "text": "Knowledge graphs, that are fueled by machine learning, utilize natural language processing  (NLP) to construct a comprehensive view of nodes, edges, and labels through a process called  semantic enrichment. When data is ingested, this process allows knowledge graphs to identify  individual objects and understand the relationships between different objects. This working  knowledge is then compared and integrated with other datasets, which are relevant and similar  in nature. Once a knowledge graph is complete, it allows question answering and search  systems to retrieve and reuse comprehensive answers to given queries. While consumer facing  products demonstrate its ability to save time, the same systems can also be applied in a  business setting, eliminating manual data collection and integration work to support business  decision-making. The data integration efforts around knowledge graphs can also support the creation of new  knowledge, establishing connections between data points that may not have been realized  before. Use cases of knowledge graphs There are a number of popular, consumer-facing knowledge graphs, which are setting user  expectations for search systems across enterprises. Some of these knowledge graphs include:    DBPedia and Wikidata are two different knowledge graphs for data on Wikipedia.org.  DBPedia is comprised of data from the infoboxes of Wikipedia while Wikidata focuses on  secondary and tertiary objects. Both typically publish in a RDF format.    Google Knowledge Graph is represented through Google Search Engine Results Pages  (SERPs), serving information based on what people search. This knowledge graph is  comprised of over 500 million objects, sourcing data from Freebase, Wikipedia, the CIA  World Factbook, and more. However, knowledge graphs also have applications in other industries, such as:    Retail: Knowledge graphs have been for up-sell and cross-sell strategies,  recommending products based on individual purchase behavior and popular purchase  trends across demographic groups.    Entertainment: Knowledge graphs are also leveraged for artificial intelligence (AI) based  recommendation engines for content platforms, like Netflix, SEO, or social media.  Based on click and other online engagement behaviors, these providers recommend  new content for users to read or watch.    Finance: This technology has also been used for know-your-customer (KYC) and anti- money laundering initiatives within the finance industry. They assist in financial crime  prevention and investigation, allowing banking institutions to understand the flow of  money across their clientele and identify noncompliant customers.    Healthcare: Knowledge graphs are also benefiting the healthcare industry by organizing  and categorizing relationships within medical research. This information assists  providers by validating diagnoses and identifying treatment plans based on individual  needs. "
            }
        ],
        "images": []
    },
    {
        "file_name": "machine_learning.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "What is machine learning? Machine learning (ML) is a branch of  and computer science that focuses on the using data and  algorithms to enable AI to imitate the way that humans learn, gradually improving its accuracy. How does machine learning work? (link resides outside ibm.com) breaks out the learning system of a machine learning algorithm  into three main parts. 1. A Decision Process: In general, machine learning algorithms are used to make a prediction or classification. Based on some input data, which can be labeled or  unlabeled, your algorithm will produce an estimate about a pattern in the data. 2. An Error Function: An error function evaluates the prediction of the model. If there are known examples, an error function can make a comparison to assess the accuracy of  the model. 3. A Model Optimization Process: If the model can fit better to the data points in the training set, then weights are adjusted to reduce the discrepancy between the known  example and the model estimate. The algorithm will repeat this iterative “evaluate and  optimize” process, updating weights autonomously until a threshold of accuracy has  been met. Machine learning versus deep learning versus neural networks Since deep learning and machine learning tend to be used interchangeably, it’s worth noting the  nuances between the two. Machine learning, deep learning, and neural networks are all sub- fields of artificial intelligence. However, neural networks is actually a sub-field of machine  learning, and deep learning is a sub-field of neural networks. The way in which deep learning and machine learning differ is in how each algorithm learns.  'Deep' machine learning can use labeled datasets, also known as supervised learning, to  inform its algorithm, but it doesn’t necessarily require a labeled dataset. The deep learning  process can ingest unstructured data in its raw form (e.g., text or images), and it can  automatically determine the set of features which distinguish different categories of data from  one another. This eliminates some of the human intervention required and enables the use of  large amounts of data. You can think of deep learning as 'scalable machine learning' as Lex  Fridman notes in this MIT lecture (link resides outside ibm.com)1. Classical, or 'non-deep,' machine learning is more dependent on human intervention to learn.  Human experts determine the set of features to understand the differences between data  inputs, usually requiring more structured data to learn. Neural networks, or artificial neural networks (ANNs), are comprised of node layers, containing  an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron,  connects to another and has an associated weight and threshold. If the output of any individual  node is above the specified threshold value, that node is activated, sending data to the next  layer of the network. Otherwise, no data is passed along to the next layer of the network by that  node. The “deep” in deep learning is just referring to the number of layers in a neural network. A  neural network that consists of more than three layers—which would be inclusive of the input  and the output—can be considered a deep learning algorithm or a deep neural network. A  neural network that only has three layers is just a basic neural network. "
            },
            {
                "page_number": 2,
                "text": "Deep learning and neural networks are credited with accelerating progress in areas such as  computer vision, natural language processing, and speech recognition. See the blog post “AI vs. Machine Learning vs. Deep Learning vs. Neural Networks: What’s the  Difference?” for a closer look at how the different concepts relate. Machine learning methods Machine learning models fall into three primary categories. Supervised machine learning Supervised learning, also known as supervised machine learning, is defined by its use of labeled  datasets to train algorithms to classify data or predict outcomes accurately. As input data is fed  into the model, the model adjusts its weights until it has been fitted appropriately. This occurs  as part of the cross validation process to ensure that the model avoids overfitting or underfitting.  Supervised learning helps organizations solve a variety of real-world problems at scale, such as  classifying spam in a separate folder from your inbox. Some methods used in supervised  learning include neural networks, naïve bayes, linear regression, logistic regression, random  forest, and support vector machine (SVM). Unsupervised machine learning Unsupervised learning, also known as unsupervised machine learning, uses machine learning  algorithms to analyze and cluster unlabeled datasets (subsets called clusters). These  algorithms discover hidden patterns or data groupings without the need for human intervention.  This method’s ability to discover similarities and differences in information make it ideal for  exploratory data analysis, cross-selling strategies, customer segmentation, and image and  pattern recognition. It’s also used to reduce the number of features in a model through the  process of dimensionality reduction. Principal component analysis (PCA) and singular value  decomposition (SVD) are two common approaches for this. Other algorithms used in  unsupervised learning include neural networks, k-means clustering, and probabilistic clustering  methods. Semi-supervised learning Semi-supervised learning offers a happy medium between supervised and unsupervised  learning. During training, it uses a smaller labeled data set to guide classification and feature  extraction from a larger, unlabeled data set. Semi-supervised learning can solve the problem of  not having enough labeled data for a supervised learning algorithm. It also helps if it’s too costly  to label enough data. For a deep dive into the differences between these approaches, check out 'Supervised vs.  Unsupervised Learning: What's the Difference?' Reinforcement machine learning Reinforcement machine learning is a machine learning model that is similar to supervised  learning, but the algorithm isn’t trained using sample data. This model learns as it goes by using  trial and error. A sequence of successful outcomes will be reinforced to develop the best  recommendation or policy for a given problem. The IBM Watson® system that won the Jeopardy! challenge in 2011 is a good example. The  system used reinforcement learning to learn when to attempt an answer (or question, as it "
            },
            {
                "page_number": 3,
                "text": "were), which square to select on the board, and how much to wager—especially on daily  doubles. Learn more about reinforcement learning Common machine learning algorithms A number of machine learning algorithms are commonly used. These include: 1. Neural networks 2. Linear regression 3. Logistic regression 4. Clustering 5. Decision trees 6. Random forests Neural networks Neural networks  simulate the way the human brain works, with a huge number of linked  processing nodes. Neural networks are good at recognizing patterns and play an important role  in applications including natural language translation, image recognition, speech recognition,  and image creation. Linear regression This algorithm is used to predict numerical values, based on a linear relationship between  different values. For example, the technique could be used to predict house prices based on  historical data for the area. Logistic regression This supervised learning algorithm makes predictions for categorical response variables, such  as “yes/no” answers to questions. It can be used for applications such as classifying spam and  quality control on a production line. Clustering Using unsupervised learning, clustering algorithms can identify patterns in data so that it can be  grouped. Computers can help data scientists by identifying differences between data items that  humans have overlooked. Decision trees Decision trees can be used for both predicting numerical values (regression) and classifying  data into categories. Decision trees use a branching sequence of linked decisions that can be  represented with a tree diagram. One of the advantages of decision trees is that they are easy to  validate and audit, unlike the black box of the neural network. Random forests In a random forest, the machine learning algorithm predicts a value or category by combining  the results from a number of decision trees. "
            },
            {
                "page_number": 4,
                "text": "Advantages and disadvantages of machine learning algorithms Depending on your budget, need for speed and precision required, each algorithm type— supervised, unsupervised, semi-supervised, or reinforcement—has its own advantages and  disadvantages. For example, decision tree algorithms are used for both predicting numerical  values (regression problems) and classifying data into categories. Decision trees use a  branching sequence of linked decisions that may be represented with a tree diagram. A prime  advantage of decision trees is that they are easier to validate and audit than a neural network.  The bad news is that they can be more unstable than other decision predictors.     Overall, there are many advantages to machine learning that businesses can leverage for new  efficiencies. These include machine learning identifying patterns and trends in massive volumes  of data that humans might not spot at all. And this analysis requires little human intervention:  just feed in the dataset of interest and let the machine learning system assemble and refine its  own algorithms—which will continually improve with more data input over time. Customers and  users can enjoy a more personalized experience as the model learns more with every  experience with that person. On the downside, machine learning requires large training datasets that are accurate and  unbiased. GIGO is the operative factor: garbage in / garbage out. Gathering sufficient data and  having a system robust enough to run it might also be a drain on resources. Machine learning  can also be prone to error, depending on the input. With too small a sample, the system could  produce a perfectly logical algorithm that is completely wrong or misleading. To avoid wasting  budget or displeasing customers, organizations should act on the answers only when there is  high confidence in the output. Real-world machine learning use cases Here are just a few examples of machine learning you might encounter every day: Speech recognition: It is also known as automatic speech recognition (ASR), computer speech  recognition, or speech-to-text, and it is a capability which uses natural language processing  (NLP) to translate human speech into a written format. Many mobile devices incorporate speech  recognition into their systems to conduct voice search—e.g. Siri—or improve accessibility for  texting. Customer service:  Online chatbots are replacing human agents along the customer journey,  changing the way we think about customer engagement across websites and social media  platforms. Chatbots answer frequently asked questions (FAQs) about topics such as shipping,  or provide personalized advice, cross-selling products or suggesting sizes for users. Examples  include virtual agents on e-commerce sites; messaging bots, using Slack and Facebook  Messenger; and tasks usually done by virtual assistants and voice assistants. Computer vision: This AI technology enables computers to derive meaningful information from  digital images, videos, and other visual inputs, and then take the appropriate action. Powered by  convolutional neural networks, computer vision has applications in photo tagging on social  media, radiology imaging in healthcare, and self-driving cars in the automotive industry. Recommendation engines: Using past consumption behavior data, AI algorithms can help to  discover data trends that can be used to develop more effective cross-selling strategies.  Recommendation engines are used by online retailers to make relevant product "
            },
            {
                "page_number": 5,
                "text": "recommendations to customers during the checkout process.    Robotic process automation (RPA): Also known as software robotics, RPA uses intelligent  automation technologies to perform repetitive manual tasks. Automated stock trading: Designed to optimize stock portfolios, AI-driven high-frequency  trading platforms make thousands or even millions of trades per day without human  intervention. Fraud detection: Banks and other financial institutions can use machine learning to spot  suspicious transactions. Supervised learning can train a model using information about known  fraudulent transactions. Anomaly detection can identify transactions that look atypical and  deserve further investigation. Challenges of machine learning As machine learning technology has developed, it has certainly made our lives easier. However,  implementing machine learning in businesses has also raised a number of ethical concerns  about AI technologies. Some of these include: Technological singularity While this topic garners a lot of public attention, many researchers are not concerned with the  idea of AI surpassing human intelligence in the near future. Technological singularity is also  referred to as strong AI or superintelligence. Philosopher Nick Bostrum defines  superintelligence as “any intellect that vastly outperforms the best human brains in practically  every field, including scientific creativity, general wisdom, and social skills.” Despite the fact  that superintelligence is not imminent in society, the idea of it raises some interesting questions  as we consider the use of autonomous systems, like self-driving cars. It’s unrealistic to think  that a driverless car would never have an accident, but who is responsible and liable under  those circumstances? Should we still develop autonomous vehicles, or do we limit this  technology to semi-autonomous vehicles which help people drive safely? The jury is still out on  this, but these are the types of ethical debates that are occurring as new, innovative AI  technology develops. AI impact on jobs While a lot of public perception of artificial intelligence centers around job losses, this concern  should probably be reframed. With every disruptive, new technology, we see that the market  demand for specific job roles shifts. For example, when we look at the automotive industry,  many manufacturers, like GM, are shifting to focus on electric vehicle production to align with  green initiatives. The energy industry isn’t going away, but the source of energy is shifting from a  fuel economy to an electric one. In a similar way, artificial intelligence will shift the demand for jobs to other areas. There will  need to be individuals to help manage AI systems. There will still need to be people to address  more complex problems within the industries that are most likely to be affected by job demand  shifts, such as customer service. The biggest challenge with artificial intelligence and its effect  on the job market will be helping people to transition to new roles that are in demand. Privacy "
            },
            {
                "page_number": 6,
                "text": "Privacy tends to be discussed in the context of data privacy, data protection, and data security.  These concerns have allowed policymakers to make more strides in recent years. For example,  in 2016, GDPR legislation was created to protect the personal data of people in the European  Union and European Economic Area, giving individuals more control of their data. In the United  States, individual states are developing policies, such as the California Consumer Privacy Act  (CCPA), which was introduced in 2018 and requires businesses to inform consumers about the  collection of their data. Legislation such as this has forced companies to rethink how they store  and use personally identifiable information (PII). As a result, investments in security have  become an increasing priority for businesses as they seek to eliminate any vulnerabilities and  opportunities for surveillance, hacking, and cyberattacks. Bias and discrimination Instances of bias and discrimination across a number of machine learning systems have raised  many ethical questions regarding the use of artificial intelligence. How can we safeguard  against bias and discrimination when the training data itself may be generated by biased human  processes? While companies typically have good intentions for their automation  efforts, Reuters (link resides outside ibm.com)2 highlights some of the unforeseen  consequences of incorporating AI into hiring practices. In their effort to automate and simplify a  process, Amazon unintentionally discriminated against job candidates by gender for technical  roles, and the company ultimately had to scrap the project. Harvard Business Review (link  resides outside ibm.com)3 has raised other pointed questions about the use of AI in hiring  practices, such as what data you should be able to use when evaluating a candidate for a role. Bias and discrimination aren’t limited to the human resources function either; they can be found  in a number of applications from facial recognition software to social media algorithms. As businesses become more aware of the risks with AI, they’ve also become more active in this  discussion around AI ethics and values. For example, IBM has sunset its general purpose facial  recognition and analysis products. IBM CEO Arvind Krishna wrote: “IBM firmly opposes and will  not condone uses of any technology, including facial recognition technology offered by other  vendors, for mass surveillance, racial profiling, violations of basic human rights and freedoms,  or any purpose which is not consistent with our values and Principles of Trust and  Transparency.” Accountability Since there isn’t significant legislation to regulate AI practices, there is no real enforcement  mechanism to ensure that ethical AI is practiced. The current incentives for companies to be  ethical are the negative repercussions of an unethical AI system on the bottom line. To fill the  gap, ethical frameworks have emerged as part of a collaboration between ethicists and  researchers to govern the construction and distribution of AI models within society. However, at  the moment, these only serve to guide. Some research (link resides outside ibm.com)4 shows  that the combination of distributed responsibility and a lack of foresight into potential  consequences aren’t conducive to preventing harm to society. How to choose the right AI platform for machine learning Selecting a platform can be a challenging process, as the wrong system can drive up costs, or  limit the use of other valuable tools or technologies. When reviewing multiple vendors to select  an AI platform, there is often a tendency to think that more features = a better system. Maybe "
            },
            {
                "page_number": 7,
                "text": "so, but reviewers should start by thinking through what the AI platform will be doing for their  organization. What machine learning capabilities need to be delivered and what features are  important to accomplish them? One missing feature might doom the usefulness of an entire  system. Here are some features to consider.    MLOps capabilities. Does the system have: o  a unified interface for ease of management? o  automated machine learning tools for faster model creation with low-code and  no-code functionality? o  decision optimization to streamline the selection and deployment of  optimization models? o  visual modeling to combine visual data science with open-source libraries and  notebook-based interfaces on a unified data and AI studio? o  automated development for beginners to get started quickly and more advanced  data scientists to experiment? o  synthetic data generator as an alternative or supplement to real-world data  when real-world data is not readily available? Generative AI capabilities. Does the system have:    a content generator that can generate text, images and other content based on the data  it was trained on?    automated classification to read and classify written input, such as evaluating and  sorting customer complaints or reviewing customer feedback sentiment?    a summary generator that can transform dense text into a high-quality summary,  capture key points from financial reports, and generate meeting transcriptions?    a data extraction capability to sort through complex details and quickly pull the  necessary information from large documents? "
            }
        ],
        "images": []
    },
    {
        "file_name": "machine_learning2.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "What is machine learning? Guide, definition and examples Machine learning is a branch of AI focused on building computer systems that learn from data.  The breadth of ML techniques enables software applications to improve their performance over  time. ML algorithms are trained to find relationships and patterns in data. Using historical data as  input, these algorithms can make predictions, classify information, cluster data points, reduce  dimensionality and even generate new content. Examples of the latter, known as generative AI,  include OpenAI's ChatGPT, Anthropic's Claude and GitHub Copilot. Machine learning is widely applicable across many industries. For example, e-commerce, social  media and news organizations use recommendation engines to suggest content based on a  customer's past behavior. In self-driving cars, ML algorithms and computer vision play a critical  role in safe road navigation. In healthcare, ML can aid in diagnosis and suggest treatment plans.  Other common ML use cases include fraud detection, spam filtering, malware threat detection,  predictive maintenance and business process automation. While ML is a powerful tool for solving problems, improving business operations and automating  tasks, it's also complex and resource-intensive, requiring deep expertise and significant data  and infrastructure. Choosing the right algorithm for a task calls for a strong grasp of  mathematics and statistics. Training ML algorithms often demands large amounts of high- quality data to produce accurate results. The results themselves, particularly those from  complex algorithms such as deep neural networks, can be difficult to understand. And ML  models can be costly to run and fine-tune. Still, most organizations are embracing machine learning, either directly or through ML-infused  products. According to a 2024 report from Rackspace Technology, AI spending in 2024 is  expected to more than double compared with 2023, and 86% of companies surveyed reported  seeing gains from AI adoption. Companies reported using the technology to enhance customer  experience (53%), innovate in product design (49%) and support human resources (47%),  among other applications. TechTarget's guide to machine learning serves as a primer on this important field, explaining  what machine learning is, how to implement it and its business applications. You'll find  information on the various types of ML algorithms, challenges and best practices associated  with developing and deploying ML models, and what the future holds for machine learning.  Throughout the guide, there are hyperlinks to related articles that cover these topics in greater  depth. "
            },
            {
                "page_number": 2,
                "text": ""
            },
            {
                "page_number": 3,
                "text": "Why is machine learning important? ML has played an increasingly important role in human society since its beginnings in the mid- 20th century, when AI pioneers like Walter Pitts, Warren McCulloch, Alan Turing and John von  Neumann laid the field's computational groundwork. Training machines to learn from data and  improve over time has enabled organizations to automate routine tasks -- which, in theory, frees  humans to pursue more creative and strategic work. Machine learning has extensive and diverse practical applications. In finance, ML algorithms  help banks detect fraudulent transactions by analyzing vast amounts of data in real time at a  speed and accuracy humans cannot match. In healthcare, ML assists doctors in diagnosing  diseases based on medical images and informs treatment plans with predictive models of  patient outcomes. And in retail, many companies use ML to personalize shopping experiences,  predict inventory needs and optimize supply chains. ML also performs manual tasks that are beyond human ability to execute at scale -- for example,  processing the huge quantities of data generated daily by digital devices. This ability to extract  patterns and insights from vast data sets has become a competitive differentiator in fields like  banking and scientific discovery. Many of today's leading companies, including Meta, Google  and Uber, integrate ML into their operations to inform decision-making and improve efficiency. Machine learning is necessary to make sense of the ever-growing volume of data generated by  modern societies. The abundance of data humans create can also be used to further train and  fine-tune ML models, accelerating advances in ML. This continuous learning loop underpins  today's most advanced AI systems, with profound implications. Philosophically, the prospect of machines processing vast amounts of data challenges humans'  understanding of our intelligence and our role in interpreting and acting on complex information.  Practically, it raises important ethical considerations about the decisions made by advanced ML  models. Transparency and explainability in ML training and decision-making, as well as these  models' effects on employment and societal structures, are areas for ongoing oversight and  discussion. What are the different types of machine learning? Classical ML is often categorized by how an algorithm learns to become more accurate in its  predictions. The four basic types of ML are:    supervised learning    unsupervised learning    semisupervised learning    reinforcement learning. The choice of algorithm depends on the nature of the data. Many algorithms and techniques  aren't limited to a single type of ML; they can be adapted to multiple types depending on the  problem and data set. For instance, deep learning algorithms such as convolutional and  recurrent neural networks are used in supervised, unsupervised and reinforcement learning  tasks, based on the specific problem and data availability. Machine learning vs. deep learning neural networks "
            },
            {
                "page_number": 4,
                "text": "Deep learning is a subfield of ML that focuses on models with multiple levels of neural  networks, known as deep neural networks. These models can automatically learn and extract  hierarchical features from data, making them effective for tasks such as image and speech  recognition. How does supervised machine learning work? Supervised learning supplies algorithms with labeled training data and defines which variables  the algorithm should assess for correlations. Both the input and output of the algorithm are  specified. Initially, most ML algorithms used supervised learning, but unsupervised approaches  are gaining popularity. Supervised learning algorithms are used for numerous tasks, including the following:    Binary classification. This divides data into two categories.    Multiclass classification. This chooses among more than two categories.    Ensemble modeling. This combines the predictions of multiple ML models to produce a  more accurate prediction.    Regression modeling. This predicts continuous values based on relationships within  data. Each regression algorithm has a different ideal use case. For example, linear regression excels  at predicting continuous outputs, while time series regression is best for forecasting future  values. How does unsupervised machine learning work? Unsupervised learning doesn't require labeled data. Instead, these algorithms analyze  unlabeled data to identify patterns and group data points into subsets using techniques such as  gradient descent. Most types of deep learning, including neural networks, are unsupervised  algorithms. Unsupervised learning is effective for various tasks, including the following: "
            },
            {
                "page_number": 5,
                "text": "   Splitting the data set into groups based on similarity using clustering algorithms.    Identifying unusual data points in a data set using anomaly detection algorithms.    Discovering sets of items in a data set that frequently occur together using association  rule mining.    Decreasing the number of variables in a data set using dimensionality reduction  techniques. How does semisupervised learning work? Semisupervised learning provides an algorithm with only a small amount of labeled training  data. From this data, the algorithm learns the dimensions of the data set, which it can then  apply to new, unlabeled data. Note, however, that providing too little training data can lead to  overfitting, where the model simply memorizes the training data rather than truly learning the  underlying patterns. Although algorithms typically perform better when they train on labeled data sets, labeling can  be time-consuming and expensive. Semisupervised learning combines elements of supervised  learning and unsupervised learning, striking a balance between the former's superior  performance and the latter's efficiency. Semisupervised learning can be used in the following areas, among others:    Machine translation. Algorithms can learn to translate language based on less than a  full dictionary of words.    Fraud detection. Algorithms can learn to identify cases of fraud with only a few positive  examples.    Labeling data. Algorithms trained on small data sets can learn to automatically apply  data labels to larger sets. How does reinforcement learning work? Reinforcement learning involves programming an algorithm with a distinct goal and a set of rules  to follow in achieving that goal. The algorithm seeks positive rewards for performing actions that  move it closer to its goal and avoids punishments for performing actions that move it further  from the goal. Reinforcement learning is often used for tasks such as the following:    Helping robots learn to perform tasks in the physical world.    Teaching bots to play video games.    Helping enterprises plan allocation of resources. "
            },
            {
                "page_number": 6,
                "text": "Machine learning model developers can take a number of different approaches to training, with  the best choice depending on the use case and data set at hand. How to choose and build the right machine learning model Developing the right ML model to solve a problem requires diligence, experimentation and  creativity. Although the process can be complex, it can be summarized into a seven-step plan  for building an ML model. 1. Understand the business problem and define success criteria. Convert the group's  knowledge of the business problem and project objectives into a suitable ML problem definition.  Consider why the project requires machine learning, the best type of algorithm for the problem,  any requirements for transparency and bias reduction, and expected inputs and outputs. 2. Understand and identify data needs. Determine what data is necessary to build the model  and assess its readiness for model ingestion. Consider how much data is needed, how it will be  split into test and training sets, and whether a pretrained ML model can be used. 3. Collect and prepare the data for model training. Clean and label the data, including  replacing incorrect or missing data, reducing noise and removing ambiguity. This stage can also  include enhancing and augmenting data and anonymizing personal data, depending on the data  set. Finally, split the data into training, test and validation sets. 4. Determine the model's features and train it. Start by selecting the appropriate algorithms  and techniques, including setting hyperparameters. Next, train and validate the model, then  optimize it as needed by adjusting hyperparameters and weights. Depending on the business  problem, algorithms might include natural language understanding capabilities, such as  recurrent neural networks or transformers for natural language processing (NLP) tasks, or  boosting algorithms to optimize decision tree models. "
            },
            {
                "page_number": 7,
                "text": "5. Evaluate the model's performance and establish benchmarks. Perform confusion matrix  calculations, determine business KPIs and ML metrics, measure model quality, and determine  whether the model meets business goals. 6. Deploy the model and monitor its performance in production. This part of the process,  known as operationalizing the model, is typically handled collaboratively by data scientists and  machine learning engineers. Continuously measure model performance, develop benchmarks  for future model iterations and iterate to improve overall performance. Deployment  environments can be in the cloud, at the edge or on premises. 7. Continuously refine and adjust the model in production. Even after the ML model is in  production and continuously monitored, the job continues. Changes in business needs,  technology capabilities and real-world data can introduce new demands and requirements. Training and optimizing ML models Learn how the following algorithms and techniques are used in training and optimizing machine  learning models:    Regularization.    Backpropagation.    Transfer learning.    Adversarial machine learning. Machine learning applications for enterprises Machine learning has become integral to business software. The following are some examples  of how various business applications use ML:    Business intelligence. BI and predictive analytics software uses ML algorithms,  including linear regression and logistic regression, to identify significant data points,  patterns and anomalies in large data sets. These insights help businesses make data- driven decisions, forecast trends and optimize performance. Advances in generative AI  have also enabled the creation of detailed reports and dashboards that summarize  complex data in easily understandable formats.    Customer relationship management. Key ML applications in CRM include analyzing  customer data to segment customers, predicting behaviors such as churn, making  personalized recommendations, adjusting pricing, optimizing email campaigns,  providing chatbot support and detecting fraud. Generative AI can also create tailored  marketing content, automate responses in customer service and generate insights  based on customer feedback.    Security and compliance. Support vector machines can distinguish deviations in  behavior from a normal baseline, which is crucial for identifying potential cyberthreats,  by finding the best line or boundary for dividing data into different groups. Generative  adversarial networks can create adversarial examples of malware, helping security  teams train ML models that are better at distinguishing between benign and malicious  software. "
            },
            {
                "page_number": 8,
                "text": "   Human resource information systems. ML models streamline hiring by filtering  applications and identifying the best candidates for a position. They can also predict  employee turnover, suggest professional development paths and automate interview  scheduling. Generative AI can help create job descriptions and generate personalized  training materials.    Supply chain management. Machine learning can optimize inventory levels, streamline  logistics, improve supplier selection and proactively address supply chain disruptions.  Predictive analytics can forecast demand more accurately, and AI-driven simulations  can model different scenarios to improve resilience.    Natural language processing. NLP applications include sentiment analysis, language  translation and text summarization, among others. Advances in generative AI, such as  OpenAI's GPT-4 and Google's Gemini, have significantly enhanced these capabilities.  Generative NLP models can produce humanlike text, improve virtual assistants and  enable more sophisticated language-based applications, including content creation and  document summarization. Machine learning examples by industry Enterprise adoption of ML techniques across industries is transforming business processes.  Here are a few examples:    Financial services. Capital One uses ML to boost fraud detection, deliver personalized  customer experiences and improve business planning. The company is using the MLOps  methodology to deploy the ML applications at scale.    Pharmaceuticals. Drug makers use ML for drug discovery, clinical trials and drug  manufacturing. Eli Lilly has built AI and ML models, for example, to find the best sites for  clinical trials and boost participant diversity. The models have sharply reduced clinical  trial timelines, according to the company.    Insurance. Progressive Corp.'s well-known Snapshot program uses ML algorithms to  analyze driving data, offering lower rates to safe drivers. Other useful applications of ML  in insurance include underwriting and claims processing.    Retail. Walmart has deployed My Assistant, a generative AI tool to help its some 50,000  campus employees with content generation, summarizing large documents and acting  as an overall 'creative partner.' The company is also using the tool to solicit employee  feedback on use cases. What are the advantages and disadvantages of machine learning? When deployed effectively, ML provides a competitive advantage to businesses by identifying  trends and predicting outcomes with higher accuracy than conventional statistics or human  intelligence. ML can benefit businesses in several ways: "
            },
            {
                "page_number": 9,
                "text": "   Analyzing historical data to retain customers.    Launching recommender systems to grow revenue.    Improving planning and forecasting.    Assessing patterns to detect fraud.    Boosting efficiency and cutting costs. But machine learning also entails a number of business challenges. First and foremost, it can be  expensive. ML requires costly software, hardware and data management infrastructure, and ML  projects are typically driven by data scientists and engineers who command high salaries. Another significant issue is ML bias. Algorithms trained on data sets that exclude certain  populations or contain errors can lead to inaccurate models. These models can fail and, at  worst, produce discriminatory outcomes. Basing core enterprise processes on biased models  can cause businesses regulatory and reputational harm. Importance of human-interpretable machine learning Explaining the internal workings of a specific ML model can be challenging, especially when the  model is complex. As machine learning evolves, the importance of explainable, transparent  models will only grow, particularly in industries with heavy compliance burdens, such as  banking and insurance. Developing ML models whose outcomes are understandable and explainable by human beings  has become a priority due to rapid advances in and adoption of sophisticated ML techniques,  such as generative AI. Researchers at AI labs such as Anthropic have made progress in  understanding how generative AI models work, drawing on interpretability and explainability  techniques. Interpretable vs. explainable AI Interpretability focuses on understanding an ML model's inner workings in depth, whereas  explainability involves describing the model's decision-making in an understandable way.  Interpretable ML techniques are typically used by data scientists and other ML practitioners,  where explainability is more often intended to help non-experts understand machine learning  models. A so-called black box model might still be explainable even if it is not interpretable, for "
            },
            {
                "page_number": 10,
                "text": "example. Researchers could test different inputs and observe the subsequent changes in  outputs, using methods such as Shapley additive explanations (SHAP) to see which factors  most influence the output. In this way, researchers can arrive at a clear picture of how the  model makes decisions (explainability), even if they do not fully understand the mechanics of  the complex neural network inside (interpretability). Interpretable ML techniques aim to make a model's decision-making process clearer and more  transparent. Examples include decision trees, which provide a visual representation of decision  paths; linear regression, which explains predictions based on weighted sums of input features;  and Bayesian networks, which represent dependencies among variables in a structured and  interpretable way. Explainable AI (XAI) techniques are used after the fact to make the output of more complex ML  models more comprehensible to human observers. Examples include local interpretable  model-agnostic explanations (LIME), which approximate the model's behavior locally with  simpler models to explain individual predictions, and SHAP values, which assign importance  scores to each feature to clarify how they contribute to the model's decision. Transparency requirements can dictate ML model choice In some industries, data scientists must use simple ML models because it's important for the  business to explain how every decision was made. This need for transparency often results in a  tradeoff between simplicity and accuracy. Although complex models can produce highly  accurate predictions, explaining their outputs to a layperson -- or even an expert -- can be  difficult. Simpler, more interpretable models are often preferred in highly regulated industries where  decisions must be justified and audited. But advances in interpretability and XAI techniques are  making it increasingly feasible to deploy complex models while maintaining the transparency  necessary for compliance and trust. Machine learning teams, roles and workflows Building an ML team starts with defining the goals and scope of the ML project. Essential  questions to ask include: What business problems does the ML team need to solve? What are  the team's objectives? What metrics will be used to assess performance? Answering these questions is an essential part of planning a machine learning project. It helps  the organization understand the project's focus (e.g., research, product development, data  analysis) and the types of ML expertise required (e.g., computer vision, NLP, predictive  modeling). Next, based on these considerations and budget constraints, organizations must decide what  job roles will be necessary for the ML team. The project budget should include not just standard  HR costs, such as salaries, benefits and onboarding, but also ML tools, infrastructure and  training. While the specific composition of an ML team will vary, most enterprise ML teams will  include a mix of technical and business professionals, each contributing an area of expertise to  the project. "
            },
            {
                "page_number": 11,
                "text": "Data scientists focus on extracting insights from data, whereas machine learning engineers  build and deploy ML models, but the two roles have some overlap in skills, background and job  responsibilities. ML team roles An ML team typically includes some non-ML roles, such as domain experts who help interpret  data and ensure relevance to the project's field, project managers who oversee the machine  learning project lifecycle, product managers who plan the development of ML applications and  software, and software engineers who build those applications. In addition, several more narrowly ML-focused roles are essential for an ML team:    Data scientist. Data scientists design experiments and build models to predict  outcomes and identify patterns. They collect and analyze data sets, clean and  preprocess data, design model architectures, interpret model outcomes and  communicate findings to business leaders and stakeholders. Data scientists need  expertise in statistics, computer programming and machine learning, including popular  languages like Python and R and frameworks such as PyTorch and TensorFlow.    Data engineer. Data engineers are responsible for the infrastructure supporting ML  projects, ensuring that data is collected, processed and stored in an accessible way.  They design, build and maintain data pipelines; manage large-scale data processing  systems; and create and optimize data integration processes. They need expertise in "
            },
            {
                "page_number": 12,
                "text": "database management, data warehousing, programming languages such as SQL and  Scala, and big data technologies like Hadoop and Apache Spark.    ML engineer. Also known as MLOps engineers, ML engineers help bring the models  developed by data scientists into production environments by using the ML pipelines  maintained by data engineers. They optimize algorithms for performance; deploy and  monitor ML models; maintain and scale ML infrastructure; and automate the ML  lifecycle through practices such as CI/CD and data versioning. In addition to knowledge  of machine learning and AI, ML engineers typically need expertise in software  engineering, data architecture and cloud computing. Steps for establishing ML workflows Once the ML team is formed, it's important that everything runs smoothly. Ensure that team  members can easily share knowledge and resources to establish consistent workflows and best  practices. For example, implement tools for collaboration, version control and project  management, such as Git and Jira. Clear and thorough documentation is also important for debugging, knowledge transfer and  maintainability. For ML projects, this includes documenting data sets, model runs and code,  with detailed descriptions of data sources, preprocessing steps, model architectures,  hyperparameters and experiment results. A common methodology for managing ML projects is MLOps, short for machine learning  operations: a set of practices for deploying, monitoring and maintaining ML models in  production. It draws inspiration from DevOps but accounts for the nuances that differentiate ML  from software engineering. Just as DevOps improves collaboration between software  developers and IT operations, MLOps connects data scientists and ML engineers with  development and operations teams. By adopting MLOps, organizations aim to improve consistency, reproducibility and  collaboration in ML workflows. This involves tracking experiments, managing model versions  and keeping detailed logs of data and model changes. Keeping records of model versions, data  sources and parameter settings ensures that ML project teams can easily track changes and  understand how different variables affect model performance. Similarly, standardized workflows and automation of repetitive tasks reduce the time and effort  involved in moving models from development to production. This includes automating model  training, testing and deployment. After deploying, continuous monitoring and logging ensure  that models are always updated with the latest data and performing optimally. Careers in machine learning and AI The global AI market's value is expected to reach nearly $2 trillion by 2030, and the need for  skilled AI professionals is growing in kind. Check out the following articles related to ML and AI  professional development:    How to build and organize a machine learning team    Prep with 19 machine learning interview questions and answers    4 popular machine learning certificates to get in 2024 Machine learning tools and platforms "
            },
            {
                "page_number": 13,
                "text": "ML development relies on a range of platforms, software frameworks, code libraries and  programming languages. Here's an overview of each category and some of the top tools in that  category. Platforms ML platforms are integrated environments that provide tools and infrastructure to support the  ML model lifecycle. Key functionalities include data management; model development, training,  validation and deployment; and postdeployment monitoring and management. Many platforms  also include features for improving collaboration, compliance and security, as well as  automated machine learning (AutoML) components that automate tasks such as model  selection and parameterization. Each of the three major cloud providers offers an ML platform designed to integrate with its  cloud ecosystem: Google Vertex AI, Amazon SageMaker and Microsoft Azure ML. These unified  environments offer tools for model development, training and deployment, including AutoML  and MLOps capabilities and support for popular frameworks such as TensorFlow and PyTorch.  The choice often comes down to which platform integrates best with an organization's existing  IT environment. In addition to the cloud providers' offerings, there are several third-party and open source  alternatives. The following are some other popular ML platforms:    IBM Watson Studio. Offers comprehensive tools for data scientists, application  developers and MLOps engineers. It emphasizes AI ethics and transparency and  integrates well with IBM Cloud.    Databricks. A unified analytics platform well suited for big data processing. It offers  collaboration features, such as collaborative notebooks, and a managed version of  MLflow, a Databricks-developed open source tool for managing the ML lifecycle.    Snowflake. A cloud-based data platform offering data warehousing and support for ML  and data science workloads. It integrates with a wide variety of data tools and ML  frameworks.    DataRobot. A platform for rapid model development, deployment and management that  emphasizes AutoML and MLOps. It offers an extensive prebuilt model selection and data  preparation tools. Frameworks and libraries ML frameworks and libraries provide the building blocks for model development: collections of  functions and algorithms that ML engineers can use to design, train and deploy ML models more  quickly and efficiently. In the real world, the terms framework and library are often used somewhat interchangeably.  But strictly speaking, a framework is a comprehensive environment with high-level tools and  resources for building and managing ML applications, whereas a library is a collection of  reusable code for particular ML tasks. The following are some of the most common ML frameworks and libraries: "
            },
            {
                "page_number": 14,
                "text": "   TensorFlow. An open source ML framework originally developed by Google. It is widely  used for deep learning, as it offers extensive support for neural networks and large-scale  ML.    PyTorch. An open source ML framework originally developed by Meta. It is known for its  flexibility and ease of use and, like TensorFlow, is popular for deep learning models.    Keras. An open source Python library that acts as an interface for building and training  neural networks. It is user-friendly and is often used as a high-level API for TensorFlow  and other back ends.    Scikit-learn. An open source Python library for data analysis and machine learning, also  known as sklearn. It is ideal for tasks such as classification, regression and clustering.    OpenCV. A computer vision library that supports Python, Java and C++. It provides tools  for real-time computer vision applications, including image processing, video capture  and analysis.    NLTK. A Python library specialized for NLP tasks. Its features include text processing  libraries for classification, tokenization, stemming, tagging and parsing, among others. Programming languages In theory, almost any programming language can be used for ML. But in practice, most  programmers choose a language for an ML project based on considerations such as the  availability of ML-focused code libraries, community support and versatility. Much of the time, this means Python, the most widely used language in machine learning.  Python is simple and readable, making it easy for coding newcomers or developers familiar with  other languages to pick up. Python also boasts a wide range of data science and ML libraries  and frameworks, including TensorFlow, PyTorch, Keras, scikit-learn, pandas and NumPy. Other languages used in ML include the following:    R. Known for its statistical analysis and visualization capabilities, R is widely used in  academia and research. It is well suited for data manipulation, statistical modeling and  graphical representation.    Julia. Julia is a less well-known language designed specifically for numerical and  scientific computing. It is known for its high performance, particularly when handling  mathematical computations and large data sets.    C++. C++ is an efficient and performant general-purpose language that is often used in  production environments. It is valued for its speed and control over system resources,  which make it well suited for performance-critical ML applications.    Scala. The concise, general-purpose language, Scala is often used with big data  frameworks such as Apache Spark. Scala combines object-oriented and functional  programming paradigms, offering scalable and efficient data processing.    Java. Like Scala, Java is a good fit for working with big data frameworks. It is a  performant, portable and scalable general-purpose language that is commonly found in  enterprise environments. "
            },
            {
                "page_number": 15,
                "text": "What is the future of machine learning? Fueled by extensive research from companies, universities and governments around the globe,  machine learning continues to evolve rapidly. Breakthroughs in AI and ML occur frequently,  rendering accepted practices obsolete almost as soon as they're established. One certainty  about the future of machine learning is its continued central role in the 21st century,  transforming how work is done and the way we live. Several emerging trends are shaping the future of ML:    NLP. Advances in algorithms and infrastructure have led to more fluent conversational  AI, more versatile ML models capable of adapting to new tasks and customized  language models fine-tuned to business needs. Large language models are becoming  more prominent, enabling sophisticated content creation and enhanced human- computer interactions.    Computer vision. Evolving computer vision capabilities are expected to have a  profound effect on many domains. In healthcare, it plays an increasingly important role  in diagnosis and monitoring. Environmental science benefits from computer vision  models' ability to analyze and monitor wildlife and their habitats. In software  engineering, it is a core component of augmented and virtual reality technologies.    Enterprise technology. Major vendors like Amazon, Google, Microsoft, IBM and OpenAI  are racing to sign customers up for AutoML platform services that cover the spectrum of  ML activities, including data collection, preparation and classification; model building  and training; and application deployment.    Interpretable ML and XAI. These concepts are gaining traction as organizations attempt  to make their ML models more transparent and understandable. Techniques such as "
            },
            {
                "page_number": 16,
                "text": "LIME, SHAP and interpretable model architectures are increasingly integrated into ML  development to ensure that AI systems are not only accurate but also comprehensible  and trustworthy. Amid the enthusiasm, companies face challenges akin to those presented by previous cutting- edge, fast-evolving technologies. These challenges include adapting legacy infrastructure to  accommodate ML systems, mitigating bias and other damaging outcomes, and optimizing the  use of machine learning to generate profits while minimizing costs. Ethical considerations, data  privacy and regulatory compliance are also critical issues that organizations must address as  they integrate advanced AI and ML technologies into their operations. . "
            }
        ],
        "images": [
            "Image_24",
            "Image_49",
            "Image_74",
            "Image_109",
            "Image_116",
            "Image_145"
        ]
    },
    {
        "file_name": "nlp1.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "What is NLP? Natural language processing (NLP) is a subfield of computer science and artificial intelligence  (AI) that uses machine learning to enable computers to understand and communicate with  human language. NLP enables computers and digital devices to recognize, understand and generate text and  speech by combining computational linguistics—the rule-based modeling of human language— together with statistical modeling, machine learning and deep learning. NLP research has helped enable the era of generative AI, from the communication skills of large  language models (LLMs) to the ability of image generation models to understand requests. NLP  is already part of everyday life for many, powering search engines, prompting chatbots for  customer service with spoken commands, voice-operated GPS systems and question- answering digital assistants on smartphones such as Amazon’s Alexa, Apple’s Siri and  Microsoft’s Cortana. NLP also plays a growing role in enterprise solutions that help streamline and automate  business operations, increase employee productivity and simplify business processes. Benefits of NLP NLP makes it easier for humans to communicate and collaborate with machines, by allowing  them to do so in the natural human language they use every day. This offers benefits across  many industries and applications.    Automation of repetitive tasks    Improved data analysis and insights    Enhanced search    Content generation Automation of repetitive tasks NLP is especially useful in fully or partially automating tasks like customer support, data entry  and document handling. For example, NLP-powered chatbots can handle routine customer  queries, freeing up human agents for more complex issues. In document processing, NLP tools  can automatically classify, extract key information and summarize content, reducing the time  and errors associated with manual data handling. NLP facilitates language translation,  converting text from one language to another while preserving meaning, context and nuances. Improved data analysis NLP enhances data analysis by enabling the extraction of insights from unstructured text data,  such as customer reviews, social media posts and news articles. By using text mining  techniques, NLP can identify patterns, trends and sentiments that are not immediately obvious  in large datasets. Sentiment analysis enables the extraction of  subjective qualities—attitudes,  emotions, sarcasm, confusion or suspicion—from text. This is often used for routing  communications to the system or the person most likely to make the next response. "
            },
            {
                "page_number": 2,
                "text": "This allows businesses to better understand customer preferences, market conditions and  public opinion. NLP tools can also perform categorization and summarization of vast amounts  of text, making it easier for analysts to identify key information and make data-driven decisions  more efficiently. Enhanced search NLP benefits search by enabling systems to understand the intent behind user queries,  providing more accurate and contextually relevant results. Instead of relying solely on keyword  matching, NLP-powered search engines analyze the meaning of words and phrases, making it  easier to find information even when queries are vague or complex. This improves user  experience, whether in web searches, document retrieval or enterprise data systems. Powerful content generation NLP powers advanced language models to create human-like text for various purposes. Pre- trained models, such as GPT-4, can generate articles, reports, marketing copy, product  descriptions and even creative writing based on prompts provided by users. NLP-powered tools  can also assist in automating tasks like drafting emails, writing social media posts or legal  documentation. By understanding context, tone and style, NLP sees to it that the generated  content is coherent, relevant and aligned with the intended message, saving time and effort in  content creation while maintaining quality. Approaches to NLP NLP combines the power of computational linguistics together with machine learning  algorithms and deep learning. Computational linguistics uses data science to analyze language  and speech. It includes two main types of analysis: syntactical analysis and semantical  analysis. Syntactical analysis determines the meaning of a word, phrase or sentence by parsing  the syntax of the words and applying preprogrammed rules of grammar. Semantical analysis  uses the syntactic output to draw meaning from the words and interpret their meaning within  the sentence structure. The parsing of words can take one of two forms. Dependency parsing looks at the relationships  between words, such as identifying nouns and verbs, while constituency parsing then builds a  parse tree (or syntax tree): a rooted and ordered representation of the syntactic structure of the  sentence or string of words. The resulting parse trees underly the functions of language  translators and speech recognition. Ideally, this analysis makes the output—either text or  speech—understandable to both NLP models and people. Self-supervised learning (SSL) in particular is useful for supporting NLP because NLP requires  large amounts of labeled data to train AI models. Because these labeled datasets require time- consuming annotation—a process involving manual labeling by humans—gathering sufficient  data can be prohibitively difficult. Self-supervised approaches can be more time-effective and  cost-effective, as they replace some or all manually labeled training data. Three different approaches to NLP include: Rules-based NLP "
            },
            {
                "page_number": 3,
                "text": "The earliest NLP applications were simple if-then decision trees, requiring preprogrammed  rules. They are only able to provide answers in response to specific prompts, such as the  original version of Moviefone, which had rudimentary natural language generation (NLG)  capabilities. Because there is no machine learning or AI capability in rules-based NLP, this  function is highly limited and not scalable. Statistical NLP Developed later, statistical NLP automatically extracts, classifies and labels elements of text  and voice data and then assigns a statistical likelihood to each possible meaning of those  elements. This relies on machine learning, enabling a sophisticated breakdown of linguistics  such as part-of-speech tagging.      Statistical NLP introduced the essential technique of mapping language elements—such as  words and grammatical rules—to a vector representation so that language can be modeled by  using mathematical (statistical) methods, including regression or Markov models. This informed  early NLP developments such as spellcheckers and T9 texting (Text on 9 keys, to be used on  Touch-Tone telephones). Deep learning NLP Recently, deep learning models have become the dominant mode of NLP, by using huge  volumes of raw, unstructured data—both text and voice—to become ever more accurate. Deep  learning can be viewed as a further evolution of statistical NLP, with the difference that it uses  neural network models. There are several subcategories of models:    Sequence-to-Sequence (seq2seq) models: Based on recurrent neural networks (RNN),  they have mostly been used for machine translation by converting a phrase from one  domain (such as the German language) into the phrase of another domain (such as  English).    Transformer models: They use tokenization of language (the position of each token— words or subwords) and self-attention (capturing dependencies and relationships) to  calculate the relation of different language parts to one another. Transformer models  can be efficiently trained by using self-supervised learning on massive text databases. A  landmark in transformer models was Google’s bidirectional encoder representations  from transformers (BERT), which became and remains the basis of how Google’s search  engine works.    Autoregressive models: This type of transformer model is trained specifically to predict  the next word in a sequence, which represents a huge leap forward in the ability to  generate text. Examples of autoregressive LLMs include GPT, Llama, Claude and the  open-source Mistral.    Foundation models: Prebuilt and curated foundation models can speed the launching of  an NLP effort and boost trust in its operation. For example, the IBM® Granite™ foundation  models are widely applicable across industries. They support NLP tasks including  content generation and insight extraction. Additionally, they facilitate retrieval- "
            },
            {
                "page_number": 4,
                "text": "augmented generation, a framework for improving the quality of response by linking the  model to external sources of knowledge. The models also perform named entity  recognition which involves identifying and extracting key information in a text. NLP Tasks Several NLP tasks typically help process human text and voice data in ways that help the  computer make sense of what it’s ingesting. Some of these tasks include:    Coreference resolution    Named entity recognition    Part-of-speech tagging    Word sense disambiguation Coreference resolution This is the task of identifying if and when two words refer to the same entity. The most common  example is determining the person or object to which a certain pronoun refers (such as “she” =  “Mary”). But it can also identify a metaphor or an idiom in the text (such as an instance in which  “bear” isn’t an animal, but a large and hairy person). Named entity recognition (NER) NER identifies words or phrases as useful entities. NER identifies “London” as a location or  “Maria” as a person's name. Part-of-speech tagging Also called grammatical tagging, this is the process of determining which part of speech a word  or piece of text is, based on its use and context. For example, part-of-speech identifies “make”  as a verb in “I can make a paper plane,” and as a noun in “What make of car do you own?” Word sense disambiguation This is the selection of a word meaning for a word with multiple possible meanings. This uses a  process of semantic analysis to examine the word in context. For example, word sense  disambiguation helps distinguish the meaning of the verb “make” in “make the grade” (to  achieve) versus “make a bet” (to place). Sorting out “I will be merry when I marry Mary” requires  a sophisticated NLP system. How NLP works NLP works by combining various computational techniques to analyze, understand and  generate human language in a way that machines can process. Here is an overview of a typical  NLP pipeline and its steps: Text preprocessing "
            },
            {
                "page_number": 5,
                "text": "NLP text preprocessing prepares raw text for analysis by transforming it into a format that  machines can more easily understand. It begins with tokenization, which involves splitting the  text into smaller units like words, sentences or phrases. This helps break down complex text  into manageable parts. Next, lowercasing is applied to standardize the text by converting all  characters to lowercase, ensuring that words like 'Apple' and 'apple' are treated the same.  Stop word removal is another common step, where frequently used words like 'is' or 'the' are  filtered out because they don't add significant meaning to the text. Stemming or lemmatization  reduces words to their root form (e.g., 'running' becomes 'run'), making it easier to analyze  language by grouping different forms of the same word. Additionally, text cleaning removes  unwanted elements such as punctuation, special characters and numbers that may clutter the  analysis. After preprocessing, the text is clean, standardized and ready for machine learning models to  interpret effectively. Feature extraction Feature extraction is the process of converting raw text into numerical representations that  machines can analyze and interpret. This involves transforming text into structured data by  using NLP techniques like Bag of Words and TF-IDF, which quantify the presence and  importance of words in a document. More advanced methods include word embeddings like  Word2Vec or GloVe, which represent words as dense vectors in a continuous space, capturing  semantic relationships between words. Contextual embeddings further enhance this by  considering the context in which words appear, allowing for richer, more nuanced  representations. Text analysis Text analysis involves interpreting and extracting meaningful information from text data through  various computational techniques. This process includes tasks such as part-of-speech (POS)  tagging, which identifies grammatical roles of words and named entity recognition (NER), which  detects specific entities like names, locations and dates. Dependency parsing analyzes  grammatical relationships between words to understand sentence structure, while sentiment  analysis determines the emotional tone of the text, assessing whether it is positive, negative or  neutral. Topic modeling identifies underlying themes or topics within a text or across a corpus of  documents. Natural language understanding (NLU) is a subset of NLP that focuses on analyzing  the meaning behind sentences. NLU enables software to find similar meanings in different  sentences or to process words that have different meanings. Through these techniques, NLP  text analysis transforms unstructured text into insights. Model training Processed data is then used to train machine learning models, which learn patterns and  relationships within the data. During training, the model adjusts its parameters to minimize  errors and improve its performance. Once trained, the model can be used to make predictions  or generate outputs on new, unseen data. The effectiveness of NLP modeling is continually "
            },
            {
                "page_number": 6,
                "text": "refined through evaluation, validation and fine-tuning to enhance accuracy and relevance in  real-world applications. Different software environments are useful throughout the said processes. For example, the  Natural Language Toolkit (NLTK) is a suite of libraries and programs for English that is written in  the Python programming language. It supports text classification, tokenization, stemming,  tagging, parsing and semantic reasoning functionalities. TensorFlow is a free and open-source  software library for machine learning and AI that can be used to train models for NLP  applications. Tutorials and certifications abound for those interested in familiarizing themselves  with such tools. Challenges of NLP Even state-of-the-art NLP models are not perfect, just as human speech is prone to error. As  with any AI technology, NLP comes with potential pitfalls. Human language is filled with  ambiguities that make it difficult for programmers to write software that accurately determines  the intended meaning of text or voice data. Human language might take years for humans to  learn—and many never stop learning. But then programmers must teach natural language- powered applications to recognize and understand irregularities so their applications can be  accurate and useful. Associated risks might include: Biased training As with any AI function, biased data used in training will skew the answers. The more diverse the  users of an NLP function, the more significant this risk becomes, such as in government  services, healthcare and HR interactions. Training datasets scraped from the web, for example,  are prone to bias. Misinterpretation As in programming, there is a risk of garbage in, garbage out (GIGO). Speech recognition, also  known as speech-to-text, is the task of reliably converting voice data into text data. But NLP  solutions can become confused if spoken input is in an obscure dialect, mumbled, too full of  slang, homonyms, incorrect grammar, idioms, fragments, mispronunciations, contractions or  recorded with too much background noise. New vocabulary New words are continually being invented or imported. The conventions of grammar can evolve  or be intentionally broken. In these cases, NLP can either make a best guess or admit it’s  unsure—and either way, this creates a complication. Tone of voice When people speak, their verbal delivery or even body language can give an entirely different  meaning than the words alone. Exaggeration for effect, stressing words for importance or  sarcasm can be confused by NLP, making the semantic analysis more difficult and less reliable. "
            },
            {
                "page_number": 7,
                "text": "NLP use cases by industry NLP applications can now be found across virtually every industry. Finance In financial dealings, nanoseconds might make the difference between success and failure  when accessing data, or making trades or deals. NLP can speed the mining of information from  financial statements, annual and regulatory reports, news releases or even social media. Healthcare New medical insights and breakthroughs can arrive faster than many healthcare professionals  can keep up. NLP and AI-based tools can help speed the analysis of health records and medical  research papers, making better-informed medical decisions possible, or assisting in the  detection or even prevention of medical conditions. Insurance NLP can analyze claims to look for patterns that can identify areas of concern and find  inefficiencies in claims processing—leading to greater optimization of processing and employee  efforts. Legal Almost any legal case might require reviewing mounds of paperwork, background information  and legal precedent. NLP can help automate legal discovery, assisting in the organization of  information, speeding review and making sure that all relevant details are captured for  consideration. "
            }
        ],
        "images": []
    },
    {
        "file_name": "PracticalImplementationConcepts1.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Artificial intelligence implementation: 8 steps for success Step 1: Define goals Defining goals is the foundation of a successful implementation of AI. The first step is  to identify the problems or opportunities digital transformation can address. This  involves a careful assessment of business processes and objectives, asking  questions such as: What inefficiencies need solving? How can generative AI (gen AI)  enhance customer experiences? Are there decision-making processes that could be  improved with automation? These goals should be precise and measurable for  effective evaluation and ensure that the impact of AI technologies can be tracked.  Examine case studies from other firms to see what might be possible for your  organization. After identifying problems to be solved, companies can translate these into  objectives. These might include improving operational efficiency by a certain  percentage, enhancing customer service response times or increasing the accuracy  of sales forecasts. Defining success metrics such as accuracy, speed, cost reduction  or customer satisfaction—gives teams concrete targets and helps avoid scope  creep. This structured approach ensures that the AI initiative is focused, with clear  end points for evaluation, and that the AI model’s deployment aligns with business  goals. Step 2: Assess data quality and accessibility Given how AI outcomes are only as good as the input data, assessing training data  quality and accessibility is a critical early step in any AI implementation process. AI  systems rely on data to learn patterns and make predictions, and even the most  advanced machine learning algorithms cannot perform effectively on flawed data.  First, data quality should be evaluated based on several criteria, including accuracy,  completeness, consistency and relevance to the business problem. High-quality data  sources are essential for producing reliable insights; poor data quality can lead to  biased models and inaccurate predictions. This assessment often involves data  cleaning to address inaccuracies, filling in missing values and ensuring that data is  up to date. Additionally, data should be representative of real-world scenarios the AI  model will encounter to prevent biased or limited predictions. AI systems must be able to access data appropriately. This includes ensuring that  data is stored in a structured, machine-readable format and that it complies with  relevant privacy regulations and security best practices, especially if sensitive data is  involved. Accessibility also considers the compatibility of data across  sources—different departments or systems often store data in diverse formats, which "
            },
            {
                "page_number": 2,
                "text": "might need to be standardized or integrated. Establishing streamlined data pipelines  and adequate storage solutions ensures that the data can flow efficiently into the AI  model, allowing for smooth deployment and scalability. Step 3: Choose the right AI technology The technology selected for implementation must be compatible with the tasks that  the AI will perform—whether it’s predictive modeling, natural language processing  (NLP) or computer vision. Organizations must first determine the type of AI model  architecture and methodology that best suits their AI strategy. For example, machine  learning techniques such as supervised learning are effective for tasks where data  has undergone labeling, whereas unsupervised learning can be better suited for  clustering or anomaly detection. Additionally, if the goal involves understanding  language, a language model might be ideal, while computer vision tasks typically  require deep learningframeworks such as convolutional neural networks (CNNs).  Choosing technology that directly supports the intended task ensures greater  efficiency and performance. Beyond model selection, organizations must also consider the infrastructure and  platforms that will support the AI system. Cloud services providers offer flexible  solutions for AI processing and storage needs, especially for companies that lack  extensive on-premises resources. Additionally, open-source libraries like Scikit-Learn  and Keras offer prebuilt algorithms and model architectures, reducing development  time. Step 4: Build an AI-proficient team A skilled team can handle the complexities of AI development, deployment and  maintenance. The team should include a range of specialized roles, such as data  scientists, machine learning engineers and software developers, each bringing  expertise in their area. Data scientists focus on understanding data patterns,  developing algorithms and fine-tuning models. Machine learning engineers bridge  the gap between the data science and engineering teams, performing model training,  deploying models and optimizing them for performance. It’s also beneficial to have  domain experts who understand the specific business needs and can interpret  results to ensure that AI outcomes are actionable and aligned with strategic goals. In addition to technical skills, an AI-proficient team needs a range of complementary  skills to support a smooth implementation. For example, project managers with  experience in AI can coordinate and streamline workflows, set timelines and track  progress to ensure that milestones are met. Ethical AI specialists or compliance "
            },
            {
                "page_number": 3,
                "text": "experts can help ensure that AI solutions adhere to data privacy laws and ethical  guidelines. Upskilling existing employees, particularly those in related fields like data  analysis or IT, can be a cost-effective way to build the team, allowing the  organization to draw on in-house expertise and foster a culture of continuous  learning. An AI-proficient team not only enhances the immediate implementation but  also builds the internal capacity for ongoing AI innovation and adaptation. Step 5: Foster a culture of AI innovation Fostering a culture of innovation encourages employees to embrace change, explore  new ideas and participate in the AI adoption process. Creating this culture begins  with leadership that promotes openness, creativity and curiosity, encouraging teams  to consider how AI can drive value and improve business operations. Leadership can  support a proinnovation mindset by communicating a clear vision for AI’s role in the  organization, explaining its potential benefits and addressing common fears. Implementing pilot projects allows teams to try out small-scale AI applications before  full deployment, creating a low-risk way to assess AI capabilities, gain insights and  refine approaches. By embracing a culture of innovation, organizations not only  enhance the success of individual AI projects but also build a resilient, adaptive  workforce ready to leverage AI in future initiatives. Step 6: Manage risks and build ethical frameworks AI models, particularly those that process sensitive data, come with risks related to  data privacy, model bias, security vulnerabilities and unintended consequences. To  address these issues, organizations should conduct thorough risk assessments  throughout the AI development process, identifying areas where the model’s  predictions might go wrong, inadvertently discriminate or expose data to breaches.  Implementing robust data protection practices—such as data anonymization,  encryption and access control—can help protect user information. Regular testing  and monitoring of models in real-world settings are also critical for identifying  unexpected outputs or biases, allowing teams to adjust and retrain models to  improve accuracy and fairness. Building an ethical framework for the use of AI alongside these risk management  practices ensures that AI use aligns with both regulatory standards and the  organization’s values. Ethical guidelines should cover principles such as fairness,  accountability, transparency and respect for user autonomy. A cross-functional AI  ethics committee or review board can oversee AI projects, assessing potential  societal impacts, ethical dilemmas and compliance with data protection laws such as  GDPR or CCPA. By embedding these ethical frameworks, organizations cannot only "
            },
            {
                "page_number": 4,
                "text": "mitigate legal and reputational risks but also build trust with customers and  stakeholders. Step 7: Test and evaluate models Testing and evaluating models help to ensure that the model is accurate, reliable and  capable of delivering value in real-world scenarios. Before deployment, models  should undergo rigorous testing by using separate validation and test datasets to  evaluate their performance. This helps reveal whether the model can generalize  effectively and whether it performs well on new data. Metrics such as accuracy,  precision, recall and F1 score are KPIs often used to assess performance,  depending on the model’s purpose. Testing also includes checking for biases or any  systematic errors that might lead to unintended outcomes, such as discrimination in  decision-making models. By carefully evaluating these metrics, teams can gain  confidence that the model is suitable for deployment. In addition to initial testing, ongoing evaluation helps encourage high performance  over time. Real-world environments are dynamic, with data patterns and business  needs that can change, potentially impacting the model’s effectiveness. Continuous  monitoring and feedback loops allow teams to track the model’s performance, detect  any drift in data or predictions and retrain it as needed. Implementing automated  alerts and performance dashboards can make it easier to identify issues early and  respond quickly. Regularly scheduled model retraining ensures that the AI system  stays aligned with current conditions, maintaining accuracy and value as it adapts to  new patterns. This combination of thorough testing and consistent evaluation  safeguards the AI implementation, making it both resilient and responsive to change. Step 8: Plan for scalability and continuous improvement Scalability is essential for any successful AI implementation, as it allows the system  to handle growing volumes of data, users or processes without sacrificing  performance. When planning for scalability, organizations should choose  infrastructure and frameworks that can support expansion, whether through cloud  services, distributed computing or modular architecture. Cloud platforms are often  ideal for scalable AI solutions, offering on-demand resources and tools that make it  easier to manage increased workloads. This flexibility enables organizations to add  more data, users or capabilities over time, which is particularly useful as business  needs evolve. A scalable setup not only maximizes the long-term value of the AI  system but also reduces the risk of needing costly adjustments in the future. The AI implementation should remain relevant, accurate and aligned with changing  conditions over time. This approach involves regularly retraining models with new  data to prevent performance degradation, as well as monitoring model outcomes to  detect any biases or inaccuracies that might develop. Feedback from users and "
            },
            {
                "page_number": 5,
                "text": "stakeholders should also be incorporated to refine and improve the system based on  real-world usage. Continuous improvement can include updating AI algorithms,  adding new features or fine-tuning model parameters to adapt to shifting business  requirements. This approach enables the AI system to remain effective and reliable,  fostering long-term trust and maximizing its impact across the organization. As every type of organization, from startups to large institutions, seeks to optimize  time-consuming workflows and get more value out of their data with AI tools, it’s  important to remember that goals should be tightly aligned with high-level business  priorities to ensure that AI solutions serve as a tool to advance them, rather than  simply adopting technology for its own sake. It’s easy to get caught up in the AI hype  cycle, especially when there’s a shiny new products released every few weeks. But  to truly capture the benefits of AI, organizations should adopt an implementation  strategy that’s fit to purpose and focused intently on outcomes that are aligned with  the organization’s needs. "
            }
        ],
        "images": []
    },
    {
        "file_name": "PracticalImplementationConcepts2.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Vol.:(0123456789) Discover Artificial Intelligence            (2024) 4:22   | https://doi.org/10.1007/s44163-023-00100-5 Discover Artificial Intelligence Research Managerial insights for AI/ML implementation: a playbook  for successful organizational integration Abdullah A. Abonamah1 · Neda Abdelhamid1 Received: 4 September 2023 / Accepted: 29 December 2023 © The Author(s) 2024    OPEN Abstract In the contemporary business environment, the assimilation of artificial intelligence (AI) and machine learning (ML) is  pivotal for fostering innovation and ensuring long-term growth. This paper examines the strategic aspects of AI/ML adop- tion, emphasizing that its success rests not just on technology but also on strategic alignment, collaboration, and robust  leadership. Highlighting the indispensable role of senior leaders, the paper offers a managerial framework for AI/ML  integration, ensuring its alignment with organizational goals. Using real-world examples, the paper presents how AI/ML  can be strategically embedded to enhance customer interactions, streamline operations, and unveil new revenue streams.  The objective is to provide senior leaders with an understanding, enabling them to harness AI/ML effectively, ensuring  their organizations remain at the innovation forefront in a digital age dominated by disruptive AI/ML technologies. 1  Introduction In today’s evolving business landscape, the integration of artificial intelligence (AI) and machine learning (ML) initiatives  within organizational frameworks has emerged as a crucial component for driving innovation and achieving sustainable  growth. As businesses across industries seek to benefit from the transformative potential of AI and ML, they are in search  for a comprehensive exploration of the strategic dimensions of these technologies and their organizational implications.  With a managerial perspective, this paper discusses the particulars of AI/ML integration, recognizing that successful  implementation requires more than just technological ability—it demands a holistic approach that encompasses stra- tegic alignment, cross-functional collaboration, and effective leadership [1]. The paper places emphasis on the critical role of senior managers, executives, and decision-makers in leading and  championing the integration of AI and ML technologies to ensure their alignment with the organizational strategy, priori- ties, and development. It recognizes that while the technological advancements are critical, the manner in which these  technologies are integrated into an organization’s fabric is equally important. By adopting a managerial perspective,  the paper provides actionable insights and a step-by-step playbook exhibited in Fig. 1, that empowers senior leaders to  navigate the complexities of AI/ML integration with clarity and confidence. Drawing from real-world use cases, the paper illustrates how the implementation of AI and ML can transcend tech- nological perspective to become integral components of an organization’s strategy. It explores diverse industries and  sectors, demonstrating how strategic AI/ML adoption can enhance customer experiences, optimize processes, and unlock  new revenue streams. Through a blend of theoretical insights and practical application, the paper equips readers with *  Neda Abdelhamid, n.abdelhamid@adsm.ac.ae; Abdullah A. Abonamah, a.abonamah@adsm.ac.ae | 1Abu Dhabi School of Management,  Abu Dhabi, United Arab Emirates. "
            },
            {
                "page_number": 2,
                "text": "Vol:.(1234567890) Research  Discover Artificial Intelligence            (2024) 4:22   | https://doi.org/10.1007/s44163-023-00100-5 the tools to assess their organization’s readiness for AI/ML integration, devise tailored strategies, allocate resources  effectively, and measure the impact of these initiatives on business outcomes [2]. Ultimately, the paper aims to empower senior managers to embark on the AI/ML journey with a well-informed and  strategic mindset. By combining managerial insights with technological expertise, organizations can position themselves  at the forefront of innovation by leveraging AI and ML to their full potential, and plan a path toward sustainable success  in an increasingly competitive and digitally driven landscape. 1.1  \u0007The playbook for successful organizational integration Having a playbook for the implementation of artificial intelligence (AI) and machine learning (ML) initiatives within an  organizational context is crucial for several compelling reasons. First, the implementation of AI/ML projects is a multi- faceted endeavor that involves various stakeholders, intricate processes, and potential risks [3]. A playbook provides a  structured and systematic approach, guiding senior managers through each step of the implementation process. The  systematic guidance helps ensure that no critical aspect is overlooked, reducing the likelihood of errors and increasing  the chances of successful deployment. Second, AI/ML technologies are rapidly evolving, and their integration demands an understanding of both technical  intricacies and strategic implications. A playbook acts as a repository of knowledge, capturing best practices, insights, and  experiences from previous implementations. This collective wisdom aids decision-makers in making informed choices,  optimizing resource allocation, and avoiding common pitfalls. Third, AI/ML initiatives often involve cross-functional collaboration between technical teams and business units. A  playbook serves as a common reference point, fostering clear communication and alignment among diverse stakehold- ers [4]. It bridges the gap between technologists and business leaders, ensuring that everyone is on the same page and  working towards shared goals. Fourth, the playbook serves as a strategic tool for managing change within the organization. Implementing AI/ML  technologies can disrupt existing workflows and require adjustments to organizational culture. A playbook provides  a clear roadmap for managing this transition, outlining how to address resistance, train employees, and embed new  practices seamlessly. Last, the ROI and success of AI/ML projects heavily depend on their alignment with the organization’s overarching  strategic goals and objectives [5]. A playbook helps organizations define measurable key performance indicators (KPIs)  and metrics that reflect their strategic priorities. Regularly tracking and evaluating these metrics allows for timely adjust- ments, optimizing the impact of AI/ML initiatives on the organization’s bottom line. Fig. 1   10-Step AL/ML implementation Playbook "
            },
            {
                "page_number": 3,
                "text": "Vol.:(0123456789) Discover Artificial Intelligence            (2024) 4:22   | https://doi.org/10.1007/s44163-023-00100-5  Research In essence, a playbook for AI/ML implementation is a strategic asset that empowers organizations to navigate the  complexities of these transformative technologies. It ensures systematic, well-informed, and successful integration,  ultimately driving innovation, efficiency, and competitiveness in today’s rapidly evolving business landscape. 2  \u000710‑Step playbook with industry use‑cases: 1.  Educate yourself: Begin your journey by diving deep into the foundational concepts of Artificial Intelligence (AI) and  Machine Learning (ML). These technologies, which are at the forefront of the digital revolution, come with a vast array  of capabilities. From automating ordinary tasks to predicting complex patterns, AI and ML have made an impact in  almost every industry.     Understanding their potential is just the tip of the iceberg. It is equally crucial to recognize the diverse applications  they can be tailored to, from healthcare diagnostics to financial forecasting and beyond. Each industry presents its  own set of challenges when integrating AI and ML, whether it is data privacy concerns in healthcare or accuracy  demands in finance. Being aware of these challenges will equip you to navigate them more effectively and harness  the full potential of these technologies.     Fortunately, we live in an era where information is at our fingertips. The digital age has provided us with an abun- dance of online platforms, courses, webinars, and articles that explore the intricacies of AI and ML. These resources,  many of which are freely accessible, offer insights, case studies, and tutorials that span a wide range of industries  and applications. Whether you are a novice looking to get started or a professional aiming to stay updated, there is  a wealth of knowledge waiting to be tapped into. So, take advantage of these resources and embark on a journey  of continuous learning and exploration in the ever-evolving world of AI and ML.     Use-Case: Within finance, the introduction of AI and ML has revolutionized the way fraud detection operates [6].  In the past, conventional systems depended heavily on pre-established rules to flag potentially suspicious activities.  These systems, while effective in their time, often found themselves outpaced by the continuously evolving strate- gies employed by fraudsters. Enter AI and ML, technologies that heralded a transformative shift in fraud detection.  By analyzing and learning from vast amounts of historical transaction data, these systems can discern even the most  subtle and emerging patterns of fraud. This not only amplifies the speed and precision of fraud detection but also  enhances its adaptability to new threats.     Outcome: As a result of this technological evolution, financial institutions are now equipped to identify fraudulent  activities in real-time. This capability not only fortifies the security of transactions but also amplifies the trust custom- ers place in these institutions. 2.  Define clear objectives: Start by articulating the specific goals and outcomes you intend to achieve through the  implementation of Artificial Intelligence (AI) and Machine Learning (ML) initiatives. It’s crucial to ensure that these  objectives are not isolated technical milestones but are intrinsically aligned with the broader business goals of the  organization.     For instance, if your overarching business goal is to improve customer satisfaction, your AI/ML objectives could  include implementing a chatbot for 24/7 customer service or using machine learning algorithms to personalize user  experiences on your platform. If cost reduction is a primary business objective, then automating certain operational  processes using AI could be a specific goal.     Once the objectives are defined, they should be broken down into measurable key performance indicators (KPIs).  These KPIs serve as the yardstick for evaluating the success of the AI/ML initiatives. For example, if the objective is  to improve customer service, KPIs could include metrics like response time, customer satisfaction scores, or the rate  of issue resolution.     It’s also essential to involve stakeholders from both technical and business units in the objective-setting process.  This ensures a balanced perspective, taking into account technical feasibility and business viability. Regular meetings  should be held to review these objectives, making adjustments as needed based on real-world performance data  and changing business needs.     The objectives should also be time-bound, providing a clear timeline for when each goal should be achieved. This  helps in resource allocation and keeps the team focused and accountable. "
            },
            {
                "page_number": 4,
                "text": "Vol:.(1234567890) Research  Discover Artificial Intelligence            (2024) 4:22   | https://doi.org/10.1007/s44163-023-00100-5 By clearly defining objectives that are aligned with broader business goals, you set the stage for the successful  implementation of AI and ML initiatives. This alignment ensures that the technology serves to advance the organi- zation’s overall strategy, rather than being a disconnected endeavor. It also provides a clear roadmap for all team  members, ensuring that everyone understands what success looks like and how their contributions will help achieve  it. 2.1  \u0007Use case: retail personalization for customer engagement A retail chain seeks to enhance customer engagement and drive sales by leveraging AI and ML technologies. The organi- zation’s broader business objective is to improve customer satisfaction, increase purchase frequency, and boost revenue. The cross-functional team, comprising marketing managers, data scientists, and sales representatives, collaborates  to define clear goals and outcomes. The marketing managers articulate the business objectives: to personalize the  shopping experience, offer tailored recommendations, and create targeted marketing campaigns to increase customer  retention and loyalty. The data scientists analyze customer behavior data to identify patterns and preferences. They propose implementing  recommendation algorithms that suggest products based on customers’ previous purchases and browsing history. By  clearly defining these goals, the team ensures that the AI initiatives align with the broader business objectives of enhanc- ing customer satisfaction and increasing revenue. Outcome: An AI-powered personalization system that tailors product recommendations and marketing messages to  individual customers. As a result, customers receive offers and suggestions that resonate with their preferences, increas- ing their likelihood of making purchases. By aligning the AI and ML initiatives with the broader business objectives, the  retail chain effectively uses technology to drive customer engagement, leading to higher customer satisfaction and  increased sales. 3.  Align with strategy: Ensure that AI and ML initiatives align with your organization’s strategic goals. Identify areas  where these technologies can create value, enhance efficiency, and drive innovation.     Leveraging AI and machine learning (ML) technologies to align with an organization’s strategic objectives offers  numerous valuable use cases across various sectors [7]. Senior leaders can enhance customer experience by imple- menting personalized recommendation systems and optimizing supply chains through predictive algorithms. In  the financial sector, AI aids in risk management and fraud detection, safeguarding transactions and reputation. In  manufacturing, predictive maintenance optimizes operations, while healthcare benefits from accurate diagnostics.  AI-driven insights support decision-making, while HR benefits from efficient recruitment. Personalized marketing  campaigns increase engagement, and quality control is enhanced through ML. These applications ensure that AI and  ML initiatives not only align with strategic goals but also bring tangible value, efficiency, and innovation to organiza- tions. 2.2  \u0007Use‑case: enhancing customer experience in E‑commerce through AI/ML solutions Shopee [8] is an e-commerce company from Malaysia with a business strategy focused on enhancing customer experi- ence to drive sales, increase repeat purchases, and foster brand loyalty. The company aims to provide personalized shop- ping experiences, streamline customer support, and optimize the supply chain to ensure timely deliveries. 3  \u0007Aligning AI/ML with business strategy: a.  Personalized Shopping Experience:    Objective: Offer product recommendations tailored to individual customer preferences to increase sales and  customer satisfaction. "
            },
            {
                "page_number": 5,
                "text": "Vol.:(0123456789) Discover Artificial Intelligence            (2024) 4:22   | https://doi.org/10.1007/s44163-023-00100-5  Research    AI/ML Solution: Implement a machine learning algorithm that analyzes a customer’s browsing history, past  purchases, and search queries. The algorithm predicts products the customer is likely to purchase and show- cases them prominently on the customer’s homepage. b.  Chatbots for Customer Support:    Objective: Provide instant responses to customer queries, reducing wait times and improving overall customer  satisfaction.    AI/ML Solution: Deploy an AI-powered chatbot on the website and mobile app. The chatbot is trained using  natural language processing (NLP) to understand and respond to common customer queries, such as order  status, return policies, and product details. For more complex issues, the chatbot seamlessly transfers the  query to a human representative. c.  Optimized supply chain and inventory management:    Objective: Ensure products are in stock and delivered to customers promptly.    AI/ML Solution: Implement a predictive analytics model that forecasts product demand based on historical  sales data, current market trends, and seasonal factors. This helps ShopTrendy maintain optimal inventory  levels, reducing storage costs and minimizing out-of-stock scenarios. Additionally, AI algorithms analyze  delivery routes and traffic patterns to optimize delivery times and reduce shipping delays. Outcome: By aligning their AI/ML solutions with its business strategy, Shopee provides a superior shopping expe- rience for its customers. Personalized product recommendations lead to higher sales conversions, the AI chatbot  ensures customers receive instant support, and the optimized supply chain guarantees timely product deliveries.  As a result, Shopee sees an increase in repeat purchases, a higher customer retention rate, and positive customer  reviews, all of which align perfectly with its overarching business strategy. 4.  Take a holistic approach: Adopt a comprehensive, holistic perspective that encompasses the entire organization  when considering the implementation of technological solutions like Artificial Intelligence (AI) and Machine Learning  (ML). This approach should not only focus on the technical aspects but also integrate strategic insights, operational  elements, organizational culture, policies, and regulations.     Understanding that technology is just one piece of the puzzle is crucial. For instance, while AI and ML can offer  powerful tools for data analysis and automation, their successful implementation is deeply intertwined with the  organization’s strategic goals. Are you looking to enhance customer experience, improve operational efficiency, or  perhaps break into new markets? Your technological solutions should be aligned with these strategic objectives.     Operational elements, such as workflow processes, resource allocation, and employee training, also play a sig- nificant role. The technology must fit seamlessly into existing operations or necessitate operational changes that  are feasible and beneficial. For example, if an AI tool is designed to improve supply chain efficiency, it should be  compatible with the existing supply chain management software and processes.     Organizational culture can’t be overlooked either. The acceptance and effective utilization of new technology are  largely dependent on the workforce’s willingness to adapt. This involves not just training but also creating a culture  of innovation and openness to change.     Policies and regulations are another critical component. Compliance with industry standards, data protection  laws, and ethical considerations must be built into the technological solution from the ground up. This is especially  important in sectors like healthcare or finance, where regulatory compliance is stringent.     By taking a holistic approach, you ensure that all these elements—technological, strategic, operational, cultural,  and regulatory—are incorporated into the planning and implementation phases. This not only increases the likeli- hood of successful implementation but also maximizes the return on investment (ROI) by ensuring that the technol- ogy serves broader organizational goals. Regular cross-departmental meetings and reviews can help in maintaining  this holistic view and making necessary adjustments as the project progresses.     In summary, a holistic approach ensures that the technological solution is not implemented in isolation but is a  well-integrated part of the organization’s overall ecosystem, aligned with its strategic objectives and compliant with  its operational and regulatory framework. "
            },
            {
                "page_number": 6,
                "text": "Vol:.(1234567890) Research  Discover Artificial Intelligence            (2024) 4:22   | https://doi.org/10.1007/s44163-023-00100-5 3.1  \u0007Use Case: holistic approach for artificial intelligence implementation in the pharmaceutical sector A pharmaceutical company is aiming to implement AI applications for drug development, clinical trials, and health- care. The comprehensive approach involves collaboration across disciplines, emphasizing a cross-functional strategy  [9]. The following 8-steps summarize how to achieve a Holistic Approach to the implementation of the AI-powered  project: a)  Strategic Alignment: Senior management recognizes that AI can be used in a variety of domains, including drug  development, clinical trial design, and pharmacovigilance. This implies a strategic focus on improving healthcare  and pharmaceutical processes. b)  Technological Solution: Machine learning, artificial intelligence, and big data indicate a focus on advanced technolo- gies to improve drug discovery, clinical trials, and healthcare decision-making. To develop or integrate an AI algorithm  capable of drug discovery, the IT department can work with external AI experts. c)  Operational Elements: Technological Approach: Machine learning, artificial intelligence, and big data point to an  emphasis on advanced technologies to improve drug discovery, clinical trials, and healthcare decision-making. To  develop or integrate an AI algorithm capable of drug discovery, the IT department can work with external AI experts. d)  Organizational Culture: This case discusses the importance of guidelines in clinical trial protocols and reporting,  indicating an understanding of the importance of aligning AI applications with existing standards and practices.  As a result, HR and internal communications teams can launch a company-wide educational campaign to prepare  employees for the new technology. They explain the benefits and changes that employees can expect, fostering an  innovative and adaptable culture. e)  Policies and Regulations: there is a significance of reporting guidelines for clinical trials involving AI, demonstrating  an understanding of the regulatory landscape and the need to follow guidelines. As a result, a legal team ensures  that the use of patient data is in accordance with privacy laws and regulations. They collaborate with the technology  team to ensure that data storage and processing are secure and in accordance with applicable legislation. f)  Cross-Functional Team: Data scientists and machine learning experts work alongside medical professionals to develop  AI algorithms for data analysis and predictive modeling. IT and technology teams are involved in the deployment of  these algorithms, as well as the integration of AI solutions into existing systems and the protection of data. Teams in  operations and logistics work to integrate AI technologies into daily operations in healthcare facilities and pharma- ceutical processes. Regulatory and compliance experts ensure that standards and healthcare laws are followed, and  they work closely with data scientists to validate AI models. Human resources and internal communications teams  oversee the human side of AI implementation, addressing concerns and cultivating an innovative culture. Legal and  ethical teams are critical in dealing with legal issues and ensuring compliance with data privacy laws. g)  Performance Metrics: KPIs, such as clinical trial success rates, are established to measure the effectiveness of per- sonalized marketing, and the application of AI in various healthcare domains implies consideration of measurable  outcomes. h)  Regular Reviews: Perform reviews over several months will most likely reveal new insights. Another avenue for future  work is to conduct a more in-depth examination of each individual area of AI implementation. Outcome: By taking a holistic approach, the pharmaceutical sector successfully implements an AI-powered holistic  approach, such as strategic alignment, technological solutions, operational elements, organizational culture, policies  and regulations, cross-functional teams, performance metrics, and regular reviews. While the article emphasizes the  importance of these components in successfully integrating AI into the healthcare domain. 5.  Engage in cross-functional collaboration: Foster collaboration between your technical teams and business units.  Encourage open communication to bridge the gap between technology and business requirements.     To foster collaboration between technical teams and business units involves implementing a cross-functional pro- ject focused on developing a new customer relationship management (CRM) system. In this scenario, the technical  team, comprising data scientists and software engineers, collaborates closely with the sales and marketing depart- ments, representing the business units.     To encourage open communication and bridge the gap between technology and business requirements, regular  multidisciplinary meetings are held. During these sessions, the technical team explains the capabilities and limitations "
            },
            {
                "page_number": 7,
                "text": "Vol.:(0123456789) Discover Artificial Intelligence            (2024) 4:22   | https://doi.org/10.1007/s44163-023-00100-5  Research of AI and ML algorithms to the business units in a non-technical language, ensuring mutual understanding. In turn,  the business units articulate their specific needs and challenges, providing valuable context to guide the technical  development.     The technical team then designs and builds a CRM system incorporating AI-powered predictive analytics. This sys- tem aids sales and marketing by analyzing historical customer data to predict future purchasing behavior, enabling  targeted campaigns. The collaboration ensures that the AI algorithms align with the business goals and enhance  efficiency.     Throughout the project, clear and constant communication channels are maintained, facilitating feedback loops.  Business units offer insights on the CRM’s functionality, ensuring that it meets their operational requirements effec- tively. Simultaneously, the technical team educates business units on how to interpret AI-driven insights and leverage  them for strategic decision-making.     By fostering such collaboration, the organization successfully bridges the gap between technical expertise and  business needs, resulting in a CRM system that not only integrates AI effectively but also brings tangible benefits to  the business units and enhances overall productivity and customer engagement [10]. 3.2  \u0007Use‑Case: developing human/AI interactions for chat‑based customer services: Norwegian government The Norwegian government, facing increased demand for citizen services, seeks to enhance its customer support through  the integration of AI technologies. However, there exists a gap between the technical expertise of the research teams  and the practical insights of service agents responsible for citizen interactions [5]. Implementation: The implementation of the AI-powered chatbot goes thru the following 6 stages, these are: a)  Formation of a Cross-Functional Team: Collaboration between researchers, service agents, and practitioners from the  public service organization, a multi-disciplinary team is assembled, including AI/ML experts, software developers  and customer support representatives, b)  Initial Brainstorming Session: The project kicks off with a brainstorming session. Workshops where both research- ers and employees of the public organization participated. These workshops involved reflections on experiences,  adaptations in technology configurations, and discussions about the use of chatbots. The AI/ML team, in contrast,  presents the capabilities of current chatbot technologies, potential challenges, and the data requirements for train- ing the bot. c)  Collaborative Design and Development: Regular collaboration meetings are set up. A collaborative effort where the  research team and practitioners were fully involved in the process. The iterative workshops allowed for refining the  understanding of human-AI partnerships and developing abstractions based on the organization’s experiences with  chatbots. d)  Open Communication Channels: ongoing communication and engagement between the research team, practition- ers, and service agents. This continuous questioning and probing during workshops indicate an open channel for  feedback. A dedicated communication channel (like a Slack channel) can be established for continuous feedback  and queries. This ensures that any issues or new requirements are promptly addressed. e)  Training Workshops: Once the chatbot is nearing completion, the AI/ML team conducts training sessions for the  customer support representatives. These training sessions where service agents were sensitized to discover action  possibilities offered by chatbots. Although the focus is on affordances, the concept aligns with the training aspect  mentioned in the example. f)  Post-Implementation Review: The final round of data analysis where key lessons learned are distilled. This reflects a  form of post-implementation review to understand the insights gained from the project. Its advised that after the  chatbot has been live for a few months, a review session is organized. Both teams discuss the chatbot’s performance  and areas for improvement. Outcome: The Norwegian government successfully implements chatbots in its customer service operations by fos- tering a collaborative environment and facilitating ongoing communication between researchers, service agents, and  practitioners. The collaborative efforts result in a better understanding of human-AI partnerships and the identification of  key chatbot affordances. These affordances, derived from service agents’ experiences, contribute to the gradual develop- ment of their capabilities. The project demonstrates the potential of hybrid human/AI service teams, in which chatbots  serve as intelligent personal assistants to service agents. The findings highlight the importance of leveraging chatbots "
            },
            {
                "page_number": 8,
                "text": "Vol:.(1234567890) Research  Discover Artificial Intelligence            (2024) 4:22   | https://doi.org/10.1007/s44163-023-00100-5 not only for automation but also for augmenting the tasks of service agents, resulting in increased service efficiency. The  document sheds light on the dynamic interplay of technology and human expertise in public service delivery. showcas- ing the potential for further advancements in human-AI interactions. 6.  Create a Dedicated Team: Assemble a cross-functional team that includes representatives from both the technical  departments and each individual business unit within the organization. This specialized team will serve as the engine  propelling the AI/ML project forward, ensuring not only a comprehensive understanding of the project’s objectives  but also securing buy-in from each business unit involved.     The technical side of the team should comprise data scientists, machine learning engineers, and IT experts who  can navigate the complexities of AI and ML technologies. These individuals will be responsible for the technical  architecture, data analytics, and the actual implementation of the AI/ML solutions.     On the other side, each business unit should delegate representatives—such as project managers, business ana- lysts, or even department heads—who understand the specific needs, challenges, and goals of their respective  domains. These representatives will ensure that the AI/ML solutions being developed are in alignment with the each  business unit requirements, workflow, and practices, thereby guaranteeing organizational coherence and effective- ness.     The dual representation ensures several key advantages. First, it guarantees that the technical solutions are not  just cutting-edge but also tailored to meet the unique needs and objectives of each business unit. Second, it fosters  a culture of open dialogue and collaboration, breaking down the silos that can often impede technological innova- tion. Third, it ensures that each business unit has a stake in the project, thereby increasing the likelihood of successful  implementation and adoption across the organization.     Regular meetings and check-ins should be scheduled among team members to discuss progress, resolve chal- lenges, and possibly recalibrate the project’s objectives. These sessions act as a collaborative platform for transparent  communication, enabling the team to make data-driven decisions that are both technically sound and strategically  aligned.     By establishing a dedicated, cross-functional team, you ensure that the AI/ML project is not only technically robust  but also enjoys the support and engagement of each business unit, thereby setting the stage for a successful, organ- ization-wide implementation [11]. 2  Top of form 3.3  \u0007Use case: artificial Intelligence is the future of surgical departments An organization in the medical field is setting out to improve diagnostic precision by combining AI and ML [12]. A dedi- cated cross-functional team has been carefully developed by the hospital to ensure the smooth execution and success of  this transformative project. This team, which includes IT specialists, data scientists, administrative managers, and medical  professionals including radiologists and doctors, brings a variety of experience together. The medical professionals provide invaluable domain knowledge by drawing from their in-depth knowledge of intri- cate patient care processes and diagnostic procedures. Data scientists use their technical expertise to create state-of- the-art algorithms for pattern recognition and picture identification. When it comes to smoothly integrating AI tools  into the current medical systems, IT specialists are essential. Administrative managers offer valuable perspectives on  streamlining processes and distributing resources efficiently. The interdisciplinary team collaborates to establish well-defined project goals, with a primary emphasis on optimiz- ing diagnostic precision, reducing false positives and negatives, and ultimately improved patient outcomes. Technical  specialists suggest the best machine learning algorithms to meet clinical goals that are expressed by medical practition- ers. The result is an advanced diagnostic system that makes use of artificial intelligence (AI) to evaluate medical images,  helping medical personnel diagnose patients quickly and accurately. The formation of this interdisciplinary group guarantees congruence between health goals and technology develop- ments, encouraging a collaborative approach to innovation. This committed team approach makes it easier to combine  technological prowess with medical knowledge, which improves patient care in the end. The healthcare organization  understands how important it is to the success of AI and ML projects in terms of resource allocation. Allocating resources  goes beyond budgeting and is a deliberate undertaking that encompasses people, money, and time. The organization "
            },
            {
                "page_number": 9,
                "text": "Vol.:(0123456789) Discover Artificial Intelligence            (2024) 4:22   | https://doi.org/10.1007/s44163-023-00100-5  Research demonstrates its commitment to investing in its future by carefully allocating funds for infrastructure and technology,  as well as allocating time for the phases of research, development, and implementation. To successfully negotiate the complexity of AI and ML, qualified staff members—such as data scientists, engineers,  and project managers—are carefully selected. The organization recognizes the need of making backup plans and the  possibility that unanticipated obstacles would crop up during the project and demand more funding. By adopting an  all-encompassing strategy for allocating resources, the healthcare facility is creating the groundwork for the effective  application of AI. Outcome: The result is a diagnostic system driven by artificial intelligence has been successfully implemented because  of the committed cross-functional team’s cooperative efforts. The precision of medical picture analysis is greatly improved  by this technology, giving medical personnel crucial assistance in establishing accurate and fast diagnosis. False positives  and negatives have significantly decreased at the hospital, improving patient outcomes and overall care quality. The  ability to seamlessly combine technical innovations with medical expertise demonstrates the efficacy of the interdiscipli- nary approach. The project’s success is guaranteed by the institution’s careful resource allocation, which also establishes  a standard for upcoming AI and ML projects inside the healthcare organization. The result indicates a commitment to  technology innovation as well as, more crucially, an improvement in the provision of healthcare services. 7.  Allocate resources: In the context of AI and ML projects, resource allocation is a pivotal aspect that goes beyond mere  budgeting; it’s a strategic endeavor that encompasses time, personnel, and financial resources. Recognizing that  innovation is not a cost-free endeavor but an investment in the organization’s future, it’s crucial to allocate resources  thoughtfully and deliberately. This involves not just earmarking a budget for technology and infrastructure, but also  dedicating time for research, development, and implementation phases [13]. Additionally, it’s essential to assign  skilled personnel who can navigate the complexities of AI and ML, from data scientists and engineers to project  managers who can keep the initiative on track. Planning should also account for contingencies, as AI and ML projects  often involve unforeseen challenges that may require additional resources. By taking a comprehensive approach  to resource allocation, organizations can set the stage for the successful implementation of AI and ML initiatives,  ensuring that they are well-equipped to innovate effectively, meet project objectives, and ultimately realize a return  on their investment. 3.4  \u0007Use case: a method for implementation of machine learning solutions for predictive maintenance  in small and medium sized enterprises A medical device manufacturing company aims to optimize its operations by implementing a predictive maintenance  system using AI and ML. The organization’s strategic goal is to minimize downtime, reduce maintenance costs, and  increase overall equipment efficiency [14] They implemented ML solutions in predictive maintenance emphasizing the  importance of a more detailed guide or process model that takes into account organizational considerations, required  competencies, investment and operational costs, and data privacy concerns. Preparation, Design, Implementation, and  Monitoring are the four major phases of implementation. Each phase includes several steps, such as checking data avail- ability, preparing data, selecting an ML model, and evaluating the model. SMEs are particularly challenged during the  early stages of ML projects due to a lack of a clear starting point and necessary data. To ensure the success of the AI/ML project they focused on the importance of data in the design phase, emphasizing  the need to assess data availability, derive required datasets, and select appropriate sensors and data collection sys- tems. It recognizes that frequency of the lack trained employees to efficiently navigate this phase and they discuss the  difficulties of data preparation, model selection, and model evaluation, emphasizing the importance of collaboration  between domain experts and data scientists. Challenge. By allocating appropriate resources, including budget, time, and  personnel, the manufacturing company ensures that the AI/ML project is well-supported. The organization understands  that investing in technology-driven innovation will ultimately lead to improved operational efficiency, reduced costs,  and enhanced profitability. As a result, the creation of a machine learning demonstrator focused on predicting failures  of a rotating drive unit based on electrical current data. a critical aspect for cost reduction in fully automated production  lines. The ML solution’s economic value was obvious, particularly in preventing unplanned downtimes that could result  in significant production losses. Outcome: The implementation method improved employee awareness in both the production and maintenance  departments. It contributed to the development of ideas for future applications of ML methods for predictive mainte- nance, demonstrating an increase in worker motivation to adopt this new technology. "
            },
            {
                "page_number": 10,
                "text": "Vol:.(1234567890) Research  Discover Artificial Intelligence            (2024) 4:22   | https://doi.org/10.1007/s44163-023-00100-5 8.  Pilot projects: Initiating small-scale pilot projects is a strategic approach to integrating Artificial Intelligence (AI) and  Machine Learning (ML) into your organization, serving as a low-risk, controlled environment for testing and valida- tion. These pilot projects not only mitigate financial and operational risks but also generate invaluable insights into  the specific benefits that AI and ML can bring to your organization, such as operational efficiency and enhanced  customer experience. By clearly defining the scope, objectives, and success metrics of the pilot, and assembling a  cross-functional team of technical experts, business analysts, and key stakeholders, you set the stage for a focused  and effective test run. Resource allocation is crucial at this stage; dedicating the necessary hardware, software, and  personnel ensures the pilot’s success. Data preparation follows, where relevant and clean data is collected to train the  AI or ML model, which is then rigorously tested to meet predefined objectives and Key Performance Indicators (KPIs).  Once deployed within the limited scope of the pilot, continuous monitoring and adjustments are essential for opti- mizing the model’s performance. The pilot concludes with a thorough evaluation based on metrics and stakeholder  feedback, empowering the organization to make an informed decision on whether to scale the project for broader  implementation or to pivot the AI and ML strategy based on the learnings. Thus, pilot projects act as a foundational  step, providing the data and experience needed to align your AI and ML initiatives with broader organizational goals. 3.5  \u0007Use case: artificial intelligence deployment pilot study at manufacturing industry A manufacturing company launches an AI deployment pilot study to improve its operational processes. With a thorough  understanding of AI’s transformative potential, the organization takes a cautious approach to mitigate risks and gain  insights prior to widespread implementation [15]. The pilot study focuses on one specific operational aspect: high-voltage compliance testing of finished products prior  to customer delivery. A specialized team of experts in AI, data science, and manufacturing operations is formed. They  meticulously plan and execute the step-by-step deployment of AI in the high-voltage testing process collaboratively. The purpose of the pilot is to determine the viability of using AI to improve cycle times, product traceability, and overall  process efficiency. Real-time data capture and analysis are integrated into the study, providing actionable insights to  the organization. Key performance indicators such as process cycle time, traceability improvements, and overall productivity are closely  monitored throughout the pilot. The organization compares the outcomes of AI-driven improvements to traditional  methods, assessing the impact on efficiency and customer satisfaction. The small-scale pilot study yields positive results, demonstrating that AI improves overall productivity, builds trust in  the manufacturing process, improves product traceability, and reduces lead time, thereby increasing customer satisfac- tion. This success validates the organization’s decision to strategically deploy AI. The there were multiple outcomes such  as given below. 4  \u0007Outcomes    Improvement in Process Cycle Time: The use of AI in the compliance high voltage testing of finished products resulted  in a significant improvement in process cycle time. As a result, the testing process has become more efficient and  streamlined.    Improved Traceability: The pilot study resulted in improved product traceability. The organization could track and  trace products more effectively, which would likely lead to improved quality control and accountability.    Increased Productivity: The incorporation of AI into the manufacturing process resulted in increased overall produc- tivity. This suggests that AI had a positive impact on the entire production workflow rather than just specific tasks.    Real-time Data Capture and Availability: The use of AI enabled real-time data capture and availability. This is a signifi- cant advantage because it enables quick decision-making based on the most recent information, improving overall  operational agility.    Supervisor Work Elimination: After implementing FG barcode scanning and real-time production declaration in the  ERP (Enterprise Resource Planning) system, supervisor work was eliminated at the end of the day. This implies that  artificial intelligence (AI) automation reduced manual supervisory tasks, potentially lowering labor costs and increas- ing operational efficiency. "
            },
            {
                "page_number": 11,
                "text": "Vol.:(0123456789) Discover Artificial Intelligence            (2024) 4:22   | https://doi.org/10.1007/s44163-023-00100-5  Research With the pilot study findings in hand, the manufacturing company can make informed decisions about expanding AI  deployment in other areas of its operations. 9.  Learn continuously: In the rapidly evolving fields of Artificial Intelligence (AI) and Machine Learning (ML), continuous  learning is not just an option but a necessity for staying competitive. To keep abreast of the latest trends, technologies,  and methodologies, it’s imperative for managers and senior executives to engage in ongoing educational activi- ties. This can include attending specialized workshops, seminars, and conferences that are tailored to the needs of  decision-makers in the organization. These events serve as platforms for gaining insights into the current state of AI  and ML, understanding their practical applications, and learning from experts in the field. They also offer network- ing opportunities that can lead to valuable partnerships and collaborations. Beyond formal events, subscribing to  industry journals, following thought leaders on social media, and participating in online forums can also provide  up-to-date information. This commitment to continuous learning enables leaders to make informed decisions about  the adoption and implementation of AI and ML technologies, ensuring that the organization remains at the forefront  of innovation. By integrating this ongoing education into their roles, managers and executives are better equipped  to guide their teams and organizations in leveraging AI and ML effectively, responsibly, and strategically. 4.1  \u0007Use Case: financial services industry In the fast-evolving landscape of the financial services industry, a financial senior management recognizes the need to  stay updated on the latest trends and developments in AI and ML. They understand that these technologies can signifi- cantly impact customer experience, risk management, and operational efficiency. To achieve this, the financial senior executives decide to do the following after meeting:    Training and Education: o  Participate in AI and ML training programs and educational courses. They emphasized the importance of gain- ing a fundamental understanding of these technologies for effective decision-making and collaboration with  technical teams.    Collaboration and Team Building: o  While they do not need to be experts in coding or technical details, having a basic understanding aids in effective  communication with AI specialists. They are encouraged collaboration with skilled professionals can improve the  team’s overall capability.    Awareness and Communication: o  Effectively communicate the benefits of AI to both internal teams and external stakeholders, fostering a favorable  perception of AI initiatives.    Stay Informed about Industry Trends: o  This was accomplished through regular interactions with industry experts. The managers began to make informed  decisions that are in line with industry best practices if they are up to date on the latest developments.    Understand Ethical Implications: o  They have evolved in understanding the implications of data collection, processing, and algorithmic decision- making is part of this. This knowledge is critical for making ethical decisions and adhering to regulatory require- ments. Outcome: After attending the meeting the senior management team returns with a clearer understanding of the  potential benefits and risks of AI and ML technologies. They initiate internal discussions to explore how these technolo- gies could be applied within the financial operations, considering factors such as data privacy, customer trust, and regu- latory compliance.By actively participating in industry events and learning from experts, the financial senior executives  are better equipped to make informed decisions about AI and ML initiatives [16]. Their proactive approach to staying  updated on trends ensures that they remain at the forefront of technological advancements, enabling them to strategize  effectively and position the bank for success in an increasingly AI-driven financial landscape. "
            },
            {
                "page_number": 12,
                "text": "Vol:.(1234567890) Research  Discover Artificial Intelligence            (2024) 4:22   | https://doi.org/10.1007/s44163-023-00100-5 10.  Measure and adjust: To ensure success of Artificial Intelligence (AI) and Machine Learning (ML) projects, the impor- tance of measurement and adaptability cannot be overstated. To ensure that these initiatives are not just techno- logically sound but also aligned with business objectives, it’s crucial to establish key performance indicators (KPIs)  tailored to the specific goals of each project.       These KPIs could range from operational metrics like processing speed and accuracy to business outcomes such  as customer engagement levels or cost savings. Regularly scheduled reviews of these metrics should be an integral  part of the project’s lifecycle. These reviews involve not just the technical team but also stakeholders from various  business units, ensuring a comprehensive evaluation of the project’s impact. The data collected from these KPIs  serves as a feedback mechanism, providing invaluable insights into the effectiveness of the AI and ML solutions and  strategies in place. If the metrics reveal that certain objectives are not being met, or if they indicate new opportuni- ties, be prepared to adjust your strategies accordingly. This could mean refining algorithms, reallocating resources,  or even redefining project goals. By adopting a dynamic approach that combines rigorous measurement with the  flexibility to adapt, organizations can optimize the impact of their AI and ML initiatives, ensuring that they deliver  tangible value while staying aligned with broader business objectives. 4.2  \u0007Use Case: digital marketing environment with KPIs and web analytics In the competitive realm of e-commerce, a digital marketing team recognizes the transformative potential of AI and ML  in enhancing customer experiences and optimizing operations [17]. To address this, the team established a set of key performance indicators (KPIs) tailored to evaluate the impact of  AI and ML projects. These KPIs include metrics related to customer engagement, sales conversion rates, operational  efficiency, and cost savings. They explore the integration of artificial intelligence (AI) and machine learning (ML) in the context of Digital Marketing  (DM) strategies. One prominent application highlighted was AI for customer segmentation, which employs algorithms to  analyze user behavior, preferences, and interactions. Key Performance Indicators (KPIs) such as conversion rates, engage- ment rates, and customer lifetime value are recommended for tracking segmentation effectiveness. Furthermore, the Digital Marketing team uses predictive analytics powered by AI to estimate conversion rates, empha- sizing the tracking of prediction accuracy against actual conversions, with KPIs such as conversion rate variance. For  example, the team benefited from dynamic content personalization facilitated by AI, and KPIs such as click-through  rates, time spent on site, and bounce rates for assessing the impact of personalized content. In addition, the team integrated AI-powered chatbots, setting key performance indicators (KPIs) such as response  times, resolution rates, and customer satisfaction scores for monitoring. AI in social media sentiment analysis was also  applied along with KPIs including sentiment accuracy, social engagement metrics, and brand sentiment indices. Continuous reviews of AI/ML algorithms has been established as key to maintain quality insights,. Based on the insights  gained from the KPI reviews, the Digital Marketing teams remains agile in adjusting its strategies. For instance, if the  conversion rate of recommended products is not meeting expectations, the company might refine the recommendation  algorithm or modify the presentation of recommended products on the website. In this way, the company ensures that its AI and ML projects are aligned with business goals and deliver tangible  results. By establishing and regularly reviewing KPIs, the company demonstrates a commitment to data-driven decision- making and continuous improvement. This approach allows them to make informed adjustments, optimize AI and ML  implementations, and ultimately enhance their competitive edge in the dynamic e-commerce landscape Outcome: An AI-powered recommendation engine for cross-selling. Overall, the Digital Marketing Team emphasized  the importance of specific KPIs in measuring the success of various AI-driven Digital Marketing strategies. 5  \u0007Conclusions In conclusion, the 10-step playbook we presented offers a comprehensive and pragmatic approach through which sen- ior managers can adeptly navigate the intricate landscape of AI and ML projects. By proactively following these steps,  managers can steer the integration of these technologies towards a strategic, effective, and harmonious alignment with  their organization’s overarching goals. This playbook isn’t a mere theoretical framework; it’s enriched by real-world use  cases spanning diverse industries, all viewed through the lens of managerial insights. These insightful use cases vividly "
            },
            {
                "page_number": 13,
                "text": "Vol.:(0123456789) Discover Artificial Intelligence            (2024) 4:22   | https://doi.org/10.1007/s44163-023-00100-5  Research illustrate how each individual step within the playbook can be seamlessly applied, underscoring the playbook’s practi- cality and versatility. Importantly, the role of senior managers transcends mere implementation; it necessitates active advocacy, motiva- tion, and guidance throughout the life cycle of AI and ML projects. Managers function as driving forces, catalyzing these  advanced technologies to yield tangible benefits for the organization’s success. By embracing this playbook, managers  become champions of innovation, fostering a culture where AI and ML flourish as powerful tools that propel business  endeavors forward. Through the strategic integration of these technologies, senior managers forge a path towards sus- tainable growth, operational efficiency, and enhanced competitiveness. Author contributions  All authors whose names appear on the submission made substantial contributions to the drafted the work or revised  it critically for important intellectual content. Conception and Design: Dr AA Design and development of Playbook: Both authors contributed  ideas and strategies to the 10 step playbook Manuscript Preparation: The preparation of the manuscript, including the drafting and revising  of the text, was a collaborative effort. We equally contributed to the structuring of the paper, ensuring clarity and coherence throughout.  Theoretical Insights and Practical Applications: We collectively contributed to the development of theoretical insights and the application of  these concepts to real-world scenarios. Our discussions and shared expertise enriched the content of the paper. Figure and Table Creation: We  jointly created figures, tables, and other visual aids presented in the paper. Our collaboration ensured that these visual elements effectively  complemented the narrative. Final Approval: Both authors reviewed and approved the final version of the manuscript for submission to the  journal. We both agree to be accountable for all aspects of the work. Data availability  No data were used in the research presented in this journal article. The study is based entirely on theoretical or conceptual  analysis, and no empirical data or datasets were collected, analyzed, or referenced in the preparation of this manuscript. Declarations Competing interests  The authors declare no competing interests. Open Access   This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adapta- tion, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source,  provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article  are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in  the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will  need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. References 1.  Kiron D, Schrage M, “Strategy For and With AI”, Harvard Business Review, 2019.   2.  Westerman G, Bonnet D. A. McAfee, “Leading Digital Turning Technology Into Business Transformation.” Cambridge: Harvard Business  Press; 2014.   3.  Hoffman RR, Klein G, Mueller ST, Jalaeian M, Tate C. (2021). The Stakeholder Playbook for Explaining AI Systems. Arxiv.   4.  Lee J, Suh T, Roy D, Baucus M. Emerging technology and business model innovation: the case of artificial intelligence. J Open Innov Technol  Market Compl. 2019;5(3):44.   5.  Vassilakopoulou P, Haug A, Salvesen LM, Pappas IO. Developing human/AI interactions for chat-based customer services: lessons learned  from the Norwegian government. Eur J Inf Syst. 2023;32(1):10–22. https://doi.org/10.1080/0960085X.2022.2096490.   6.  Cirqueira D, Helfert M, Bezbradica M. Towards design principles for user-centric explainable AI in fraud detection. In: Degen Helmut, Ntoa  Stavroula, editors. International Conference on Human-Computer Interaction. Cham: Springer International Publishing; 2021. p. 21–40.   7.  Borges AF, Laurindo FJ, Spínola MM, Gonçalves RF, Mattos CA. The strategic use of artificial intelligence in the digital era: systematic  literature review and future research directions. Int J Inf Manage. 2021;57: 102225.   8.  Madan S, Pérez-Morón J, Chua XN, Kee DMH, Chua J, Chua KZ, Vidal LDS. Analysis of the Shopee’s Strategies to Succeed in the Global  E-commerce Market: Malaysia Case. Int J Tourism Hospit Asia Pasific (IJTHAP). 2022;5(1):34–48.   9.  Koshechkin KA, Lebedev GS, Fartushnyi EN, Orlov YL. Holistic approach for artificial intelligence implementation in pharmaceutical  products lifecycle: aMeta-analysis. Appl Sci. 2022;12:8373. https://doi.org/10.3390/app12168373.  10.  Piorkowski D, Park S, Wang AY, Wang D, Muller M, Portnoy F. How ai developers overcome communication challenges in a multidisciplinary  team: a case study. Proc ACM Human-Comp Interact. 2021;5(CSCW1):1–25.  11.  Fountaine T, McCarthy B, Saleh T. Building the AI-powered organization. Harv Bus Rev. 2019;97(4):62–73.  12.  Valente M, Bellini V, Rio P, Freyrie A, Bignami E. Artificial intelligence is the future of surgical departments are we ready? Angiology.  2022;74:397–8. https://doi.org/10.1177/00033197221121192.  13.  Kruhse-Lehtonen U, Hofmann D. How to define and execute your data and AI strategy. Harvard Data Sci Rev. 2020. https://doi.org/10. 1162/99608f92.a010feeb.  14.  Welte R, Estler M, Lucke D. A method for implementation of machine learning solutions for predictive maintenance in small and medium  sized enterprises. Procedia CIRP. 2020;93:909–14. "
            },
            {
                "page_number": 14,
                "text": "Vol:.(1234567890) Research  Discover Artificial Intelligence            (2024) 4:22   | https://doi.org/10.1007/s44163-023-00100-5 15.  Kenge R, Khan Z. Artificial intelligence deployment pilot study at manufacturing industry. Int J Data Mining Emerging Technol.  2020;10(2):39–46.  16.  Mogaji E, Nguyen NP. Managers’ understanding of artificial intelligence in relation to marketing financial services: insights from a cross- country study. Int J Bank Market. 2022;40(6):1272–98.  17.  Saura J, Palos-Sánchez P, Suárez L. Understanding the digital marketing environment with KPIs and web analytics. Future Internet.  2017;9:76. https://doi.org/10.3390/fi9040076. Publisher’s Note  Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. "
            }
        ],
        "images": [
            "Image_221",
            "Image_23"
        ]
    },
    {
        "file_name": "PracticalImplementationConcepts3.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Practical steps for AI implementation in your business Artificial Intelligence (AI) is the missing piece of the business puzzle, completing the picture of a more efficient, productive future. But putting it in place isn’t just about technology; it’s about aligning AI with your company’s unique needs and people. CNN named AI “Word of the Year” because of the changes it brought! Whether it's changing how we do customer service or making our shopping trips feel super personal, AI is becoming a part of our everyday lives. "
            },
            {
                "page_number": 2,
                "text": "Let's explore how AI is transforming business operations and shaping our future! In this article, we'll discuss: 1. AI definition 2. What are some examples of how to apply AI? 3. Benefits of successful AI implementation 4. Step-by-step AI Implementation Strategy: ○Step 1: Identify the Main Goals to Grow Your Business ○Step 2: Set Realistic Expectations ○Step 3: Choose the Right AI Tools and Technologies ○Step 4:Train Your Workforce on AI Adoption ○Step 5: Implementation Process with Existing Systems ○Step 6: Start with a Pilot Project ○Step 7: Measure AI Impact and Optimise 5. Risks of not implementing AI correctly 6. Wrapping up 7. FAQ AI definition "
            },
            {
                "page_number": 3,
                "text": "In today's world, AI seamlessly blends with our routine tasks. We hardly notice it. But do you know what it is? Artificial intelligence (AI) is the simulation of human intelligence in computer systems, allowing them to mimic processes like learning, reasoning, and self-correction. Through AI, computers are designed to learn from data inputs and make decisions based on that knowledge. One key method is natural language processing, which helps machines understand and interpret human language. But it doesn't stop there! As AI evolves, a new frontier emerges: Generative AI. It goes beyond analysing data—it creates new content, like text, images, or even music, based on patterns it learns. "
            },
            {
                "page_number": 4,
                "text": "Instead of just analysing or classifying information, it generates new outputs that mimic human-like creativity. Think of it as AI’s creative engine, always generating fresh ideas from what it knows. Now that we know the definition, let's dig deeper! "
            },
            {
                "page_number": 5,
                "text": "What are some examples of how to apply AI? Simply put, it's the process of integrating AI technologies into your business operations. Think of efficiency, innovation, productivity, and continuous learning. This involves designing, developing, deploying, and maintaining AI models or systems that can perform tasks traditionally done by humans, such as recognising patterns, analysing data, predicting outcomes, and learning from experience. To fully understand a successful implementation in your organisation, here are a few use cases. "
            },
            {
                "page_number": 6,
                "text": "1. Customer Service Remember when you were put on hold by the customer service you needed to talk to? Well, those days are over! Say hi to fully automated AI chatbots to boost your customer service experience. These chatbots are basically your virtual assistants, available 24/7 and ready to answer any customer queries and even provide personalised product recommendations. "
            },
            {
                "page_number": 7,
                "text": "A good example is H&M ChatBot. Not only does it answer basic questions, but it also shares the company's vision for a more sustainable future in fashion. 2. Predictive analytics. No more tarot card reading predictions! "
            },
            {
                "page_number": 8,
                "text": "Predictive analytics checks historical and current data to estimate future events using statistical modelling, data mining methods, and machine learning. Take Netflix, for example. It tracks your viewing habits and suggests 80% of the shows you end up watching based on your preferences. It also creates tailored thumbnails based on the scenes you liked. Netflix’s AI employs machine learning, data mining, and statistical modelling to predict trends, optimise content creation, and improve user experience. The goal is to optimise the feed based on your preferences. By making the browsing experience more personalised and engaging, Netflix helps you discover new shows and movies that suit your tastes, enhancing your overall experience on the platform. "
            },
            {
                "page_number": 9,
                "text": "That's how they not only got your full attention but made your experience much more personalised! But this isn't just about offering a personalised experience. Boosting sales and customer engagement involves making data-driven decisions to resonate with your audience truly. Are you ready to implement AI into your organisation? Discover the most important steps here. 🚀 "
            },
            {
                "page_number": 10,
                "text": "3. Personalised customer preferences. Isn't it great when you walk into your favourite coffee shop, and the barista remembers your usual order? Just like that barista knows exactly how you like your latte, AI picks up your preferences and habits to tailor every interaction. “Applied in the proper ways, AI can help develop new, trusted personalised experiences that make every customer feel like they are the most important.” - says Frank Keller, executive vice president of PayPal. Look at Amazon—they cracked the code! As a result of their AI initiatives, during their 2024 Prime Day event, they generated $13 billion in global sales! "
            },
            {
                "page_number": 11,
                "text": "Using data analysis allows them to whip up spot-on personalised recommendations. That's the reason for their huge boost in cross-selling and upselling! 4. Content Generation: No budget for a graphic designer? No worries—Generative AI has got your back! "
            },
            {
                "page_number": 12,
                "text": "It helps businesses to produce personalised, high-quality material at lightning speed. Tools like Persado and Copy.ai leverage AI to craft content that resonates with specific audiences, boosting engagement. Instead of having to manually create content for each audience group, these AI tools quickly generate personalised messages based on what each audience likes and how they behave. This makes sure that every piece of content is spot-on and interesting, which boosts the chances of grabbing the audience’s attention and getting them to act! And here’s the kicker: The CMO Survey provides actionable insights; businesses utilising AI for content creation have seen a 6.2% increase in sales and a 7% boost in customer satisfaction. With AI, you can scale your content production like a pro while taking your customer experience to the next level. For more tips on boosting your productivity with AI, check out our webinar on AI for Productivity and Tools. "
            },
            {
                "page_number": 13,
                "text": "5. Risk management: Catching red flags straight away! AI can take care of risk safety for you, whether it’s some odd behaviour or sketchy transactions. It's a pro with fraud detection, checks transactional data and picks up the patterns indicative of fraudulent activities, all in real-time. "
            },
            {
                "page_number": 14,
                "text": "In predictive analytics, AI sifts through tons of data to forecast potential risks, ensuring you’re not caught off guard. For compliance monitoring, AI continuously tracks evolving regulations, ensuring you remain compliant and free from legal hassles! A good example is Intesa Sanpaolo, which became the first European bank to adopt AI for regulatory purposes. Through collaboration with Aptus.AI, they streamline regulation management, assess impacts, identify risks, and automate document creation. On top of that, the AI Rapport from DeNederlandscheBank shows that 60% of European banks are utilising AI in their systems. Thanks to that, banks can protect their and your financial peace. So let's sum up all the perks! 👇 "
            },
            {
                "page_number": 15,
                "text": "Benefits of successful AI implementation Firstly, it automates your team's routine tasks, letting them focus on the essentials. Secondly, it personalises customer interactions, boosting loyalty and satisfaction across the board. "
            },
            {
                "page_number": 16,
                "text": "Besides that, it helps you to make faster and smarter decisions by turning your data into useful analytical insights. And last but not least, AI reduces errors and cuts costs, making your decision-making processes more efficient. All this sets you up for faster growth and greater success. Pretty convincing, right? Well, AI business implementation can be a bumpy road if you are unsure where to start. "
            },
            {
                "page_number": 17,
                "text": "So let's take a look at some important steps to take! Ready? Step-by-step AI Implementation strategy Step 1: Identify the Main Goals to Grow Your Business Know what you are aiming for! Having a structured direction always helps to move faster. Are you looking to automate routine tasks, streamline business processes, or gain deeper insights from your data? "
            },
            {
                "page_number": 18,
                "text": "For example, your business goal is to streamline customer support. Then, you might aim to implement an AI chatbot to handle common enquiries and free up your team for more complex issues. 💡Pro Tip: Use the SMART method to outline the business objectives trajectory! Specific: Clearly define what you want to achieve. A specific goal focuses on a particular outcome or result, leaving no room for ambiguity. For example, instead of saying, 'Increase sales,' say, 'Increase sales by 10% in the next quarter.' Measurable: Make sure you can track your progress. Set concrete criteria to measure your success, like using numbers or milestones to evaluate how close you are to your goal. Actionable: Your goal should be within your control and require actionable steps. Define the specific actions you'll take to achieve it, ensuring you have a clear path forward. Realistic: Set a goal that is challenging but achievable given the resources, time, and skills you have. It should push you but still be attainable within your current situation. Timing: Assign a deadline or time frame to your goal to maintain urgency and focus. A clear time frame keeps you accountable and on track. Here's an example of a SMART-goal statement: "
            },
            {
                "page_number": 19,
                "text": "Our business goal is to [quantifiable objective] by [timeframe or deadline]. [Key players or teams] will accomplish this goal by [what steps you’ll take to achieve the goal]. Accomplishing this goal will [result or benefit]. Source: K. Boogard- How to write SMART goals. 👇Define a clear business case for each area to understand if AI is worth further investigating. Step 2: Set Realistic expectations "
            },
            {
                "page_number": 20,
                "text": "Still here? Well done, because you made it halfway through the article already. 🎉 Before starting, it’s crucial to set clear, realistic expectations. While AI can transform your business by streamlining operations and improving efficiency, it’s not an instant fix. Start by identifying specific areas where AI can provide real value—whether it’s automating customer service, improving decision-making, or analysing large volumes of data. Also, it’s super important to keep everyone on your team in the loop about what to expect. AI isn’t here to take anyone’s job; instead, it will handle routine tasks, allowing your team to focus on more important work. By getting everyone on the same page from the get-go, you’ll make it way easier to bring in AI and ensure it works out well in the long run. "
            },
            {
                "page_number": 21,
                "text": "Step 3: Choose the Right AI Tools and Technologies Now it’s time to choose the tools that will get you there. Different AI technologies suit different business needs—whether it’s machine learning algorithms, natural language processing (NLP), or computer vision. Do you need a cloud-based solution like Google Cloud AI or AWS? Or perhaps you're looking at tools like HubSpot’s AI integrations for marketing? Whatever the case, make sure your tools can scale with your business and work seamlessly with your existing tech stack. "
            },
            {
                "page_number": 22,
                "text": "💡Pro tip: Look for tools that offer flexibility in model training, so you can tailor the AI’s learning process to your specific data and needs. Step 4: Train Your Workforce on AI Adoption As we mentioned, your team should also be ready! Make sure everyone gets proper training for a smooth AI integration. You should encourage your team to not be afraid of this advanced technology! You can start by sharing this AI quiz with your team to see what the base level of everyone is. "
            },
            {
                "page_number": 23,
                "text": "As a next step, engaging in a dedicated AI course would be helpful! Check out this AI for Business course to help you and your team boost productivity with AI solutions and make data-driven decisions for the future. Don't be afraid if you meet some resistance —changes are difficult, but they are inevitable for effective business growth. With proper handling, AI integration into the company's culture will soon become an untouchable part of it. Step 5: Implementation Process with Existing Systems This is where the real magic happens! "
            },
            {
                "page_number": 24,
                "text": "Start by pinpointing the key areas AI can boost—like customer support, marketing, sales, and operations. Not sure? Check this table for the possible business cases with AI implementation. Also, you need to check that AI tools can seamlessly connect with your current software and platforms. Using HubSpot? Your AI should be able to sync with these systems to provide real-time insights and automation. If you are unsure which tool can help you optimise your business the best way possible, check out our ultimate guide with growth-hacking AI tools. Keep in mind that integration is not just about plugging in new tech; it’s about ensuring data flows smoothly between AI and your existing tools. That's how you can keep on rolling! "
            },
            {
                "page_number": 25,
                "text": "Step 6: Start with a Pilot Project As tempting as it may be to dive head-first into AI across all departments, resist that urge. Start small, start smart. Launching a pilot project allows you to test AI in a low-risk environment. For instance, your goal is customer service automation. Then it's better to start with a chatbot in a specific department rather than company-wide. The pilot phase is all about continuous learning. What works? What doesn’t? "
            },
            {
                "page_number": 26,
                "text": "Gather insights, refine the AI system, and based on that, tweak your strategy. This way, by the time you’re ready to scale AI, you’ll have concrete data to back up your decisions and the confidence to move forward. Step 7: Measure AI Impact and Optimise Once your AI pilot is up and running, the work isn’t over! Now, it’s time to measure the impact. Are your objectives being met? Track key performance indicators (KPIs) like customer satisfaction, cost savings, or process efficiency. "
            },
            {
                "page_number": 27,
                "text": "Make sure to gather feedback from both your team and the data. AI is an evolving tool, so continuous optimisation is key. Adjust the model as needed to fine-tune its performance, ensuring you get the maximum value from your AI investment. Risks of not implementing AI correctly When implementing AI, it's essential to approach it carefully to avoid major pitfalls. If AI isn't implemented the right way, businesses can run into all sorts of issues, from sketchy predictions to privacy concerns. AI presents quite a few risks if not managed well. Especially with the latest AI EU Act launched in 2024, check it out here. "
            },
            {
                "page_number": 28,
                "text": "Can AI make mistakes in forecasting? Of course, at the end of the day, it's still artificial intelligence. Incorrect predictions can lead to poor business decisions. That’s why it’s crucial to double-check AI’s conclusions and make informed decisions yourself. Not everything is predictable. While AI learns from vast amounts of data, its decisions can sometimes be difficult to explain, so it won’t fully replace human analysts. Data privacy. Using neural networks may need access to personal customer data. This can increase the risk of data breaches. Understanding what data is being used to train the model and the potential consequences is essential. Making sure personal data is secure when using AI should be a top priority. But don't worry too much! By managing things well and following the instructions carefully, the implementation process will provide clear benefits for your business. "
            },
            {
                "page_number": 29,
                "text": "Want to Learn More About AI for Business? You are in the right place! AI is an exciting field that's making huge waves in the business world. "
            },
            {
                "page_number": 30,
                "text": "As tech keeps advancing, AI is becoming more practical for everyday use in various roles and industries. AI is all about saving time and boosting productivity across the board. It’s not just a futuristic dream anymore—it's here, it's real, and it's ready to roll. So how can you learn more about using AI in business? As a leading educational course provider, we created the most effective AI for business course you can find! This course will help you and your team boost productivity with AI solutions and make data-driven decisions for the future. "
            }
        ],
        "images": [
            "Image_31",
            "Image_37",
            "Image_40",
            "Image_47",
            "Image_56",
            "Image_66",
            "Image_74",
            "Image_77",
            "Image_80",
            "Image_86",
            "Image_87",
            "Image_95",
            "Image_99",
            "Image_104",
            "Image_114",
            "Image_118",
            "Image_127"
        ]
    },
    {
        "file_name": "PracticalImplementationConcepts4.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "What is an AI Strategy? An AI strategy is a comprehensive plan that outlines how an organization will leverage artificial intelligence (AI) to achieve its business objectives. It serves as a roadmap, guiding the adoption and management of AI technologies within the organizational framework. An effective AI strategy encompasses several critical components, including:  AI governance  data management  talent acquisition  technology infrastructure  ethical considerations An AI strategy sets the stage for how AI will be utilized to drive innovation, improve efficiency, and create value. It involves assessing the current AI capabilities, setting clear goals, and defining the necessary steps to embed AI into the organization's operations and culture. This strategic approach ensures that AI initiatives are aligned with the broader business strategy and are executed in a manner that maximizes their impact. Developing an AI Strategy Developing an AI strategy involves several key steps to ensure that artificial intelligence (AI) initiatives are effectively aligned with an organization's overall business objectives. This section covers the essential components needed to create a robust AI strategy, from assessing readiness to identifying opportunities for AI adoption. 1. Assessing Organizational AI Readiness Before embarking on an AI initiative, it's crucial to assess your organization's readiness. This involves evaluating the current state of technology, data infrastructure, and employee skills. "
            },
            {
                "page_number": 2,
                "text": "A readiness assessment helps identify existing capabilities and gaps that need to be addressed to support AI projects, and will include:  Technology audit: Review the current technology stack to determine if it can support AI applications. This includes hardware capabilities, software tools, and existing AI solutions.  Data infrastructure: Evaluate the quality, accessibility, and security of data. Effective AI requires high-quality data that is well-organized and readily available.  Skill assessment: Assess the skills and expertise of the current workforce. Identify areas where additional training or new hires are needed to support AI initiatives.  Cultural readiness: Consider the organization's culture and openness to change. Successful AI implementation requires a culture that embraces innovation and continuous learning. 2. Defining Clear Objectives Clear and measurable objectives are essential for the success of an AI strategy. Organizations should identify specific business problems that AI can address and set achievable goals. Objectives should be aligned with the overall business strategy and focused on areas where AI can provide the most significant impact. Defining the objectives of an AI initiative will involve:  Identifying business needs: Determine the specific business problems or opportunities that AI can address.  Setting measurable goals: Establish clear, measurable goals that define what success looks like for AI initiatives.  Aligning with business strategy: Ensure that AI objectives support the broader business strategy and contribute to long-term goals.  Prioritizing initiatives: Prioritize AI projects based on their potential impact and feasibility. 3. Identifying AI Opportunities Identifying opportunities for AI involves analyzing business processes to determine where AI can have the most significant impact. This may include automating routine tasks, enhancing customer interactions, or improving decision-making processes, as well as:  Business process analysis: Conduct a thorough analysis of existing business processes to identify inefficiencies and areas for improvement. "
            },
            {
                "page_number": 3,
                "text": " Benchmarking: Look at industry standards and competitor strategies to identify potential AI applications.  Stakeholder engagement: Engage with stakeholders across the organization to gather insights and identify pain points that AI can address.  Feasibility studies: Conduct feasibility studies to evaluate the practicality and potential return on investment of AI initiatives. [ Components of an Effective AI Strategy Developing an effective AI strategy requires a comprehensive approach that includes multiple components, each playing a critical role in the successful implementation and governance of AI initiatives. AI Governance AI Governance is the foundational layer driving all other aspects. It involves setting policies, frameworks, and regulations to guide the ethical and responsible use of AI. This governance structure promotes fairness by ensuring AI systems are designed and used in ways that do not discriminate and are transparent, enabling stakeholders to understand how decisions are made. Accountability mechanisms are implemented to hold individuals and teams responsible for the outcomes of AI adoption, ensuring that the deployment of AI is monitored and that any adverse effects are swiftly addressed. Data Management Organizations need to ensure data quality, accessibility, and security to support AI initiatives. This involves regular cleaning of data to remove errors and inconsistencies, integrating data from various sources to create a unified dataset, and ensuring robust data governance frameworks are in place. Data accessibility is enhanced through a well-developed data infrastructure that allows easy access to data for AI development, while data security is maintained through encryption and strict access controls to protect sensitive information from unauthorized access. Talent Acquisition and Development AI initiatives require a skilled workforce with expertise in data science, machine learning, and AI ethics. Developing and attracting top talent is essential for the success of an AI strategy. "
            },
            {
                "page_number": 4,
                "text": "Organizations should offer competitive salaries and benefits to attract top talent, and provide opportunities for continuous learning and development in AI and related fields. Upskilling existing employees through training programs, workshops, and seminars helps keep the workforce updated on the latest AI trends and practices. Fostering a culture of learning and innovation is crucial, with initiatives such as innovation labs where employees can experiment with AI technologies and develop new solutions. Technology and Infrastructure Choosing the right technology and infrastructure includes selecting appropriate AI platforms that offer robust capabilities for developing, deploying, and managing AI models. Organizations should consider hybrid multi-cloud environments to enhance scalability and flexibility, ensuring the platform can scale to meet the growing demands of AI applications. High-performance computing resources are essential for handling complex AI computations, and efficient storage solutions are necessary for managing large volumes of data. mplementing AI Initiatives Implementing AI initiatives requires a structured approach to ensure successful deployment and integration within an organization. This section covers the essential steps involved in turning AI strategies into actionable projects, from building a roadmap to scaling successful pilot projects and monitoring progress. Building an AI Roadmap Creating a detailed roadmap is a critical first step in implementing AI initiatives. This roadmap outlines the steps needed to achieve AI goals, including setting timelines, allocating resources, and defining key milestones. A well-constructed roadmap prioritizes early successes to demonstrate value and gain stakeholder buy-in, ensuring that the AI strategy is aligned with the organization’s broader business objectives. A roadmap helps in identifying short-term and long-term goals, setting realistic deadlines, and defining the resources needed for each phase of implementation. It "
            },
            {
                "page_number": 5,
                "text": "serves as a guiding document that keeps the project on track and ensures that all team members are aligned with the strategic objectives. Regularly updating the roadmap based on progress and any new insights or challenges encountered is essential for maintaining momentum and achieving the desired outcomes. Pilot Projects and Scaling Starting with pilot projects allows organizations to test AI applications in a controlled environment. Pilot projects should be low-risk, but visible enough to showcase the potential benefits of AI to the wider organization. These projects provide valuable insights into the feasibility and impact of AI solutions, helping to refine the approach before full-scale implementation. Once pilot projects demonstrate success, scaling these initiatives across the organization is the next step. Scaling involves expanding the use of successful AI applications to other areas of the business, ensuring that the benefits are realized more broadly. This process requires careful planning to manage resources, train staff, and integrate AI solutions seamlessly into existing workflows. Monitoring and Evaluation Continuous monitoring and evaluation are essential for the success of AI initiatives. Organizations should establish metrics and KPIs to track progress, identify areas for improvement, and ensure alignment with business objectives. This involves setting up systems to collect and analyze data on AI performance, user feedback, and overall impact. Regular evaluation helps in understanding the effectiveness of AI applications and making necessary adjustments to improve performance. It also ensures that AI initiatives continue to align with the strategic goals of the organization and provide the expected benefits. Transparent reporting of AI outcomes to stakeholders builds trust and demonstrates the value of AI investments. By following these steps, organizations can effectively implement AI initiatives, turning strategic plans into tangible results. Building a roadmap, starting with pilot "
            },
            {
                "page_number": 6,
                "text": "projects, and continuously monitoring and evaluating progress is crucial for achieving success and driving innovation with AI. Challenges and Solutions Implementing an AI strategy involves navigating several obstacles. Below are common challenges and strategies for overcoming them, ensuring a smooth AI integration process: Data Quality Issues  Obstacle: AI models require high-quality data to perform accurately. However, organizations often have data that is incomplete, inconsistent, or outdated, affecting model training and outcomes.  Strategy: Implement comprehensive data governance frameworks that include policies for data collection, storage, and management. Conduct regular data audits to identify and rectify data quality problems, ensuring reliable and accurate data for AI initiatives. Lack of AI Expertise  Obstacle: Developing and deploying AI solutions require specialized skills that many organizations do not possess internally, leading to slower AI initiatives and suboptimal outcomes.  Strategy: Invest in training and development programs to upskill existing employees through workshops, courses, and certifications. Additionally, hire new talent with specialized AI skills and establish partnerships with academic institutions and AI vendors to access necessary expertise and resources. Resistance to Change  Obstacle: Employees may resist new AI technologies due to fears of job displacement or increased workload, which can hinder AI adoption efforts.  Strategy: Clearly communicate the benefits of AI and involve employees in the transition process. Providing training and support can help alleviate fears and build confidence in new technologies. Cultivate a culture that values innovation and continuous learning to support AI integration. Regulatory Challenges  Obstacle: The regulatory landscape for AI is constantly evolving, requiring organizations to ensure compliance with new and existing laws to avoid legal issues. "
            },
            {
                "page_number": 7,
                "text": " Strategy: Establish a dedicated team to monitor regulatory developments and ensure compliance. Engage with industry groups and participate in regulatory discussions to stay informed about changes and advocate for favorable policies. Developing clear guidelines and maintaining thorough documentation can also help navigate complex regulations. By addressing these challenges proactively, organizations can enhance the success of their AI initiatives. Implementing robust data governance, investing in talent development, managing change effectively, and staying compliant with regulations are crucial steps for leveraging AI to achieve strategic goals. "
            }
        ],
        "images": []
    },
    {
        "file_name": "PracticalImplementationConcepts5.pdf",
        "title": "A B S T R A C T",
        "pages": [
            {
                "page_number": 1,
                "text": "Technological Forecasting & Social Change 197 (2023) 122878 Available online 5 October 2023 0040-1625/© 2023 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/). Implementing and scaling artificial intelligence: A review, framework, and  research agenda Naomi Haefner a,b, Vinit Parida c,d,*, Oliver Gassmann a,b, Joakim Wincent a,e a Global Center for Entrepreneurship & Innovation, University of St. Gallen, Switzerland  b Institute of Technology Management, University of St. Gallen, Switzerland  c Entrepreneurship and Innovation, Luleå University of Technology, Sweden  d Department of Management/USN Business School, University of Vaasa/University of South-Eastern Norway, Finland/Norway  e Entrepreneurship and Innovation, Hanken School of Economics, Finland A R T I C L E  I N F O Keywords:  Artificial intelligence  Machine learning  Review  Scaling  Innovation  Technology A B S T R A C T Artificial intelligence (AI) will have a substantial impact on firms in virtually all industries. Without guidance on  how to implement and scale AI, companies will be outcompeted by the next generation of highly innovative and  competitive companies that manage to incorporate AI into their operations. Research shows that competition is  fierce and that there is a lack of frameworks to implement and scale AI successfully. This study begins to address  this gap by providing a systematic review and analysis of different approaches by companies to using AI in their  organizations. Based on these experiences, we identify key components of implementing and scaling AI in or­ ganizations and propose phases of implementing and scaling AI in firms. 1. Introduction The release of ChatGPT and other generative artificial intelligence  (AI) systems changed the rules of the game for businesses (Edelman and  Abraham, 2023; OpenAI, 2022a). For several years now, experts have  expected AI to have a far-reaching impact on virtually all industries  (Berg et al., 2018; Chui et al., 2018). However, this new type of AI –  generative AI – is supercharging these predictions (Chui et al., 2022).  Generative AI includes large language models (e.g., LLaMA, see Meta AI,  2023; GPT-3, see OpenAI and Pilipiszyn, 2021; Bard, see Pichai, 2023),  image-based systems (e.g., Midjourney, see Midjourney, 2022; DALL-E,  see OpenAI, 2022b; Stable Diffusion, see Stability AI, 2022), and  multimodal systems that combine different types of input (e.g., GPT-4,  see OpenAI, 2023) as well as application-specific systems, such as  AlphaFold for protein structure prediction (Hassabis, 2022). Anyone  who has experimented with these systems can quickly see that they will  enable more than just efficiency and efficacy improvements for busi­ nesses; they will create the basis for powerful new capabilities for firms  (Chui et al., 2022). The largest tech firms, which are pushing the  development of these foundation models (The Economist, 2022), are  already integrating the technology into the core of their value proposi­ tions (Iansiti and Lakhani, 2020). Many firms, however, clearly struggle to successfully implement AI.  Recent surveys show that the vast majority of AI initiatives fail to take  off (Browder et al., 2022; Ransbotham et al., 2020). Why is this? And  how can firms gain traction with AI? To take full advantage of the po­ tential benefits of AI technologies, firms must be able to successfully  implement and ultimately scale AI in their organization. This requires  going through a process of implementing and scaling AI. Firms must  create the necessary prerequisites to successfully use AI technologies –  that is to say, improve operational efficiency or build new value-creation  capabilities based on AI technology. Without adapting to and adopting  AI technologies, it will be difficult for many firms to remain competitive.  Given that implementing AI technologies is a necessity to stay  competitive in the long run and that most firms continue to struggle with  successfully deploying AI in their organizations, a detailed analysis of  how firms can approach the implementation and scaling of AI in orga­ nizations is vital. Conversely, the implementation and scaling of AI in  organizations has not been studied extensively thus far (Makarius et al.,  2020). Moreover, companies lack guidance in managing these processes.  Specifically, companies require frameworks to support the imple­ mentation and scaling of AI (Kanioura and Lucini, 2020). This study  begins to address this gap by providing a systematic analysis of firms’  explanations of their approaches to using AI in their organizations. * Corresponding author at: Entrepreneurship and Innovation, Luleå University of Technology, Sweden.  E-mail addresses: naomi.haefner@unisg.ch (N. Haefner), vinit.parida@ltu.se (V. Parida), oliver.gassmann@unisg.ch (O. Gassmann), joakim.wincent@hanken.fi  (J. Wincent). Contents lists available at ScienceDirect Technological Forecasting & Social Change journal homepage: www.elsevier.com/locate/techfore https://doi.org/10.1016/j.techfore.2023.122878  Received 18 June 2022; Received in revised form 18 September 2023; Accepted 18 September 2023 "
            },
            {
                "page_number": 2,
                "text": "Technological Forecasting & Social Change 197 (2023) 122878 2 Based on these experiences, we identify the key components of AI sys­ tems in organizations, and we pinpoint the phases that firms need to go  through to implement and scale AI. 2. Theoretical background 2.1. Understanding AI adoption AI, defined as “the science and engineering of making intelligent  machines, especially intelligent computer programs” (McCarthy, 2007,  p. 2), is gaining increasing relevance for firms. In particular, it is the  relatively recent developments in machine learning, neural networks,  and deep learning – approaches “concerned with the question of how to  construct computer programs that automatically improve with experi­ ence” (Mitchell, 1997, p. xv) – that are driving substantial change for  companies (Chui et al., 2018; Lakshmi and Bahli, 2020). Various authors  have argued that the transformation toward the general use of AI in  almost all company functions and areas is one of the most significant  change drivers facing firms today (Acemoglu and Restrepo, 2020;  Chalmers et al., 2021; Obschonka and Audretsch, 2020; Ransbotham  et al., 2017; Zeba et al., 2021). The changes are expected to be far-  reaching, touching virtually all aspects of firms’ business including de­ cision making, manufacturing, marketing, supply chain management,  logistics, recruiting, and more (Holmstr¨om, 2022; Makarius et al., 2020;  Mishra et al., 2022; Murray et al., 2021; Pachidi et al., 2021; van den  Broek et al., 2021; Wang and Su, 2021). Therefore, the potential from,  and perhaps even the necessity of, adopting AI technologies are, for the  most part, apparent to companies.  However, as with past phases when new technologies were intro­ duced into organizations (Boothby et al., 2010), adopting AI is not as  straightforward as many would hope. Very many companies struggle  greatly to gain traction with AI (Browder et al., 2022; Ransbotham et al.,  2020; Zolas et al., 2020). Part of the reason why firms struggle to  implement AI is due to the fact that it “[differs] from other advanced  technologies in [its] capacity to make determinations by [itself], as well  as evolve [its] determinations over time once [it is] deployed in an or­ ganization” (Murray et al., 2021). This means that firms must be able to  organize appropriately to ensure that the AI system can independently  contribute to firm value creation as well as track and maintain these  systems and their contributions to firm processes. Furthermore, other  authors have pointed out that AI’s myopia (Balasubramanian et al.,  2022) and inability to perceive interdependencies within the firm  (Raisch and Krakowski, 2021) indicate that firms will likely encounter  considerable difficulty in improving firm efficiency and expanding firm  value creation using AI (Kemp, 2023).  Consequently, it is very important to better understand how AI  implementation, and ultimately scaling, can be advanced in firms so that  the considerable benefits inherent in the technology can be reaped.  Research on technology adoption has pointed out that successful  adoption hinges on the firm completing a transformation where it learns  to both use the new technology and create the appropriate organiza­ tional setting. For instance, early studies on information technology (IT)  adoption have shown that there is a “complementarity between com­ puter investment and organizational investment” (Brynjolfsson et al.,  2002, p. 138). More generally, the adoption of technology necessitates  both changes in the firm’s technology use itself and in its organization  (Boothby et al., 2010). AI technology is often discussed as an element in  digital technologies more generally. The current wave of technological  changes is significantly different from previous waves of technological  development. Hanelt et al. (2021) note that current technologies are in  and of themselves different from past technologies, such as IT. Current  digital technologies are generative, malleable, and combinatorial (Kal­ linikos et al., 2013). Furthermore, digital technologies are reshaping  firm boundaries and evoking more fundamental change even in firms’  business models (Hanelt et al., 2021). However, this literature has  insufficiently examined the transformation process relating specifically to AI. Such an analysis – separate from other digital technologies, such as  cloud computing – is essential because AI is effecting a fundamentally  different advance for firms in “offload[ing] cognitive work from humans  to computers” (Peretz-Andersson and Torkar, 2022, p. 2). Therefore, this  paper examines the transformation of firms as they reach an initial  readiness for AI and ultimately move forward to scale the use of the  technology within their business. Companies that are known to be strong  users of AI, such as Google and Uber, build their organizations around  their AI systems (Johnson et al., 2022). Combining this practical  knowledge on how AI-first firms operate with previous research on  technology adoption, we seek to analyze the technological and organi­ zational changes necessary for firms to embark on and ultimately com­ plete the AI transformation successfully. 2.2. A socio-technical systems perspective on AI adoption Socio-technical systems theory (STST) presents a very suitable  framework for analyzing the implementation and scaling of AI in orga­ nizations because it has been used to study technology adoption more  generally. It has been employed in different contexts including advanced  manufacturing (Shani et al., 1992), renewable energy (Li et al., 2015;  Yun and Lee, 2015), shipping (Geels, 2002), and mobile communica­ tions (Ansari and Garud, 2009; Shin et al., 2011). STST adopts a systems  view of organizations – that is to say, it views any organization, or part  of it, as consisting of a set of interacting sub-systems (Appelbaum, 1997;  Geels, 2004). The theory is based on the socio-technical model devel­ oped by Leavitt (1965), whose original conceptualization proposed four  interrelated and coordinated dimensions – namely, people, task, struc­ ture, and technologies – as the core components of organizational work  systems. More recent work in the area of STST has tended to focus on a  six-dimensional conceptualization consisting of people, goals, culture,  infrastructure, technology, and processes (see e.g., Münch et al., 2022;  Sony and Naik, 2020). Moreover, the external environment with the  firm’s stakeholders and general market environment are often studied as  part of STST (Münch et al., 2022).  STST is a suitable theoretical framework for our analysis of AI  implementation and scaling in firms for several reasons. Our introduc­ tion to STST above shows that the theory presents a comprehensive  framework to analyze emerging phenomena with interconnected tech­ nical and social dimensions. Critically, AI systems contain both technical  and social aspects (Anthony et al., 2023; Glikson and Woolley, 2020;  Lebovitz et al., 2022; Makarius et al., 2020). Therefore, STST is a highly  appropriate framework to study AI implementation and scaling. Prior  research in related areas such as servitization has shown that the joint  study of social and technical components is beneficial in understanding  the adoption and success of digital technologies (Münch et al., 2022;  Sony and Naik, 2020).  Studies examining social and technical aspects of artificial intelli­ gence specifically are, however, still very rare. The exceptions include  Chowdhury et al. (2022) and Anthony (2021) who both examined socio-  technical factors affecting individual employees’ collaboration with and  use of AI. Xing et al. (2021) studied socio-technical barriers to the  adoption of AI-based products by consumers. The only study to consider  firm-level socio-technical aspects is Makarius et al. (2020), which pro­ poses that the scope and novelty level of AI are key criteria in defining  the approach to AI implementation. However, Makarius and colleagues  note that “the fundamental issues related to structure and functioning in  organizations in relation to AI systems remain underexplored” (Makar­ ius et al., 2020, p. 271). Consequently, the goal of our research is to  begin to address this precise gap in the literature by studying the process  followed by companies in implementing and scaling AI in their organi­ zations as well as the socio-technical components involved in this  process. N. Haefner et al. "
            },
            {
                "page_number": 3,
                "text": "Technological Forecasting & Social Change 197 (2023) 122878 3 3. Data and methods We take an exploratory case study approach to examine the phases of  implementing and scaling AI in a range of different companies. We rely  on techniques for inductive theory building (Eisenhardt et al., 2016;  Gehman et al., 2018). We base our approach to sample selection on that  employed by Fisher et al. (2020), which uses podcasts featuring expert  interviews as a data source. In our study, we used interviews conducted  as part of the TWIML AI podcast (originally This Week in Machine  Learning and AI), which is one of the top podcasts on artificial intelli­ gence (Charrington, 2022). The podcast includes interviews with some  of the top minds and ideas in machine learning and AI with the goal of  elucidating the impact of AI on how businesses operate. The podcast  guests include a wide range of machine learning and AI researchers,  practitioners, and innovators. We analyzed podcast episodes that ful­ filled the following criteria:   The podcast featured a guest from industry practice rather than a  researcher. This ensured that guests would be able to speak specif­ ically to their experience with implementing and scaling AI appli­ cations in their respective firms.     We selected a diverse set of firms that included both companies that  are widely known to be among the most advanced in applying AI in  their organizations and those that have relatively less experience. In  doing so, we can capture a broader range of industry experience,  with AI technologies applied in different settings.     We included a wide variety of AI application areas from computer  vision to sales forecasting, to content moderation. This approach  allows us to derive more generally applicable approaches to using AI  in organizations that go beyond the specifics related to particular AI  technologies. The firms analyzed in this study are all active in applying AI tech­ nologies. Importantly, they are in different phases of implementing and  scaling AI (Table 1). Some interviewees have worked in more than one  organization, allowing them to compare and contrast different ap­ proaches based on their varied personal experience. These cases repre­ sent real-life experience in applying AI technologies and managing the  phases of implementing and scaling AI in different organizational con­ texts. Moreover, the interviewees provide some indication of what, in  their experience, may constitute some of the best practices for imple­ menting and scaling AI.  The study examines all interviews conducted with AI practitioners  featured on the TWIML AI podcast between May 2021 and November  2022. The podcasts were analyzed regarding descriptions provided by  practitioners on how they implemented AI technologies in their firms.  We were intentionally broad in our initial analysis, including various  types of information on using AI in firms without focusing on any spe­ cific aspect. We made notes on how experienced various practitioners  and their companies were in applying AI technologies. This allowed us  to determine how firms tend to progress through the phases of imple­ menting and scaling AI.  Overall, we examined 22 interviews with machine learning and AI  practitioners. Some interviewees providing insights into work both at  their current firm and at well-known AI-first companies, such as Google,  Uber, and Netflix, where they held prior positions. The practitioners  held various positions including chief executive officer, chief data offi­ cer, head of AI, head of data science, and machine learning engineer. All  interviewees had experience applying AI technologies in businesses and  were, therefore, able to provide insights into the approaches chosen at  their respective firms.  Our analysis of the interviews focused on recognizing key activities  in AI implementation and scaling across the firms included in our study.  We systematically uncovered themes in our complex data using thematic  analysis (Braun and Clarke, 2006). This involved coding and catego­ rizing phrases and themes mentioned by interviewees in the podcast episodes. This approach allowed us to identify various factors relevant to  implementing and scaling AI in organizations. Subsequently, we exam­ ined the factors to identify underlying aggregate patterns. Finally, we  mapped links between the aggregate patterns to derive a structured  maturity model. 4. Findings This section presents the findings from our analysis of the interviews  with machine learning and AI professionals. Section 4.1 describes the  sub-dimensions of the socio-technical system for AI implementation and  scaling. Then, Section 4.2 delineates how the socio-technical system is  designed to accommodate firms initially implementing AI and ulti­ mately scaling AI in their organizations. 4.1. Components of implementing and scaling AI Implementing and scaling new technologies is a complex endeavor  for firms (Fountaine et al., 2021). Recognizing which levers exist for  management to successfully implement these technologies is, therefore,  extremely important. Our analysis of the interviews in our study allowed  us to determine the most important components outlined by the in­ terviewees that increase the likelihood of successful implementation and  scaling of AI in firms. We therefore employed an inductive approach to  derive the key technical and social components involved in implementing  and scaling AI in firms. In the following section, we briefly elaborate on  the dimensions, elucidating the main sub-dimensions of the technical  and social components. 4.1.1. Technical components  Managing the technology itself is clearly one of the most important Table 1  Case description. Companies  Interviewee(s)  Industry 1  Toyota  Adrien Gaidon  Automotive  2  GSK  Kim Branson  Pharmaceuticals  3  Tecton/Uber  Mike del Balso  Software/mobility    Kevin Stumpf   4  Cloudera  Sushil Thomas  Software  5  RTL  Daan Odijk  Media  6  23andMe  Subarna Sinha  Personal genomics and biotechnology  7  Intuit  Srivathsan  Canchi  Financial software 8  LinkedIn  Ya Xu  Professional network service    Parvez  Ahammad   9  Overstock  Nishan Subedi  Internet retailer  10  Prosus  Paul van der  Boor  Global investment group operating in  social/gaming, classifieds, payments and  fintech, edtech, food delivery, and  ecommerce  11  ClearML/  Google  Nir Bar-Lev  Software 12  AWS  Chris Fregly  Cloud computing    Antje Barth   13  H&M  Errol  Koolmeister  Fashion 14  Metaflow/  Netflix  Ville Tuulos  Software/entertainment 15  Stack  Overflow  Prashanth  Chandrasekar  Software 16  Redfin  Akshat Kaul  Real estate  17  LEGO  Francesc Joan  Riera  Toys 18  ADP  Jack Berkowitz  Software  19  Capital One  Ali Rodell  Finance  20  T-Mobile  Heather Nolis  Telecommunications  21  Preset/  Airbnb  Maxime  Beauchemin  Software/travel 22  Gojek  Willem Pienaar  Multi-service platform N. Haefner et al. "
            },
            {
                "page_number": 4,
                "text": "Technological Forecasting & Social Change 197 (2023) 122878 4 aspects of implementing and scaling AI in organizations. Our analyses  show that there are three levers within the technical components cate­ gory. First, the data pipeline presents an intuitively important aspect.  Many organizations struggle with collecting and maintaining the data  necessary to fully implement and scale AI. Managing data is, therefore,  an important aspect of enabling the implementation and scaling of AI.  Second, the technical infrastructure refers to how and where AI systems  are developed in organizations. There are different choices that firms  can make in this area with implications for how well these AI systems  can be scaled. Third, AI models indicate the types of algorithmic  approach used by firms when developing their AI systems. These ap­ proaches can range from relatively simple traditional machine learning  approaches to ensembles of highly sophisticated deep learning-based  systems. 4.1.2. Social components  The other key dimension of successfully implementing and scaling AI  in organizations involves setting the right social context. Here too, we  see three important sub-dimensions of components. First, the AI growth  vision refers to the overall goal set by the organization regarding using AI  in the company. The vision sets the scope for choices taken within the  other dimensions and sub-dimensions. Second, AI capabilities are key in  successfully driving AI projects. Firms must bring together both tech­ nical and domain capabilities. Third, the AI organizational structure re­ lates to the organizational design of the AI team and its responsibilities  within the firm. Depending on the vision and the phase of implementing  and scaling AI in the company, different approaches will be chosen.  These components describe the main levers available to firms  attempting to implement and scale AI in their organizations. Our anal­ ysis shows that firms can develop different approaches to implementing  and scaling AI along these technical and social dimensions. 4.2. Implementing and scaling AI in organizations To successfully use AI, companies need to overcome certain hurdles  and develop essential internal capabilities. We used the insights gleaned  from machine learning and AI professionals at various companies to first  determine the key socio-technical components required to implement  and scale AI in firms. Next, we analyzed which capabilities these firms  developed to overcome the hurdles identified and to successfully use AI  in their respective firms.  This analysis of the interviews with machine learning and AI prac­ titioners indicated that there were two main areas requiring develop­ ment in firms to successfully use AI:  Develop the necessary technical infrastructure: Key technical de­ velopments are an important foundational driver for successful use of AI  in companies. These requisite technical developments are closely related  to the technical requirements of AI systems themselves. At a most basic  level, training an AI system involves combining the following assets:  collecting and managing data pipeline for training and validation,  providing the necessary technical infrastructure to carry out the training  and keep track of the deployed AI systems and, finally, hosting and  maintaining a set of AI models used in the systems. Across these areas,  firms have a variety of choices to make – for example, whether they  make or buy certain stacks, or how to design the various elements  modularly and flexibly.  Create the right social context: Having a suitable organizational  setting within which to develop AI solutions is very important to suc­ cessfully implement and scale AI in firms. First, organizations need to  have an appropriate guiding AI growth vision that sets the scope and  direction for AI implementation and scaling. Next, organizations must  develop a range of AI capabilities that allow the firm to harness the po­ tential of AI. This means building up both technical and domain capa­ bilities. Often firms can supplement their internal capabilities with those  offered by external partners to speed up the implementation and scaling  of AI. Finally, firms need to set up the organizational structure. To successfully navigate AI implementation and scaling, firms need to  combine the technical and business domain expertise needed to develop  AI solutions and drive value creation for the business.  Our analyses of the interviews with machine learning and AI experts  showed that there are various key activities in these two areas – tech­ nical and social components – supporting the implementation and  scaling of AI in firms. We categorized the key activities according to  different maturity levels to define the phases of AI implementation and  scaling in firms (Table 2). 4.2.1. Level 1. Proving the concept  The first phase of implementing and scaling AI emphasizes the firm  familiarizing itself with the potential of AI technologies and popular­ izing these within the organization. At this stage, firms can often rely on  external support to implement initial AI solutions that clearly demon­ strate the added value for the business.  Technical components: Our analyses show that, in this important  first phase of implementing and scaling AI, creating a first data pipeline is  highly important. This means starting to eliminate data silos and  ensuring that the data infrastructure is ready for the next phases of  implementing and scaling AI. Mike del Balso from Tecton/Uber noted:  “A big challenge that a lot of these companies have is that they’re still  not kind of at ‘data maturity,’ so then building ‘ML maturity’ on top of  that is a tricky spot to be in.” Without the necessary data pipeline  maturity, therefore, the next phases will become significantly more  difficult to reach.  Meanwhile, the approach to technical infrastructure can remain rela­ tively simple in this first phase of implementing AI. Proof-of-concept AI  applications can be developed on relatively modest infrastructure that is  available to most firms. For example, Heather Nolis explained that T-  Mobile started out with very rudimentary technical infrastructure: “We  originally released our models in R. It was just R in a Docker container as  an API. They ran pretty okay. We were doing two million returns a day.”  Many firms also avail themselves of externally available services as  offered, for example, by the large cloud providers. Daan Odijk from RTL  described the advantages of using externally available services thus: “By  taking these off-the-shelf models and then putting our own intelligence  in the second level, our own labeling and where we have the data in that  second level, that made a lot more sense for this learning problem as  well.”  The latter has the advantage of allowing your AI teams to scale their  systems more quickly in the future. Regarding AI model development,  most machine learning and AI practitioners recommend starting with  the easiest and simplest models possible. Errol Koolmeister describes the  lessons learned in this regard at H&M: “When we started out and some of the consultancy early use cases  and some of our early use cases, many of the people in the teams  threw themselves directly into the latest research, wanted to do  neural networks, wanted to get just a 0.1 uplift. But what we realized  as well is that this isn’t a Kaggle competition, it is not about opti­ mizing the metrics and that, then you’re done. It’s about carrying it  over into production into the infrastructure as well.” Beyond relying on simpler models to achieve quicker gains, our analysis  revealed that cloud providers often have services that allow firms to  build their own applications based on existing models. Heather Nolis  from T-Mobile recounted their own initial forays into model  development: “I can speak about our speech-to-text here where we originally did  roll out with vendor partners. We did a huge RFP (request for pro­ posal). Every major speech-to-text provider in the world that exists, I  have reviewed them. We launched our original proof of concept with  AWS Transcribe. We did use a vendor. But immediately, once we had  the audio data, we started looking at open source solutions and  saying what can we do on our specific data.” N. Haefner et al. "
            },
            {
                "page_number": 5,
                "text": "Technological Forecasting & Social Change 197 (2023) 122878 5 This quotation touches on important learning regarding AI models,  which is that firms should try to take advantage of the many open-source  options available.  Social components: At the social level, the first phase of imple­ menting AI requires the creation of an AI growth vision that focuses on  identifying and successfully pursuing both valuable and feasible proofs  of concept. To a certain extent, this allows firms to “pick the low hanging  fruit” early on, which clearly demonstrates the potential benefits of AI  technologies to a wide range of firm stakeholders, thereby improving  company-wide buy-in. For example, Nishan Subedi from Overstock  argued that “machine learning is, I think, best handled when there’s at  least clarity in terms of the objectives you want to achieve.” At T-Mobile,  meanwhile, Heather Nolis saw that it was important to “drive home a  culture of small models for small problems. Build things specific for your  use case to answer it exactly. Otherwise, you will get a deteriorated  product.”  In terms of AI capabilities, firms need to begin building their technical  capabilities so that the first AI applications can be successfully imple­ mented. Often, it can be a good tactic to look to external expertise – for  example, from consulting companies or large cloud providers to help  kick-start such early proofs of concept. Antje Barth from AWS explained  the advantages of using Amazon SageMaker: “There have been a lot of additions to this managed service that is  giving you basically the tools to build, to train, and to deploy models  easily. At the same time, it’s taking care of the infrastructure for you.  It does the heavy lifting of managing individual instances. You can  really focus on your tasks: to build models, to train the models, and to  deploy them.” Importantly, the firm should begin to create the right organizational  structure. In this phase, this means starting to build a central AI team.  This will allow the firm to begin focusing on and developing its AI ca­ pabilities. Errol Koolmeister explains that: “the H&M approach was to  do it centrally from the start basically to incubate the capability rather to  spread it out.” The organizational structure should enable good collab­ oration between the AI experts responsible for the technical imple­ mentation and the business domain experts who will be able to scope the  various AI projects so that they can create real value for the  organization. 4.2.2. Level 2. Productionizing  The second phase presents a relatively large leap forward in terms of  the impact of implementing and scaling AI. At this stage, firms must set  up the necessary technical and social components to allow AI systems to  work in production, meaning that these systems are running and sup­ porting a wide variety of business processes. The systems are being  tracked and continuously updated while they provide a very large number of (real-time) inferences and predictions.  Technical components: In the second phase of implementing and  scaling AI, it is important to have a data pipeline that is well organized so  that it can be easily re-used in the future. Often, firms will optimize re-  usability by storing processed data that can then be reapplied in many  use cases across the business as well as in possible new applications. Kim  Branson from GSK noted that it is important to: “build data for future  you so you can use it again. Collect those other data points at additional  marginal cost that are really useful.” Srivathsan Canchi from Intuit  underlined this point: “As we were building ML models for servicing these different sys­ tems, we were discovering that we are building similar features  because the data sets are highly intersecting. We have a lot of fea­ tures that need to go across these systems and be shared between  models; across turbo tax, quickbooks, and mint. To be effective at  sharing such features, we needed a way to do that.” Moreover, the analysis revealed another important aspect regarding the  data pipeline that firms faced during the “productionizing” phase. Spe­ cifically, the need to manage data access and ensure compliance in­ creases during this phase. Subarna Sinha from 23andMe noted her  company’s complex requirements in this regard: “We need to make sure that we are satisfying research compliance.  Then, we are also GDPR CCPA compliant. We have to have all of our  training data... Even though we save our training data for a little bit,  it expires. We have to have processes in place to destroy those  buckets at a regular frequency and regenerate all of the training data  as needed. The exact requirements regarding data access and compliance vary by  industry. Those companies operating in highly regulated industries,  such as 23andMe, or those involving specific customers, such as chil­ dren, must be able to address these issues very conscientiously in the  “productionizing” phase.  The technical infrastructure is perhaps the most important pillar of the  second phase of implementing and scaling AI. It must integrate all  relevant components of AI systems from data management to experi­ ment management, to orchestration and deployment management. In  this phase, firms can develop both new AI systems and continuously run  systems in production. A standardized infrastructure is the backbone  enabling this dual exploration and exploitation in AI systems. Errol  Koolmeister explains that H&M sees: “enormous productivity gain with  designing an infrastructure that’s reusable rather than building inde­ pendent use cases at this scale.” Firms need a deep understanding of the  processes involved in developing AI systems, which allows them to  standardize and automate many of the processes. Sushil Thomas from  Cloudera explains that productionizing AI systems differs markedly from Table 2  Implementing and scaling AI with socio-technical components. Phases of Implementing and Scaling AI Components  Proving Concept  Productionizing  Platformizing Technical  Data pipeline  Assemble data to run first use cases; enable data access;  begin eliminating data silos  Organize your data repositories; enable  access for variety of use cases; try to future-  proof Optimize for latency and  throughput; democratize access Technical  infrastructure  Simple on-premises setup; alternatively, rely on cloud  providers  Central reference architecture; standardized  and automated; tech agnostic; retain  flexibility Continually improve; eliminate  pain points of existing applications AI models  Experiment with suitable approaches for your specific use  cases; simple models over complicated ones; work with  open-sourced models Speed up experimentation; lower  complexity; iterate continuously  Reuse capabilities (e.g.,  forecasting) in different contexts Social  AI growth vision  Pursue valuable and feasible use cases; focus on efficiency  Customer value creation across value chain  Data-driven innovation   AI capabilities  Start building technical capabilities; bring in external  support for first use cases  Focus on speed; implement agile methods  Develop horizontal capabilities  that work across business lines   Organizational  structure  Start building a central AI team; ensure close  collaboration between technical and domain experts  Centrally organized AI team; vertical teams  to support lines of business  Establish overarching, horizontal  teams; increase (applied) research  focus N. Haefner et al. "
            },
            {
                "page_number": 6,
                "text": "Technological Forecasting & Social Change 197 (2023) 122878 6 traditional web development: “I think another challenge is just trying to understand what’s  different between that standard web app sort of development and  product development versus like a ML/AI model going into pro­ duction because there are large substantial differences that are really  important to internalize and understand so that you can focus on  different aspects of it as well.” In order to make broad use of AI across the organization, a solid tech­ nical infrastructure backbone is necessary. According to Akshat Kaul  from Redfin, building sound technological infrastructure is key to using  AI in different areas of the company: “What we’ve been trying to do more recently is really develop that  infrastructure, make it standardized, make it easy to use, really  democratize machine learning within the company, and allow peo­ ple teams across the business, across different domains to hire people  who have that machine learning talent or to grow that talent and  then use the platform that this team has built to tackle use cases in  different domains.” Interestingly, machine learning and AI practitioners mention that it can  be very helpful to be technologically agnostic and to retain a certain  level of flexibility in the exact infrastructure choice – for example,  regarding which cloud provider the firm chooses. This enables firms to  be able to switch to better options as they become available, which is  highly important in a field such as AI where many of the tools and  services are still developing and improving on a regular basis. Ya Xu  from Linkedin notes: “From [a] platform architecture standpoint, always  think about how to build a platform in an extensible way.” Jake Ber­ kowitz points out why maintaining flexibility is necessary: “The one thing you know about technology, somebody’s going to  come along with a better mousetrap. You need to be ready to re-  architect and refactor. We think about it architecturally. We  budget time for that as we go. It’s the reality of the cloud.” Some companies such as GSK, have further, more specific criteria to  build out their technical infrastructure. Kim Branson explains that: “the  compute is really important for us. It’s key to be unconstrained by  compute.” Such specific aspects will depend heavily on the use cases  pursued by the particular company, but they should also be considered  in developing the technical infrastructure.  In the “productionizing” phase of implementing and scaling AI, firms  can speed up experimentation on new models as well as continuously  improve existing AI models. Akshat Kaul from Redfin explains: “It’s an  ongoing effort. We are always working on improving the model that we  have in production.” Importantly, firms can often productionize their AI  particularly well when they place the emphasis on simpler rather than  more complex models. For instance, Errol Koolmeister from H&M rec­ ommends: “You don’t go with the most complex technology or algo­ rithm from the start because you don’t know how that will scale.”  Combined with the ability to continuously improve models, the upshot is  more reliable models overall. This aspect of the “productionizing” phase  is especially relevant for a company such as ADP. Jack Berkowitz il­ lustrates this point: “The second thing is about reliability because we’re messing with  people’s paychecks at the end of the day. That’s what we do. That’s  the most personal data there is other than maybe healthcare data. If  you want to see somebody get excited, make a mistake on their  paycheck. So, we have to have a little bit of reliability in terms of  what we’re doing.” Reliability is evidently an important aspect to productionizing AI sys­ tems because companies increasingly need to make sure their AI systems  provide accurate predictions and generate plausible output. Similarly,  they need to be able to keep track of the consistency of their AI systems.  Social components: In this phase of implementing and scaling AI, the AI growth vision changes to target the application of AI across the  value chain and throughout the entire organization. Kevin Stumpf de­ scribes the varied application areas for AI at Uber: “The use cases really  varied from everything from supporting self-driving cars to dynamic  pricing predictions to fraud detection, customer support, rider and order  ETAs, restaurant recommendations.” A key feature can be to prioritize  the time to customer value creation, i.e., focusing on those applications  and use cases where the organization can create real value for customers  in a short time frame. Subarna Sinha describes how productionizing AI  at 23andMe helped to change the growth vision for AI at the company: “I think when you see something happen this quickly and you’re able  to say ‘Oh I just want to train this and see what the results look like  even if I’m not going to ship it to customers tomorrow.’ It changes  people... It’s a delta, kind of a big shift in the way people think about  it.” At this stage, it is important to focus on one key AI capability, which is  speed. This involves ensuring that the organization can quickly experi­ ment with new AI applications and test their performance in production.  Adrien Gaidon explains how Toyota approaches this aspect of the  “productionizing” phase: “You want faster turnaround time and this kind of stuff. We want to  create some kind of Toyota production system of deep learning. It’s  so that we can iterate really quickly from idea to model to validation  and go back to the drawing board.” Machine learning and AI practitioners often work in agile ways,  completing projects in sprints. This approach is even adopted by heavily  research-oriented firms where the traditional pace is slower, and the  development roadmap is oriented much more to the long term. For  example, Kim Branson mentions that his company, GSK, “works in two-  week sprints.” Developing the capability to quickly iterate ideas and  products is important to satisfy customer requirements as well.  A strong centrally positioned organizational structure can be a key  component to transition to the second phase of implementing and  scaling AI. Many companies focus on building a core AI team to support  the various AI initiatives in the firm. Companies sometimes rely on  additional vertical teams that closely support a particular line of busi­ ness and help to solve problems specific to AI implementation in that  line of business. Sushil Thomas from Cloudera explains: “There are people who have a hub-and-spoke model where they have  a central sort of center of excellence with the these are the guys who  set up the best practices around the technologies they use and  deployment practices and stuff like that. Then business units will  have their own individual data scientists that they work with for  their actual use cases.” As part of the organizational structure for the “productionizing” phase, it  can be helpful to have key team members who evangelize for AI in the  organization. These team members help to bring the AI growth vision to  the entire organization. Ya Xu explains her approach at LinkedIn: “The way that I also see that’s worked really well is having this model  that I like to call the champion model. Let’s say for example you build  a platform and you have to convince ten other teams to use it. Don’t  say hey all of you guys come and use my thing, but start with a couple  of them who are already leaning in. They already showed interest,  they already are actually excited about this thing.” Positioning champions for AI in the organizational structure can,  therefore, be a very effective way to drive broad adoption of AI in the  firm. 4.2.3. Level 3. Platformizing  The third phase of AI implementation allows firms to truly scale their  AI systems. In “platformizing” AI in their organizations, businesses can  take advantage of size, reusing capabilities in different contexts to fully N. Haefner et al. "
            },
            {
                "page_number": 7,
                "text": "Technological Forecasting & Social Change 197 (2023) 122878 7 ramp up AI in the organization.  Technical components: In this phase of scaling AI, firms rely very  heavily on the previously established data pipeline and technical infra­ structure approaches. To fully scale AI in the organization, both data  management and infrastructure need to be optimized for latency and  throughput. In this phase, AI systems must often make lightning-fast  inferences and process vast amounts of data in real time. Indeed, dur­ ing the “platformized” phase, companies often have to plan quite  methodically for limitations in terms of inference speed and computing  power. Ali Rodell points out how Capital One walks this tightrope: “A lot of people think that compute is unlimited because we work in  the public cloud. Everybody reads compute is infinite in the cloud.  One of the things we try and do is make sure that people understand  that it is not. It is very scalable, but it is not unlimited.” The infrastructure should generally continue to improve in this phase to  consider and eliminate any pain points encountered in running existing  AI systems. To fully scale AI, it is helpful to democratize access to data  and infrastructure so that AI solutions can be developed more quickly  and by stakeholders with relatively less technical expertise. Kevin  Stumpf from Tecton/Uber notes that: “The more data scientists you  have, the more different use cases you have, the more sharing becomes a  big part of it.” Subarna Sinha shares the future potential she sees to  expand AI at 23andMe: “The way we look at it, we feel we’re really at kind of the beginning  of what machine learning can do in terms of giving information on  health. Right now a lot of our models primarily incorporate genetic  information but there is room for incorporating information from  wearables, information from lab values that you have, like your  blood report or some measure of some other reports that you can  have.” The most important technical component in the third phase of scaling AI  is the way AI models are developed at this stage. Specifically, firms in this  phase start to rely heavily on reusing capabilities. This means that they  tend to develop “platforms” for related models. For instance, many firms  find that forecasting is a problem that is highly relevant to many  different business areas. At LinkedIn, such AI-based capabilities are re-  used strategically, as described by Ya Xu: “I know that you are quite familiar with our proML platform. You can  actually build your modules on top of it. [...] A simple example is my  team actually developed this model [for] explainability capability.  They used it in their application. That went really well. They built it  as an extensible module on the platform so other people who wanted  to use it they can use it too.” In the “platformizing’ phase of scaling AI, firms focus on building  models that often open up the possibility of looking at a broader spec­ trum of related problems. Forecasting models, for instance, are germane  to general time series problems, which can be applied in various com­ pany areas.  Social components: At the organizational level, the AI growth vision  should now focus on the long-term strategy for AI to enable data-driven  innovation in the organization. Targeted use cases should now consider  reusability and applicability to multiple lines of business. Moreover,  firms should consider where to strategically invest in capabilities that  will likely become highly relevant for the business within the next few  years. Parvez Ahammad from LinkedIn explains how the company  thinks about this issue: “There are two investment pillars that I mentally use. One is how do  we pick problems that, essentially, if we solve them very well, help  multiple lines of businesses? Take, for example, something like doing  a really good job on experimentation, helps multiple lines of busi­ nesses evaluate how their products are working. [It] helps them  iteratively ship the products better. Something like forecasting, if you do a really good job, it also allows multiple businesses to have an  ability to set their goals and actually measure how things are going  and recognize when things are not going right. The utility across  multiple lines of business is a key pillar for how we think about it.  The second important pillar is what is the strategic scope or impact?  On this pillar, it doesn’t need to be actually a native part of the  products today but something that we believe is going to be really  important for us to invest in.” In terms of the AI capabilities required for this phase of scaling AI, here  too, we see that firms are changing their emphasis from vertical capa­ bilities specialized in business lines or functions to horizontal capabil­ ities that can scale across business lines and functions. Sushil Thomas  from Cloudera explains very succinctly that: “it’s important to just up-  level org-wide what you can do with all of your data.” Capabilities  should be managed as a portfolio with an eye to balancing shorter-term  capability development and more risky, longer-term capability devel­ opment. According to Parvez Ahammad, LinkedIn wants “to be able to  start new things or drop things that aren’t working and be much more  driven by market fit within the LinkedIn ecosystem.” By strategically  managing a portfolio of different AI projects, firms are able to exploit  current capabilities while crafting space to create new ones.  To support the vision and capability development, firms in this phase  of scaling AI often alter the organizational structure. Horizontal teams  that work on overarching capabilities such as forecasting are required to  fully scale AI platforms. These teams can have a more (applied) research  focus to drive horizontal capabilities more effectively. Errol Koolmeister  from H&M explains what the implications of the company’s plans to  scale AI are: “If we are 100 people today or 120-ish working on the AI use cases,  we have around 10 use cases in production right now, if we’re going  to have all our core operational decisions amplified by AI by 2025,  which is our tech leap that we’re aiming towards, we’re going to  need thousands of people if we’re scaling it vertically.” LinkedIn already operates its AI systems on a massive scale. Conse­ quently, they have adapted their organizational structure to be able to  effectively support the demands of these AI systems. Parvez Ahammad  describes the teams at LinkedIn: “One of the things that is relatively new... couple of years ago we  started these horizontal teams. There are at least a couple of them.  One of them is called Data Science Applied Research, which is my  team. There is a sister team that we have called Data Science Pro­ ductivity and Experimentation.” LinkedIn has created a new organizational structure for AI at scale,  which allows it to capitalize on the idea of reusable capabilities. The  company achieved this by building teams on overarching topics such as  applied research and experimentation. This setup allows LinkedIn to  drive these topics across business units and application areas. 5. Discussion 5.1. Theoretical and management implications The literature has repeatedly highlighted the potential of AI to  dramatically increase firm performance, regardless of industry (Berg  et al., 2018; Chui et al., 2018; Makarius et al., 2020). Some researchers  have argued that the largest technology firms in particular have built a  sustainable competitive advantage by shoring up their AI capabilities in  digital platforms (Iansiti and Lakhani, 2020). At the same time, many  other firms struggle to reap the rewards from AI (Browder et al., 2022;  Ransbotham et al., 2020). This poses an interesting question of why  some firms succeed while others fail to benefit from AI. Currently,  pressure is increasing due to the rapid change induced by ChatGPT and  other generative AI systems, which are substantially changing how N. Haefner et al. "
            },
            {
                "page_number": 8,
                "text": "Technological Forecasting & Social Change 197 (2023) 122878 8 businesses should be conducted in most industries (Edelman and  Abraham, 2023). Even though experts have long predicted that AI will  change the competitive arena and how to act in virtually all industries  (Berg et al., 2018; Chui et al., 2018), this is now happening fast, and  many firms are rapidly working on how best to implement and scale AI  in their business. In reviewing how companies are coping with these  challenges, we believe we are making several contributions to the  research in this field.  Past work on technology adoption has shown that successful  implementation centers on firms creating an appropriate socio-technical  system within which to foster the new technology. This has been shown  to be the case for IT adoption, for instance (Boothby et al., 2010;  Brynjolfsson et al., 2002). Several studies indicate that AI technology  contains socio-technical components that firms must manage in order to  benefit from it (Anthony et al., 2023; Glikson and Woolley, 2020; Leb­ ovitz et al., 2022; Makarius et al., 2020). Given that AI is generative,  malleable, and combinatorial (Kallinikos et al., 2013), and that it can  autonomously make predictions or generate outcomes and advance  these over time (Murray et al., 2021), firms are faced with sizeable  challenges in creating a suitable socio-technical system to benefit from  the technology. However, research on the socio-technical components  required to successfully exploit – that is to say, implement and scale – AI  is still very limited, and there have been calls for further research in this  area (Kanioura and Lucini, 2020; Makarius et al., 2020). The primary  contribution of our study, therefore, is to begin to address this gap in the  literature. Our exploratory qualitative approach allows us to gain a more  fine-grained understanding of the socio-technical components of  implementing and scaling AI. Specifically, we find that, on the technical  side, firms should build out their data pipelines, technical infrastructure,  and AI models. On the social side, firms should create an AI growth  vision, build AI capabilities, and create an organizational structure to  support the development of AI systems.  We further contribute to the literature by describing the phases of AI  implementation and scaling. We find that the above socio-technical  components play different roles during the process of moving from  implementing to scaling AI. Firms generally begin with “proving the  concept” of AI within the firm by launching their first viable use cases of  the technology. Then, as firms plan to use AI more systematically, they  advance to the “productionizing” phase. This phase is characterized by  an increased standardization of processes to enable firms to handle more  use cases. Finally, firms reach the “platformizing” phase, which allows  them to run AI at scale. This means that they address use cases across the  entire value chain, setting up the appropriate socio-technical compo­ nents to reuse capabilities across AI applications.  Moreover, this study contributes to our understanding of how com­ panies should act specifically when facing AI in their markets. We  identify four basic advantages of implementing and scaling AI. First,  implementing and scaling AI is considered a strategic necessity for most  industrial sectors. As Nir Bar-Lev, the CEO of Clear ML, notes: “You  probably want to have data scientists because if you want to be  competitive, in virtually every industry today, you have to integrate AI  into your business”. This assertion is strongly supported by recent  research on the applicability of AI, which indicates that AI can be  employed in almost any sector (Chui et al., 2018). Consequently, at a  most basic level, taking small steps to investigate how to implement and  scale AI is a prerequisite for firms to remain competitive in the market.  Firms that do not adopt AI risk being left behind.  Second, successfully implementing and, in particular, scaling AI are  necessary for firms to fully realize the financial benefits that the tech­ nology can offer. Firms at the very early stages of AI implementation  often struggle to do so (Ransbotham et al., 2020). These firms are  seemingly stuck in proof-of-concept purgatory. The transition to pro­ ductionized or platformized AI is difficult. Nir Bar-Lev explains how  difficult it was for AI powerhouse, Google, to accomplish the transition  only a few years ago when it worked to productionize an AI system that  could help the company improve electrical consumption in its data centers by 40 % (Evans and Gao, 2016): “That got a lot of attention, but  what wasn’t known outside was that building that initial model took  three weeks. Building a prototype to check it out, just validate it in one  data center – not a working product, but a prototype – took three  months. Rolling it out as a product? Over a year.” Since there is still a  considerable hurdle for firms to surmount in employing AI profitably, it  is important to address this question (Browder et al., 2022). Bringing AI  fully into the production environment allows firms to reap the financial  benefits from these systems. This study furthers our understanding by  identifying the key socio-technical components that firms scaling AI  need to employ in their organizations.  Third, our study shows that productionizing and platformizing in­ crease the speed, reliability, and explainability of AI systems. By stan­ dardizing and automating many of the core functions in AI systems,  productionizing and platformizing improve the efficiency of AI. Jack  Berkowitz, chief data officer at ADP, describes the main advantages his  company sees in productionizing AI: “The first one is about pace. The  world’s busy, clients are demanding, and the world situations are  changing all the time. The second thing is about reliability […] the third  thing is really about clarity and explainability, whether it’s to each other  inside the development teams or whether it’s to our end clients.” By  offering a systematic approach to AI development, productionizing  creates the foundation for measurable and benchmarkable success. Such  features are highly important because AI systems are often biased and it  is, therefore, important to ensure that the systems run as intended and  can be managed appropriately. This is perhaps especially true for  generative AI systems that are prone to creating unreliable output.  Consequently, firms should create the best possible socio-technical  system to support their AI endeavors (Jackson, 2023).  Finally, our study indicates that platformizing AI enables entirely  new opportunities for firms. This phase of scaling AI means that busi­ nesses can pursue novel data-driven innovation. The advanced tech­ niques and reuse of capabilities create space for the firm to discover new  prospects. Parvez Ahammad, head of data science applied research at  LinkedIn, explains how working on one reusable capability has opened  new paths for the company: “One of the nice side effects of getting the  forecasting part right has been that we now are starting to look at a  broader spectrum of time series problems. Forecasting is a sister problem  to anomaly detection. Most people that come to you and ask for fore­ casting or anomaly detection also want a root cause analysis. There are a  lot of statistical problems that are very adjacent. Initially, we were very  focused on one, and when we got it right we feel like we have a p(0)  answer. We are trying to slowly build something that’s more holistic.”  Ultimately, the team thinks these developments will allow it to “change  the culture of performance management [and make forecasting] a much  more natural component of how people do matrix performance man­ agement.” This example nicely illustrates how entirely new possibilities  can be shaped by platformizing AI in organizations. In this phase, the  potential is diverse, and firms will be able to create new data-driven  initiatives supported by AI-based capabilities. This finding of our  study is of particular interest for the AI literature on innovation and  management because it indicates that AI may be a key way for firms to  create capabilities going forward (Kemp, 2023). Our study is, therefore,  among the first to show which socio-technical components are pertinent  in creating AI-based capabilities and how firms can navigate the process  of gaining competitive advantage using AI capabilities.  These benefits are findings uncovered from our analysis of the early  stages. The full effects of scaling AI are presumably even more extensive.  As noted, the implementation and scaling of AI in organizations is pre­ dicted to bring wide-ranging changes in essentially all sectors of industry  and will substantially contribute to economic growth (Bughin et al.,  2018; Chui et al., 2018). To fully take advantage of the potential benefits  of AI, managers must pull the right socio-technical levers in their or­ ganizations. They need to develop the right technical and social com­ ponents for AI. N. Haefner et al. "
            },
            {
                "page_number": 9,
                "text": "Technological Forecasting & Social Change 197 (2023) 122878 9 5.2. Limitations and paths for future research This study has some important limitations that, at the same time,  point to certain interesting paths for future research. First and foremost,  as a purely qualitative study, generalizability from our findings is  somewhat limited. It would be a very worthwhile endeavor for future  research to quantitatively examine the implementation and scaling of AI  in organizations. In particular, it would be interesting for scholars to  examine the extent to which employee knowledge and skills impact  firms’ ability to implement and scale AI because these factors are of  considerable importance for firms attempting to benefit fully from  modern technologies (Zheng and Hu, 2008). Relatedly, future work  could consider studying how firms’ plans to implement and scale AI  interact with other firm strategies. For example, recent research has  shown that some firm strategies are more conducive to good perfor­ mance in turbulent market conditions (Beliaeva et al., 2020). Conse­ quently, it may be fruitful to examine how other firm strategies support  or impede technology adoption in firms.  Second, the literature on digital transformation, which is broader  than AI transformation specifically, suggests that the external context  can markedly influence the extent to which firms can and must trans­ form (Hanelt et al., 2021). In line with this research, it may be helpful for  future research to consider additional external conditions affecting  firms’ AI transformations. Future research may also benefit from  analyzing the larger labor market implications of increased AI imple­ mentation and scaling. Past research has indicated that adoption of AI  and robotics can have a significant impact on the labor market (Autor,  2015; Chen et al., 2022; Dixon et al., 2021). Since the implications of AI  adoption are still ambiguous, with some studies suggesting that  increased use of the technology will lead to lower employment and  others arguing the opposite, it would be beneficial for future work to  examine whether the AI transformation approach chosen by companies  affects AI’s impact on employees.  Finally, the current study did not limit itself in terms of industry, firm  size, or geographical location. Future work could, therefore, examine  how AI transformation is affected by the industry context, firm size, and  sector location. Such analyses would allow the literature to establish  more generalizable findings that hold across industry, firm size, and  geography as well as to determine specific findings for smaller sets  operating under special conditions. 6. Conclusion The rapid advancement of AI presents significant challenges for  companies, requiring them to adapt and navigate in order to integrate AI  into their operations. To stay competitive, companies must proactively  engage with AI technologies, strategically invest in talent and infra­ structure, and establish a comprehensive AI framework to drive inno­ vation and shape industry standards. This paper has argued that  knowledge of AI implementation and scaling is rapidly needed and that  it is time to review best practice. Our effort provides an initial frame­ work and scientific model of AI implementation and AI scaling. Devel­ oping and implementing AI is known to be a difficult and challenging  undertaking, but our accounts provide a list of benefits. Companies that  succeed in implementing and scaling AI can ensure they remain  competitive, drive value creation across the value chain, unlock effi­ cient, reliable, and explainable AI solutions, and develop new AI-based  capabilities. Success hinges on managing the implementation and  scaling of AI by pulling the right socio-technical levers – developing a  data pipeline, technical infrastructure, and AI models – while setting the  right social context of laying out the AI growth vision, expanding AI  capabilities, and establishing a suitable organizational structure. The  framework we provide in this paper offers guidance on how to imple­ ment and scale AI to facilitate the successful use of AI in companies,  leading the way to the next generation of highly innovative and  competitive companies. CRediT authorship contribution statement Naomi HAEFNER: Conceptualization, Writing - Original Draft,  Writing - Review & Editing, Data Curation, Investigation, Methodology,  Formal analysis, Visualization, Project administration, and Funding  acquisition. Vinit PARIDA: Writing - Review & Editing, Investigation,  Conceptualization, and Methodology. Oliver GASSMAN: Writing - Re­ view & Editing, and Funding acquisition. Joakim WINCENT: Supervi­ sion, Conceptualization, and Editing. Data availability The data that has been used is confidential. Acknowledgements The authors would like to express their sincere appreciation to The  Research Council of Norway, the Swedish Energy Agency, and Wal­ lander’s och Tom Hedeliu’s Stiftelse samt Tore Browaldhs Stiftelse for  their funding support, which made this research possible. References Acemoglu, D., Restrepo, P., 2020. The wrong kind of AI? Artificial intelligence and the  future of labour demand. Camb. J. Reg. Econ. Soc. 13 (1), 25–35. https://doi.org/  10.1093/cjres/rsz022.  Ansari, S., Garud, R., 2009. Inter-generational transitions in socio-technical systems: the  case of mobile communications. Res. Policy 38 (2), 382–392. https://doi.org/  10.1016/j.respol.2008.11.009.  Anthony, C., 2021. When knowledge work and analytical technologies collide: the  practices and consequences of black boxing algorithmic technologies. Adm. Sci. Q.  66 (4), 1173–1212. https://doi.org/10.1177/00018392211016755.  Anthony, C., Bechky, B.A., Fayard, A.-L., 2023. “Collaborating” with AI: taking a system  view to explore the future of work. Organ. Sci. 34 (5), 1651–1996. https://doi.org/  10.1287/orsc.2022.1651.  Appelbaum, S.H., 1997. Socio-technical systems theory: an intervention strategy for  organizational development. Manag. Decis. 35 (6), 452–463. https://doi.org/  10.1108/00251749710173823.  Autor, D.H., 2015. Why are there still so many jobs? The history and future of workplace  automation. J. Econ. Perspect. 29 (3), 3–30. https://doi.org/10.1257/jep.29.3.3.  Balasubramanian, N., Ye, Y., Xu, M., 2022. Substituting human decision-making with  machine learning: implications for organizational learning. Acad. Manag. Rev. 47  (3), 448–465. https://doi.org/10.5465/amr.2019.0470.  Beliaeva, T., Shirokova, G., Wales, W., Gafforova, E., 2020. Benefiting from economic  crisis? Strategic orientation effects, trade-offs, and configurations with resource  availability on SME performance. Int. Entrep. Manag. J. 16 (1), 165–194. https://  doi.org/10.1007/s11365-018-0499-2.  Berg, A., Buffie, E.F., Zanna, L.-F., 2018. Should we fear the robot revolution? (The  correct answer is yes). In: IMF Working Paper. https://www.imf.org/-/media/Files/  Publications/WP/2018/wp18116.ashx.  Boothby, D., Dufour, A., Tang, J., 2010. Technology adoption, training and productivity  performance. Res. Policy 39 (5), 650–661. https://doi.org/10.1016/j.  respol.2010.02.011.  Braun, V., Clarke, V., 2006. Using thematic analysis in psychology. Qual. Res. Psychol. 3  (2), 77–101. https://doi.org/10.1191/1478088706qp063oa.  van den Broek, E., Sergeeva, A., Huysman, M., 2021. When the machine meets the  expert: an ethnography of developing ai for hiring. MIS Quarterly: Management  Information Systems 45 (3), 1557–1580. https://doi.org/10.25300/MISQ/2021/  16559.  Browder, R.E., Koch, H., Long, A., Hernandez, J.M., 2022. Learning to innovate with big  data analytics in Interorganizational relationships. Academy of Management  Discoveries 8 (1), 139–166. https://doi.org/10.5465/amd.2019.0048.  Brynjolfsson, E., Hitt, L.M., Yang, S., 2002. Intangible assets: computers and  organizational capital. Brook. Pap. Econ. Act. 2002 (1), 137–198. https://doi.org/  10.1353/eca.2002.0003.  Bughin, J., Seong, J., Manyika, J., Chui, M., Joshi, R., 2018. Notes from the AI frontier:  Modeling the global economic impact of AI. In: McKinsey Global Institute,  September. https://www.mckinsey.com/~/media/McKinsey/FeaturedInsights/Art  ificialIntelligence/NotesfromthefrontierModelingtheimpactofAIontheworldeconom  y/MGI-Notes-from-the-AI-frontier-Modeling-the-impact-of-AI-on-the-world-econom  y-September-2018.ashx%0A.  Chalmers, D., MacKenzie, N.G., Carter, S., 2021. Artificial intelligence and  entrepreneurship: implications for venture creation in the fourth industrial  revolution. Entrep. Theory Pract. 45 (5), 1028–1053. https://doi.org/10.1177/  1042258720934581.  Charrington, S., 2022. About. TWIML AI Website. https://twimlai.com/about/.  Chen, N., Sun, D., Chen, J., 2022. Digital transformation, labour share, and industrial  heterogeneity. J. Innov. Knowl. 7 (2), 100173. https://doi.org/10.1016/j.  jik.2022.100173. N. Haefner et al. "
            },
            {
                "page_number": 10,
                "text": "Technological Forecasting & Social Change 197 (2023) 122878 10 Chowdhury, S., Budhwar, P., Dey, P.K., Joel-Edgar, S., Abadie, A., 2022. AI-employee  collaboration and business performance: integrating knowledge-based view, socio-  technical systems and organisational socialisation framework. J. Bus. Res. 144,  31–49. https://doi.org/10.1016/j.jbusres.2022.01.069.  Chui, M., Manyika, J., Miremadi, M., Henke, N., Chung, R., Nel, P., Malhotra, S., 2018.  Notes from the AI frontier: Insights from hundreds of use cases. In: McKinsey Global  Institute. https://www.mckinsey.com/featured-insights/artificial-intelligence/no  tes-from-the-ai-frontier-applications-and-value-of-deep-learning.  Chui, M., Roberts, R., Yee, L., 2022, December. Generative AI Is here: How Tools like  ChatGPT Could Change your Business. McKinsey QuantumBlack. https://www.mcki  nsey.com/capabilities/quantumblack/our-insights/generative-ai-is-here-how-tools-  like-chatgpt-could-change-your-business.  Dixon, J., Hong, B., Wu, L., 2021. The robot revolution: managerial and employment  consequences for firms. Manag. Sci. 67 (9), 5586–5605. https://doi.org/10.1287/  mnsc.2020.3812.  Edelman, D.C., Abraham, M., 2023. Generative AI Will Change your Business. Here’s  How To Adapt. Harvard Business Review Digital Articles. https://hbr.org/2023/04  /generative-ai-will-change-your-business-heres-how-to-adapt.  Eisenhardt, K.M., Graebner, M.E., Sonenshein, S., 2016. Grand challenges and inductive  methods: rigor without rigor mortis. Acad. Manag. J. 59 (4), 1113–1123. https://doi.  org/10.5465/amj.2016.4004.  Evans, R., Gao, J., 2016. DeepMind AI Reduces Google Data Centre Cooling Bill by 40%.  DeepMind Blog. https://deepmind.com/blog/article/deepmind-ai-reduces-google-d  ata-centre-cooling-bill-40.  Fisher, G., Stevenson, R., Neubert, E., Burnell, D., Kuratko, D.F., 2020. Entrepreneurial  hustle: navigating uncertainty and enrolling venture stakeholders through urgent  and unorthodox action. J. Manag. Stud. 57 (5), 1002–1036. https://doi.org/  10.1111/joms.12584.  Fountaine, T., McCarthy, B., Saleh, T., 2021. Getting AI to scale. Harv. Bus. Rev. 99 (3),  116–123. https://hbr.org/2021/05/getting-ai-to-scale.  Geels, F.W., 2002. Technological transitions as evolutionary reconfiguration processes: a  multi-level perspective and a case-study. Res. Policy 31 (8–9), 1257–1274. https://  doi.org/10.1016/S0048-7333(02)00062-8.  Geels, F.W., 2004. From sectoral systems of innovation to socio-technical systems. Res.  Policy 33 (6–7), 897–920. https://doi.org/10.1016/j.respol.2004.01.015.  Gehman, J., Glaser, V.L., Eisenhardt, K.M., Gioia, D., Langley, A., Corley, K.G., 2018.  Finding theory–method fit: A comparison of three qualitative approaches to theory  building. J. Manag. Inq. 27 (3), 284–300. https://doi.org/10.1177/  1056492617706029.  Glikson, E., Woolley, A.W., 2020. Human trust in artificial intelligence: review of  empirical research. Acad. Manag. Ann. 14 (2), 627–660. https://doi.org/10.5465/  annals.2018.0057.  Hanelt, A., Bohnsack, R., Marz, D., Antunes Marante, C., 2021. A systematic review of the  literature on digital transformation: insights and implications for strategy and  organizational change. J. Manag. Stud. 58 (5), 1159–1197. https://doi.org/  10.1111/joms.12639.  Hassabis, D., 2022. AlphaFold reveals the structure of the protein universe. Google  DeepMind. July 28. https://www.deepmind.com/blog/alphafold-reveals-the-structu  re-of-the-protein-universe.  Holmstr¨om, J., 2022. From AI to digital transformation: the AI readiness framework. Bus.  Horiz. 65 (3), 329–339. https://doi.org/10.1016/j.bushor.2021.03.006.  Iansiti, M., Lakhani, K.R., 2020. Competing in the age of AI. Harv. Bus. Rev. 98 (1),  60–67. https://hbr.org/2020/01/competing-in-the-age-of-ai.  Jackson, B., 2023. Consider the Risks of Generative AI Before Adopting Game-Changing  Tools. Forbes. March 3. https://www.forbes.com/sites/forbestechcouncil/2023/03  /03/consider-the-risks-of-generative-ai-before-adopting-game-changing-tools/.  Johnson, P.C., Laurell, C., Ots, M., Sandstr¨om, C., 2022. Digital innovation and the  effects of artificial intelligence on firms’ research and development – automation or  augmentation, exploration or exploitation? Technol. Forecast. Soc. Chang. 179,  121636. https://doi.org/10.1016/j.techfore.2022.121636.  Kallinikos, J., Aaltonen, A., Marton, A., 2013. The ambivalent ontology of digital  artifacts. MIS Q. 37 (2), 357–370.  Kanioura, A., Lucini, F., 2020. A Radical Solution to Scale AI Technology. Harvard  Business Review Digital Articles. https://hbr.org/2020/04/a-radical-solution-to-  scale-ai-technology.  Kemp, A., 2023. Competitive advantages through artificial intelligence: toward a theory  of situated AI. Acad. Manag. Rev. https://doi.org/10.5465/amr.2020.0205.  Lakshmi, V., Bahli, B., 2020. Understanding the robotization landscape transformation: A  centering resonance analysis. J. Innov. Knowl. 5 (1), 59–67. https://doi.org/  10.1016/j.jik.2019.01.005.  Leavitt, H.J., 1965. Applied organizational change in industry. In: March, J.G. (Ed.),  Handbook of Organizations. Rand McNally, p. 1170.  Lebovitz, S., Lifshitz-Assaf, H., Levina, N., 2022. To engage or not to engage with AI for  critical judgments: how professionals deal with opacity when using AI for medical  diagnosis. Organ. Sci. 33 (1), 1–494. https://doi.org/10.1287/orsc.2021.1549.  Li, F.G.N., Trutnevyte, E., Strachan, N., 2015. A review of socio-technical energy  transition (STET) models. Technol. Forecast. Soc. Chang. 100, 290–305. https://doi.  org/10.1016/j.techfore.2015.07.017.  Makarius, E.E., Mukherjee, D., Fox, J.D., Fox, A.K., 2020. Rising with the machines: A  sociotechnical framework for bringing artificial intelligence into the organization.  J. Bus. Res. 120, 262–273. https://doi.org/10.1016/j.jbusres.2020.07.045.  McCarthy, J., 2007. What is artificial intelligence? http://jmc.stanford.edu/artificial  -intelligence/what-is-ai/index.html.  Meta AI, 2023, February 24. Introducing LLaMA: A Foundational, 65-Billion-Parameter  Large Language Model. Meta AI Blog. https://ai.facebook.com/blog/large-langua  ge-model-llama-meta-ai/. Midjourney. (2022, July 13). We’re officially moving to open-beta! Join now at https://  discord.gg/midjourney. **Please read our directions carefully** or check out our  detailed how-to guides here: https://midjourney.gitbook.io/docs. Most importantly,  have fun! [Tweet]. Twitter. https://twitter.com/midjourney/status/1547108864  788553729.  Mishra, S., Ewing, M.T., Cooper, H.B., 2022. Artificial intelligence focus and firm  performance. J. Acad. Mark. Sci. 50 (6), 1176–1197. https://doi.org/10.1007/  s11747-022-00876-5.  Mitchell, T.M., 1997. Machine learning. In: Machine Learning, , 13th45. McGraw-Hill  Science.  Münch, C., Marx, E., Benz, L., Hartmann, E., Matzner, M., 2022. Capabilities of digital  servitization: evidence from the socio-technical systems theory. Technol. Forecast.  Soc. Chang. 176 (February 2021) https://doi.org/10.1016/j.techfore.2021.121361.  Murray, A., Rhymer, J., Sirmon, D.G., 2021. Humans and technology: forms of conjoined  agency in organizations. Acad. Manag. Rev. 46 (3), 552–571. https://doi.org/  10.5465/amr.2019.0186.  Obschonka, M., Audretsch, D.B., 2020. Artificial intelligence and big data in  entrepreneurship: A new era has begun. Small Bus. Econ. 55 (3), 529–539. https://  doi.org/10.1007/s11187-019-00202-4.  OpenAI, 2022a. Introducing ChatGPT. OpenAI Blog. https://openai.com/blog/chatgpt.  OpenAI, 2022b. DALL-E Now Available in Beta. OpenAI Blog. July 20. https://openai.co  m/blog/dall-e-now-available-in-beta.  OpenAI, 2023. GPT-4. OpenAI. March 14. https://openai.com/research/gpt-4.  OpenAI, Pilipiszyn, A., 2021. GPT-3 Powers the Next Generation of Apps. OpenAI Blog.  March 25. https://openai.com/blog/gpt-3-apps.  Pachidi, S., Berends, H., Faraj, S., Huysman, M., 2021. Make way for the algorithms:  symbolic actions and change in a regime of knowing. Organ. Sci. 32 (1), 18–41.  https://doi.org/10.1287/orsc.2020.1377.  Peretz-Andersson, E., Torkar, R., 2022. Empirical AI transformation research: A  systematic mapping study and future agenda. E-Informatica Software Engineering  Journal 16 (1), 220108. https://doi.org/10.37190/e-Inf220108.  Pichai, S., 2023. An Important Next Step on our AI Journey. Google: The Keyword.  February 6. https://blog.google/technology/ai/bard-google-ai-search-updates/.  Raisch, S., Krakowski, S., 2021. Artificial intelligence and management: the  automation–augmentation paradox. Acad. Manag. Rev. 46 (1), 192–210. https://doi.  org/10.5465/AMR.2018.0072.  Ransbotham, S., Kiron, D., Gerbert, P., Reeves, M., 2017. Reshaping business with  artificial intelligence. MIT Sloan Mangement Review 59 (1), 1–17. http://sloanrevie  w.mit.edu/AI2017.  Ransbotham, S., Khodabandeh, S., Kiron, D., Candelon, F., Chu, M., LaFountain, B.,  2020. Expanding AI’s Impact With Organizational Learning. https://sloanreview.  mit.edu/ai2020.  Shani, A. B.(Rami), Grant, R.M., Krishnan, R., Thompson, E., 1992. Advanced  manufacturing systems and organizational choice: sociotechnical system approach.  Calif. Manag. Rev. 34 (4), 91–111. https://doi.org/10.2307/41166705.  Shin, D.-H., Choo, H., Beom, K., 2011. Socio-technical dynamics in the development of  next generation Mobile network: translation beyond 3G. Technol. Forecast. Soc.  Chang. 78 (3), 514–525. https://doi.org/10.1016/j.techfore.2010.04.017.  Sony, M., Naik, S., 2020. Industry 4.0 integration with socio-technical systems theory: A  systematic review and proposed theoretical model. Technol. Soc. 61, 101248  https://doi.org/10.1016/j.techsoc.2020.101248.  Stability AI, 2022. Stable diffusion launch announcement. Stability AI Blog. August 10. https://stability.ai/blog/stable-diffusion-announcement.  The Economist. Huge “foundation models” are turbo-charging AI progress. June 11. htt  ps://www.economist.com/interactive/briefing/2022/06/11/huge-foundation-mo  dels-are-turbo-charging-ai-progress.  Wang, Y., Su, X., 2021. Driving factors of digital transformation for manufacturing  enterprises: A multi-case study from China. Int. J. Technol. Manag. 87 (2/3/4), 229.  https://doi.org/10.1504/IJTM.2021.120932.  Xing, F., Peng, G., Zhang, B., Li, S., Liang, X., 2021. Socio-technical barriers affecting  large-scale deployment of AI-enabled wearable medical devices among the ageing  population in China. Technol. Forecast. Soc. Chang. 166, 120609. https://doi.org/  10.1016/j.techfore.2021.120609.  Yun, S., Lee, J., 2015. Advancing societal readiness toward renewable energy system  adoption with a socio-technical perspective. Technol. Forecast. Soc. Chang. 95,  170–181. https://doi.org/10.1016/j.techfore.2015.01.016.  Zeba, G., Dabi´c, M., ˇCiˇcak, M., Daim, T., Yalcin, H., 2021. Technology mining: artificial  intelligence in manufacturing. Technol. Forecast. Soc. Chang. 171, 120971. https://  doi.org/10.1016/j.techfore.2021.120971.  Zheng, C., Hu, M.-C., 2008. Challenge to ICT manpower planning under the economic  restructuring: empirical evidence from MNCs in Singapore and Taiwan. Technol.  Forecast. Soc. Chang. 75 (6), 834–853. https://doi.org/10.1016/j.  techfore.2007.05.002.  Zolas, N., Kroff, Z., Brynjolfsson, E., McElheran, K., Beede, D.N., Buffington, C.,  Goldschlag, N., Foster, L., Dinlersoz, E., 2020. Advanced technologies adoption and  use by U.S. firms: evidence from the annual business survey. In: NBER Working  Paper (No. 28290). Naomi Haefner is an Assistant Professor of Technology Management at the University of  St. Gallen, Switzerland. Her research interests include the impact of artificial intelligence  on management and innovation in organizations. Her prior work has been published in  Entrepreneurship Theory and Practice, Technological Forecasting and Social Change, and  Small Business Economics. N. Haefner et al. "
            },
            {
                "page_number": 11,
                "text": "Technological Forecasting & Social Change 197 (2023) 122878 11 Vinit Parida is a Professor in Entrepreneurship and Innovation at Luleå University of  Technology, Sweden. His research interests include servitization, business models, open  innovation, and organizational capabilities. He has published in journal articles such as,  Academy of Management Journal, Strategic Management Journal, Journal of Management  Studies, Long Range Planning, Industrial Marketing Management, Production and Oper­ ations Management, Journal of Cleaner Production, and others. Oliver Gassman is Professor of Innovation Management at the University of St. Gallen,  Switzerland. His-research interests include technology, business model innovation, open  innovation, and innovation patterns. His-articles have been published in journals such as Journal of Management, Research Policy, Long Range Planning, Strategic Entrepreneur­ ship Journal, Journal of Product Innovation Management, R&D Management, and others. Joakim Wincent is Professor in Entrepreneurship and Management at Hanken School of  Economics, Finland, and a Professor in Entrepreneurship and Innovation at University of  St. Gallen. His current research interests include technology, artificial intelligence, man­ agement, and innovation in organizations. Previous articles by him have been published in  journals such as the Academy of Management Review, Strategic Management Journal,  Harvard Business Review, Journal of Management Studies, Journal of Business Venturing,  Entrepreneurship Theory and Practice and others. N. Haefner et al. "
            }
        ],
        "images": [
            "Image_863",
            "Image_866",
            "Image_871"
        ]
    },
    {
        "file_name": "precission_ai.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Precision and Recall in Machine Learning Precision and recall are important measures in machine learning that assess the performance  of a model. Precision evaluates the correctness of positive predictions, while recall determines  how well the model recognizes all pertinent instances. The balance between accuracy and  completeness is frequently emphasized in the precision vs recall discussion, as enhancing one  may result in a reduction in the other. The precision recall f1 score merges both measurements  to give a well-rounded assessment. Comprehending the difference between precision and  recall is crucial in the creation of successful machine learning models. 1. What is Precision? 2. What is Recall? 3. What is a Confusion Matrix? 4. What is Accuracy Metric? 5. Precision vs Recall in Machine Learning 6. Precision and Recall Example 7. Choosing between Precision and Recall 8. Understanding the Problem Statement 9. The Role of the F1-Score 10. False Positive Rate & True Negative Rate 11. Receiver Operating Characteristic Curve (ROC Curve) 12. Precision-Recall Curve (PRC) 13. Conclusion What is Precision? In the simplest terms, Precision is the ratio between the True Positives and all the Positives. For  our problem statement, that would be the measure of patients that we correctly identify as  having a heart disease out of all the patients actually having it. Mathematically: What is the Precision for our model? Yes, it is 0.843, or when it predicts that a patient has heart  disease, it is correct around 84% of the time. Precision also gives us a measure of the relevant data points. It is important that we don’t start  treating a patient who actually doesn’t have a heart ailment but our model predicted it as having  it. What is Recall? "
            },
            {
                "page_number": 2,
                "text": "The recall is the measure of our model correctly identifying True Positives. Thus, for all the  patients who actually have heart disease, recall tells us how many we correctly identified as  having a heart disease. Mathematically: For our model, Recall  = 0.86. Recall also gives a measure of how accurately our model is able to  identify the relevant data. We refer to it as Sensitivity or True Positive Rate. What if a patient has  heart disease, but no treatment is given to him/her because our model predicted so? That is a  situation we would like to avoid! What is a Confusion Matrix? A confusion matrix helps us gain insight into how correct our predictions were and how they  hold up against the actual values. From our training and test data, we already know that our test data consisted of 91 data points.  That is the 3rd row and 3rd column value at the end. We also notice that there are some actual  and predicted values. The actual values are the number of data points that were originally  categorized into 0 or 1. The predicted values are the number of data points our KNN model  predicted as 0 or 1. The actual values are:    The patients who actually don’t have a heart disease = 41    The patients who actually do have a heart disease = 50 The predicted values are:    Number of patients who were predicted as not having a heart disease = 40    Number of patients who were predicted as having a heart disease = 51 All the values we obtain above have a term. Let’s go over them one by one:    The cases in which the patients actually did not have heart disease and our model also  predicted as not having it are called the True Negatives. For our matrix, True Negatives =  33.    The cases in which the patients actually have heart disease and our model also  predicted as having it are called the True Positives. For our matrix, True Positives = 43    However, there are some cases where the patient actually has no heart disease, but our  model has predicted that they do. This kind of error is the Type I Error, and we call the  values False Positives. For our matrix, False Positives = 8    Similarly, there are some cases where the patient actually has heart disease, but our  model has predicted that they doesn’t. This kind of error is a Type II Error, and we call the  values False Negatives. For our matrix, False Negatives = 7 What is Accuracy Metric? "
            },
            {
                "page_number": 3,
                "text": "Now we come to one of the simplest metrics of all, Accuracy. Accuracy is the ratio of the total  number of correct predictions and the total number of predictions. Can you guess what the  formula for Accuracy will be? For our model, Accuracy will be = 0.835. Using accuracy as a defining metric for our model makes sense intuitively, but more often than  not, it is advisable to use Precision and Recall too. There might be other situations where our  accuracy is very high, but our precision or recall is low. Ideally, for our model, we would like to  avoid any situations where the patient has heart disease completely, but our model classifies as  him not having it, i.e., aim for high recall. On the other hand, for the cases where the patient is not suffering from heart disease and our  model predicts the opposite, we would also like to avoid treating a patient with no heart disease  (crucial when the input parameters could indicate a different ailment, but we end up treating  him/her for a heart ailment). Although we do aim for high precision and high recall value, achieving both at the same time is  not possible. For example, if we change the model to one giving us a high recall, we might detect  all the patients who actually have heart disease, but we might end up giving treatments to many  patients who don’t suffer from it. Similarly, suppose we aim for high precision to avoid giving any wrong and unrequired  treatment. In that case, we end up getting a lot of patients who actually have heart disease going  without any treatment. Precision vs Recall in Machine Learning For any machine learning model, achieving a ‘good fit’ on the model is crucial. This involves  achieving the actual positives, such as the balance between underfitting and overfitting, or in  other words, a trade-off between bias and variance. However, when it comes to classification, another trade-off is often overlooked in favor of the  bias-variance trade-off. This is the precision-recall trade-off. Imbalanced classes occur  commonly in datasets. When it comes to specific use cases, we would, in fact, like to give more  importance to the precision and recall metrics and how to balance them. But how to do so? This article will explore the classification evaluation metrics by focussing on  precision and recall. We will also learn to calculate these metrics in Python by taking a dataset  and a simple classification algorithm. So, let’s get started! You can learn about evaluation metrics in-depth here-Evaluation Metrics for Machine  Learning Models. Precision and Recall Example Precision and recall with an example in machine learning: Imagine a spam email detection system. Here’s how we can understand precision and recall in  this context: "
            },
            {
                "page_number": 4,
                "text": "Precision:    Focuses on the correctness of positive predictions.    Asks: “Out of all the emails flagged as spam, what proportion were actually spam?” Recall:    Emphasizes capturing all relevant instances.    Asks: “Out of all the actual spam emails, what proportion did the system correctly  identify?” Example:    Let’s say the system identifies 8 emails as spam out of a dataset of 12 emails.    Of the 8 classified as spam, only 5 are truly spam.    Precision = (Correctly Identified Spam) / (Total Emails Identified as Spam) = 5 / 8    The system has a precision of 62.5%, meaning 62.5% of the emails it flagged as spam  were actual spam.    Now, suppose there were actually 12 spam emails in the dataset.    Recall = (Correctly Identified Spam) / (Total Actual Spam Emails) = 5 / 12    The system has a recall of 41.7%, indicating it only identified 41.7% of the actual spam  emails. Choosing between Precision and Recall The importance of precision vs. recall depends on the specific application. For instance, in a  medical diagnosis system:    High recall might be crucial – catching as many positive cases (diseases) as possible,  even if it leads to some false positives (unnecessary tests).    On the other hand, a financial fraud detection system might prioritize high precision –  minimizing false positives (wrongly declined transactions) to avoid inconveniencing  customers. By understanding precision and recall, you can effectively evaluate your machine learning  models and determine which metric holds more weight for your specific task. Understanding the Problem Statement I strongly believe in learning by doing. So throughout this article, we’ll talk in practical terms – by  using a dataset. Let’s take up the popular Heart Disease Dataset available on the UCI repository. Here, we have  to predict whether the patient is suffering from a heart ailment using the given set of features.  You can download the clean dataset from this statement. Since this article solely focuses on model evaluation metrics, we will use the simplest classifier  – the kNN classification model to make predictions. "
            },
            {
                "page_number": 5,
                "text": "As always, we shall start by importing the necessary libraries and packages: Python Code: #You can also the change the code as per your needs, for now, just for the sake of simplicity the  rest of the code is commented out. import numpy as np import pandas as pd # from sklearn.model_selection import train_test_split # from sklearn.preprocessing import StandardScaler # from sklearn.neighbors import KNeighborsClassifier # from sklearn.metrics import confusion_matrix # from sklearn.metrics import classification_report # from sklearn.metrics import roc_curve # from sklearn.metrics import roc_auc_score # from sklearn.metrics import precision_recall_curve # from sklearn.metrics import auc # import matplotlib.pyplot as plt # import seaborn as sns data_file_path = 'heart.csv' data = pd.read_csv(data_file_path) #To get information of dataset and the datatypes of the features print(data.head()) print(data.dtypes) print(data.sex.value_counts()) #To run the entire code scroll down to the bottom of the blog or search the link given down  below "
            },
            {
                "page_number": 6,
                "text": "Let us check if we have missing values: data_df.isnull().sum() There are no missing values. Now we can take a look at how many patients are actually suffering  from heart disease (1) and how many are not (0): #2. distribution of target variable. sns.countplot(data_df['target']) # Add labels plt.title('Countplot of Target') plt.xlabel('target') plt.ylabel('Patients') plt.show() This is the count plot below: Let us proceed by splitting our training and test data and our input and target variables. Since we  are using KNN, it is mandatory to scale our datasets too. y = data_df['target'].values x = data_df.drop(['target'], axis = 1) #Scaling - mandatory for knn from sklearn.preprocessing import StandardScaler ss = StandardScaler() x = ss.fit_transform(x) #SPlitting into train and test "
            },
            {
                "page_number": 7,
                "text": "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3) # 70% training and 30% test The intuition behind choosing the best value of k is beyond the scope of this article, but we  should know that we can determine the optimum value of k when we get the highest test score  for that value. For that, we can evaluate the training and testing scores for up to 20 nearest  neighbors: train_score = [] test_score = [] k_vals = [] for k in range(1, 21): k_vals.append(k) knn = KNeighborsClassifier(n_neighbors = k) knn.fit(X_train, y_train) tr_score = knn.score(X_train, y_train) train_score.append(tr_score) te_score = knn.score(X_test, y_test) test_score.append(te_score) To evaluate the max test score and the k values associated with it, run the following command: ## score that comes from the testing set only max_test_score = max(test_score) test_scores_ind = [i for i, v in enumerate(test_score) if v == max_test_score] print('Max test score {} and k = {}'.format(max_test_score * 100, list(map(lambda x: x + 1,  test_scores_ind)))) Thus, we have obtained the optimum value of k to be 3, 11, or 20 with a score of 83.5. We will  finalize one of these values and fit the model accordingly: #Setup a knn classifier with k neighbors knn = KNeighborsClassifier(3) knn.fit(X_train, y_train) knn.score(X_test, y_test) "
            },
            {
                "page_number": 8,
                "text": "Now, how do we evaluate whether this model is a ‘good’ model or not? For that, we use  something called a Confusion Matrix:= y_pred = knn.predict(X_test) confusion_matrix(y_test,y_pred) pd.crosstab(y_test, y_pred, rownames = ['Actual'], colnames =['Predicted'], margins = True) The Role of the F1-Score Understanding Accuracy made us realize we need a tradeoff between Precision and Recall. We  first need to decide which is more important for our classification problem. For example, for our dataset, we can consider that achieving a high recall is more important  than getting a high precision – we would like to detect as many heart patients as possible. For  some other models, like classifying whether or not a bank customer is a loan defaulter, it is  desirable to have high precision since the bank wouldn’t want to lose customers who were  denied a loan based on the model’s prediction that they would be defaulters. There are also many situations where precision and recall are equally important. For example,  for our model, if the doctor informs us that the patients who were incorrectly classified as  suffering from heart disease are equally important since they could be indicative of some other  ailment, then we would aim for not only a high recall but a high precision as well. In such cases, we use something called F1-score. F1-score is the Harmonic mean of the  Precision and Recall: This is easier to work with since now, instead of balancing precision and recall, we can just aim  for a good F1-score, which would also indicate good Precision and a good Recall value. We can generate the above metrics for our dataset using sklearn too: print(classification_report(y_test, y_pred)) "
            },
            {
                "page_number": 9,
                "text": "False Positive Rate & True Negative Rate Along with the above terms, there are more values we can calculate from the confusion matrix:    False Positive Rate (FPR):  It is the ratio of the False Positives to the Actual number of Negatives. In the context of  our model, it is a measure of the number of cases where the model predicts that the  patient has a heart disease from all the patients who actually didn’t have the heart  disease. For our data, the FPR is = 0.195    True Negative Rate (TNR) or the Specificity:  It is the ratio of the True Negatives and the Actual Number of Negatives. For our model, it  is the measure of the number of cases where the model correctly predicts that the  patient does not have heart disease from all the patients who actually didn’t have heart  disease. The TNR for the above data = 0.804. From these 2 definitions, we can also  conclude that Specificity or TNR = 1 – FPR We can also visualize Precision and Recall using ROC curves and PRC curves. Receiver Operating Characteristic Curve (ROC Curve) It is the plot between the TPR(y-axis) and FPR(x-axis). Since our model classifies the patient as  having heart disease or not based on the probabilities generated for each class, we can decide  the threshold of the probabilities as well. For example, we want to set a threshold value of 0.4. This means that the model will classify the  data point/patient as having heart disease if the probability of the patient having a heart disease  is greater than 0.4. This will obviously give a high recall value and reduce the number of False  Positives. Similarly, using the ROC curve, we can visualize how our model performs for different  threshold values. Let us generate a ROC curve for our model with k = 3. y_pred_proba = knn.predict_proba(X_test)[:,1] fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba) "
            },
            {
                "page_number": 10,
                "text": "AUC Interpretation    At the lowest point, i.e., at (0, 0)- the threshold is set at 1.0. This means our model  classifies all patients as not having a heart disease.    At the highest point, i.e., at (1, 1), the threshold is set at 0.0. This means our model  classifies all patients as having a heart disease.    The rest of the curve is the values of FPR and TPR for the threshold values between 0 and  1. At some threshold values, we observe that for FPR close to 0, we are achieving a TPR  of close to 1. This is when the model will predict the patients having heart disease  almost perfectly.    The area with the curve and the axes as the boundaries is called the Area Under  Curve(AUC). It is this area that is considered as a metric of a good model. With this  metric ranging from 0 to 1, we should aim for a high value of AUC. Models with a high  AUC are called models with good skills. Let us compute the AUC score of our model  and the above plot: roc_auc_score(y_test, y_pred_proba)    We get a value of 0.868 as the AUC, which is a pretty good score! In simplest terms, this  means that the model can distinguish the patients with heart disease and those who "
            },
            {
                "page_number": 11,
                "text": "don’t 87% of the time. We can improve this score, and I urge you to try different  hyperparameter values.    The diagonal line is a random model with an AUC of 0.5, a model with no skill, which is  just the same as making a random prediction. Can you guess why? Precision-Recall Curve (PRC) As the name suggests, this curve directly represents the precision (y-axis) and the recall (x-axis).  If you observe our definitions and formulae for the Precision and Recall above, you will notice  that we are not using the True Negatives(the actual number of people who don’t have heart  disease). This is particularly useful for situations where we have an imbalanced dataset and the number  of negatives is much larger than the positives(or when the number of patients having no heart  disease is much larger than the patients having it). In such cases, our greater concern would be  detecting the patients with heart disease as correctly as possible and would not need the TNR. Like the ROC, we plot the precision and recall for different threshold values: precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba) plt.figure(figsize = (10,8)) plt.plot([0, 1], [0.5, 0.5],'k--') plt.plot(recall, precision, label = 'Knn') plt.xlabel('recall') plt.ylabel('precision') plt.title('Knn(n_neighbors = 8) PRC curve') plt.show() PRC Interpretation    At the lowest point, i.e., at (0, 0)- the threshold is set at 1.0. This means our model  makes no distinctions between the patients with heart disease and those without.    At the highest point, i.e., at (1, 1), the threshold is set at 0.0. This means that our  precision and recall are high, and the model makes distinctions perfectly.    The rest of the curve is the values of Precision and Recall for the threshold values  between 0 and 1. Our aim is to make the curve as close to (1, 1) as possible- meaning  good precision and recall.    Similar to ROC, the area with the curve and the axes as the boundaries is the Area Under  Curve(AUC). Consider this area as a metric of a good model. The AUC ranges from 0 to 1.  Therefore, we should aim for a high value of AUC. Let us compute the AUC for our model  and the above plot: "
            },
            {
                "page_number": 12,
                "text": "# calculate precision-recall AUC auc_prc = auc(recall, precision) print(auc_prc) As before, we get a good AUC of around 90%.  Also, the model can achieve high precision with a  recall of 0 and would achieve a high recall by compromising the precision of 50%. Conclusion To conclude, this tutorial showed how to evaluate a classification model, especially one that  focuses on precision and recall, and find a balance between them. We also explained how to  represent our model performance using different metrics and a confusion matrix. Hope you like the article. Precision and recall are crucial metrics in machine learning.  Understanding “precision vs recall” helps improve model performance. “What is precision and  recall?” Precision measures accuracy, while recall indicates completeness. “Precision recall  F1” combines both for a balanced evaluation. In “precision vs recall machine learning”  comparisons, optimizing both metrics is essential for robust predictive models. Here is an additional article for you to understand evaluation metrics- 11 Important Model  Evaluation Metrics for Machine Learning Everyone should know  Also, in case you want to start learning Machine Learning, here are some free resources for you-    Free Course – Introduction to AI and ML    Free Mobile App – Introduction to AI and ML Key Takeaways    Precision and recall are two evaluation metrics used to measure the performance of a  classifier in binary and multiclass classification problems.    Precision measures the accuracy of positive predictions, while recall measures the  completeness of positive predictions.    High precision and high recall are desirable, but there may be a trade-off between the  two metrics in some cases.    Precision and recall should be used together with other evaluation metrics, such as  accuracy and F1-score, to get a comprehensive understanding of the performance of a  classifier. "
            }
        ],
        "images": [
            "Image_53",
            "Image_56",
            "Image_60",
            "Image_65"
        ]
    },
    {
        "file_name": "rag architecture.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "A Comprehensive Guide to Different RAG Architectures Introduction In an era where data doubles every two years, sifting through mountains of information to find  exactly what you need can feel like searching for a needle in an ever-expanding haystack. Yet, this is the challenge that modern artificial intelligence (AI) systems, like Retrieval- Augmented Generation (RAG), are designed to tackle. Search engines might be among the first to face obsolescence from AI, with products like  SearchGPT and Perplexity leading the charge. These systems rely on RAG, and the tools to build  them are now within anyone's reach. But what sets apart a basic RAG system from an optimized one? The difference often lies in how  well it's tailored to the task. This guide will explore the spectrum of RAG architectures, diving deep into the techniques that  enhance their performance. Overview of RAG The acronym RAG originates from the 2020 paper, Retrieval-Augmented Generation for  Knowledge-Intensive Tasks, published by Facebook AI Research (now Meta AI). This paper characterizes RAG as “a general-purpose fine-tuning recipe” designed to integrate  any large language model (LLM) with various internal or external knowledge sources. RAG gives an LLM a superpower: the ability to consult an external knowledge base before  crafting its responses. LLM alone, no doubt, is super bright but faces several challenges:    Presenting False Information: When the LLM lacks the answer, it might generate  incorrect or misleading responses.    Providing Out-of-Date Information: LLMs can offer generic or outdated answers,  especially when users expect current and specific information.    Relying on Non-Authoritative Sources: The model may draw on sources that are not  credible or accurate.    Confusing Terminology: Different sources might use the same terms differently,  leading to misunderstandings or inaccurate responses. RAG helps address these issues by anchoring the LLM's responses in authoritative and current  knowledge, ensuring more reliable and relevant interactions. So, imagine your LLM as a brilliant but forgetful scholar—by tapping into up-to-date sources, it  can offer accurate, contextually relevant answers without needing a complete retrain. The core components of the RAG pipeline are:    Retrieval Pipeline: Retrieves relevant documents or data from an external knowledge  base. "
            },
            {
                "page_number": 2,
                "text": "   Generation Pipeline: Uses the retrieved information to generate contextually accurate  and relevant responses. Fig 1: A fundamental architecture of Retrieval-Augmented Generation (RAG) However, basic RAG has significant room for improvement, which will be explored in the next  section, and potential solutions. Types of RAG Architectures Before jumping straight to the types of architecture of RAG, let’s discuss the challenges that a  primary RAG faces:    Basic RAG often struggles with queries that span multiple domains or topics, leading to  mismatches in retrieved documents and reduced accuracy.    The reliance on vector similarity alone can cause irrelevant or suboptimal document  retrieval, especially for nuanced queries. "
            },
            {
                "page_number": 3,
                "text": "   Basic RAG systems may have higher latency due to inefficient retrieval and processing  steps.    The broad context windows in basic RAG can overwhelm the LLM with irrelevant  information, reducing the quality of the generated response.    The standard RAG approach may not effectively break down or handle multi-faceted  queries.    The lack of advanced filtering and indexing can cause higher processing costs,  especially when handling large datasets or frequent queries. The architectures presented ahead solves such problems. Let’s look at them. Query Enhancement Basic RAG does reflect the true self of the user query, and it might lead to incompetent results.  Query enhancement adjusts and refines the RAG input query process to capture and convey the  query's intent accurately. Let’s see some techniques: 1. Hypothetical Questions This approach uses an LLM to generate potential user questions about the content of each  document chunk. Before the user's query reaches the LLM, the vector store retrieves the most relevant  hypothetical questions related to the actual query and their corresponding document chunks  and forwards them to the LLM. This method addresses the cross-domain asymmetry (will be discussed later) issue in vector  search. It enables query-to-query matching, thereby reducing the reliance on vector searches. Workflow for Generating Hypothetical Questions 1. Generate Hypothetical Questions: Use the LLM to create a set of hypothetical questions based on the content of the document chunks. 2. Store in Vector Store: Save the generated hypothetical questions along with their corresponding document chunks in the vector store. 3. Retrieve Relevant Data: Query the vector store to find the most relevant hypothetical questions and document chunks based on the user query. 4. Generate Final Response: Send the retrieved hypothetical questions and document chunks to the LLM to generate the final response to the user query. 2. HyDE The acronym HyDE stands for Hypothetical Document Embeddings. Instead of contextual  information, this approach uses LLM to create “Hypothetical Documents” or simulated  responses to a user query. The answer is then translated into vector embeddings, which are utilized to find the most  suitable parts of a document in a vector database. "
            },
            {
                "page_number": 4,
                "text": "Fig 2: An illustration of the HyDE model. The vector database returns the most relevant K chunks and sends them back to LLM with user  query text to generate the final answer. However, this method increases computational costs  and uncertainties when generating hypothetical answers. Workflow for Generating Hypothetical Document 1. Generate Hypothetical Document: Create a simulated document or fake answer using the LLM based on the user query. 2. Convert to Embeddings: Transform the hypothetical document into vector embeddings. 3. Retrieve Relevant Chunks: Use the vector embeddings to search the vector database and retrieve the top-K most relevant document chunks. 4. Generate Final Response: To generate the final response, provide the retrieved document chunks and the original query to the LLM. "
            },
            {
                "page_number": 5,
                "text": "This improves the query process because vector databases, serving as knowledge hubs, match  documents more effectively than questions. 3. Sub-Queries When tackling a complex user query, an LLM can break it into bite-sized, manageable sub- queries. Imagine a user asking, “What are the key differences between AWS and Azure?” Instead of answering this all at once, the LLM might split it into more straightforward questions  like, “What features does AWS offer?” and “What features does Azure provide?” These sub-queries are then turned into vector embeddings and sent to the vector database. The  database retrieves the top-K most relevant document chunks for each subquery. With this tailored information, the LLM crafts a detailed and precise answer. Workflow for Sub-Queries 1. Break Down Query: The LLM decomposes the complex user query into simpler sub- queries. 2. Convert to Embeddings: Transform each subquery into vector embeddings. 3. Retrieve Relevant Chunks: Search the vector database for each sub-query's Top-K most relevant document chunks. 4. Generate Final Response: To generate the final response, provide the retrieved document chunks and the original query to the LLM. Indexing Enhancement Indexing enhancement involves optimizing how data is organized and retrieved to boost the  performance of RAG. Here are some top techniques that can improve it: 1. Merging Document Chunks Automatically This technique involves organizing document data into two levels: child chunks (detailed  segments) and their corresponding parent chunks (broader summaries). Initially, the system searches for detailed child chunks. When a sufficient number of child  chunks from a set belong to the same parent chunk, the parent chunk is used to provide  contextual information to the LLM. Imagine your document library as a well-organized bookshelf with detailed notes and  overarching summaries. To make searching more efficient, locate detailed notes (child chunks)  and merge related notes into a broader summary (parent chunks). This way, when you search, you find specific notes and get the broader context from associated  summaries. Workflow for Merging Document Chunks Automatically 1. Index Chunks: Index child chunks (detailed segments) and parent chunks (broader summaries). 2. Retrieve Child Chunks: Search and retrieve relevant child chunks based on the user query. "
            },
            {
                "page_number": 6,
                "text": "3. Merge Chunks: Consolidate child chunks into their corresponding parent chunks if a specified threshold is met. 4. Generate Final Response: Provide the merged parent chunks as contextual information to the LLM to generate the final response. 2. Constructing Hierarchical Indices This method involves creating a two-tier indexing system: one for document summaries and  another for individual document chunks. The retrieval process consists of two stages: filtering relevant documents based on their  summaries and then retrieving the document chunks within those selected documents. Workflow for Constructing Hierarchical Indices 1. Create Indices: Establish two-level indices: one for document summaries and another for document chunks. 2. Retrieve Relevant Documents: Search for and obtain relevant documents based on the summaries. 3. Retrieve Document Chunks: Extract the document chunks from the relevant documents. 4. Generate Final Response: Use the retrieved document chunks to generate the final response with the LLM. 3. Hybrid Retrieval and Reranking The Hybrid Retrieval and Reranking technique combines multiple retrieval methods to enhance  search performance. Initially, vector similarity retrieval is used alongside supplementary retrieval methods, such as  linguistic frequency-based approaches or sparse embedding models. After obtaining initial results, a reranking process prioritizes them based on their relevance to  the user query. "
            },
            {
                "page_number": 7,
                "text": "Fig 3: A basic hybrid retrieval and reranking approach Workflow for Hybrid Retrieval and Reranking 1. Perform Initial Retrieval: 1. Obtain initial results using vector similarity methods (e.g., using embeddings to find similar documents). 2. Obtain initial results using traditional methods like BM25 or frequency-based approaches. 2. Combine Results: Merge the results from vector similarity and lexical retrieval methods into a unified set. 3. Rerank Results: Apply a reranking algorithm (e.g., Reciprocal Rank Fusion (RRF) or Cross-Encoder) to reorder the combined results based on their relevance to the user  query. "
            },
            {
                "page_number": 8,
                "text": "4. Generate Final Response: Use the LLM to generate the final response based on the reranked results and the original query. Retriever and Generator Enhancements Regarding RAG's two main components, the Retriever and Generator, if your RAG isn't  performing well despite query and index improvements, consider optimizing these components. Here are some architectural options for their enhancement; let’s start with retriever  enhancement: 1. Retriever: Sentence Window Retrieval In a traditional RAG system, the document chunk sent to the LLM is like a giant, panoramic view  around a specific piece of information. This broader window ensures the LLM gets a comprehensive context but can sometimes  overwhelm it with excess detail. The Sentence Window Retrieval technique introduces a more focused approach: it treats the  document chunk used for embedding retrieval and the chunk provided to the LLM as distinct  entities. Think of it like having a detailed map (the embedding chunk) and a zoomed-in view (the context  window) of the area you're interested in. Workflow for Sentence Window Retrieval 1. Retrieve Embedding Chunk: Extract a chunk of the document based on embedding similarity. 2. Determine Context Window: Define a context window size for additional information around the retrieved chunk. 3. Retrieve Expanded Window: Obtain the extended context window from the document. 4. Provide Context to LLM: Pass the extended context window and the retrieved chunk to the LLM for processing. 5. Adjust Window Size: Fine-tune the window size based on specific business needs to balance context and relevance. This method is advantageous when you need to balance the richness of information with the  clarity of focus, ensuring that the LLM is guided by relevant context without being overwhelmed. 2. Retriever: Metadata Filtering Meta-data Filtering involves refining the retrieved documents by applying filters based on  metadata attributes, such as time, category, or other relevant criteria. Workflow for Metadata Filtering 1. Retrieve Initial Documents: Obtain documents based on the initial retrieval process. 2. Apply Metadata Filters: Filter the retrieved documents using metadata attributes (e.g., date range, category). So, when inserting them, add these metadata to the filter. "
            },
            {
                "page_number": 9,
                "text": "3. Refine Document Set: Narrow down the document set to match the specific criteria of the query. 4. Provide Filtered Documents to LLM: Pass the filtered documents to the LLM to generate a response. By filtering metadata, not only can retrieval be enhanced, but costs can also be reduced by  significantly decreasing the number of documents that need to be processed by the LLM. The following section will focus on techniques to manage and minimize costs effectively. 3. Generator: Compressing Retrieved document chunks often contain noise and irrelevant details that can hinder the  accuracy of the LLM's responses. Additionally, the limited prompt window of LLMs constrains the amount of context that can be  processed. To address this, compressing the prompt involves filtering out irrelevant information,  highlighting key paragraphs, and reducing the overall length of the context provided to the LLM. Workflow for Compressing    Identify Key Information: Analyze retrieved document chunks to identify and extract  essential details and critical paragraphs.    Filter Out Noise: Remove or compress irrelevant or redundant information that does not  contribute to the final answer.    Adjust Context Length: Trim the overall length of the context to fit within the LLM's  prompt window constraints, ensuring only the most pertinent information is included.    Format and Provide Prompt: Prepare the compressed context as the LLM prompt,  focusing on clarity and relevance.    Generate Response: Use the refined prompt to generate a more accurate and focused  response from the LLM. 4. Generator: Adjusting Chunk Order LLMs tend to give more weight to information at the beginning and end of documents, often  neglecting content in the middle. To address this, we can strategically rearrange the order of retrieved document chunks. By placing chunks with higher confidence or relevance at the start and end of the prompt and  positioning those with lower confidence in the middle, we improve the LLM's focus on crucial  information. "
            },
            {
                "page_number": 10,
                "text": "Fig 4: The Significance of Document Placement: Insights from the Research Paper 'Lost in the  Middle' Workflow for Adjusting Chunk Order: 1. Retrieve Document Chunks: Obtain multiple document chunks based on the query. 2. Assess Chunk Confidence: Evaluate the confidence level or relevance of each chunk. 3. Reorder Chunks: Arrange the chunks so those with higher confidence are positioned at the beginning and end of the prompt, with lower-confidence chunks in the middle. 4. Prepare the Prompt: Assemble the reordered chunks into the LLM prompt, ensuring a logical and practical presentation of information. 5. Generate Response: Use the reordered prompt to generate a response, aiming for improved accuracy and relevance based on the strategically adjusted chunk order. "
            },
            {
                "page_number": 11,
                "text": "Now that we have explored the various architectures of RAG, it's essential to consider the best  practices for selecting the most suitable RAG for your specific requirements. If your generator pipeline is not performing up to the mark, it seems like this article is for you. Best Practices for Choosing the Right RAG Architecture The architecture of RAG can be tailored based on the query type. The following table provides an overview of how the query type influences the choice of RAG  components and the rationale behind each selection: Query Type  Recommended RAG  Technique  Why This Technique Fits Broad, Cross-Domain  Query Hypothetical  Questions Effective in bridging gaps between diverse  domains by generating relevant hypothetical  questions that span across different topics. Complex, Multi-Faceted  Query HyDE (Hypothetical  Document  Embeddings) Simulated documents provide a  comprehensive view, enhancing accuracy  when dealing with complex comparisons or  analyses. Detail-Oriented Query  Needing Specific Context Sentence Window  Retrieval Offers precise context control, ensuring that  relevant details are highlighted without  overwhelming the LLM with too much  information. Large Dataset Query  Hierarchical Indexing Efficiently filters through large datasets by  first narrowing down based on summaries,  making it ideal for extensive searches. Query Needing Top  Results from Multiple  Angles Hybrid Retrieval and  Reranking Combines different retrieval methods to bring  in diverse perspectives, then reranks for the  most relevant results. Time-Specific or  Categorical Query  Metadata Filtering Narrowing down results based on metadata  makes focusing on specific time periods,  categories, or other filters easier. Query Requiring  Summarization Compressing the LLM  Prompt Reduces noise and emphasizes critical  points, making it ideal for summarizing large  amounts of data into concise answers. Complex Query with  Important Points at the  Beginning and End Adjusting Chunk  Order Strategically reorders content to ensure the  LLM focuses on the most critical information,  particularly useful when the middle content is  less relevant. Further decisions on RAG may also depend on cost and accuracy. Enhancements like hybrid  retrieval and reranking improve precision but may increase the computational load. "
            },
            {
                "page_number": 12,
                "text": "Similarly, compressing the LLM prompt reduces context size and computational cost while  reducing the accuracy. These practices serve as a reference but ultimately revolve around the specific use case and the  problem. It’s always nice to strike a balance between the metrics that you care about: accuracy, cost, and  performance. Common Problems Solved by Enhanced RAG Architectures The advanced RAG techniques not only improve retrieval accuracy, but these architectures also  solve common challenges, including: 1. Addressing Cross-Domain Asymmetry: Cross-domain asymmetry refers to the challenge that arises when there is a mismatch between the domains or contexts of the  query and the information stored in the database or knowledge base. Techniques like  Hypothetical Questions and HyDE (Hypothetical Document Embeddings) tackle this by  generating simulated queries or documents that align more closely with the content,  reducing the reliance on direct vector matching. 2. Reducing Latency: Latency can be a massive setback for a RAG app user. It can get worse in scenarios where retrieval is from massive datasets. Techniques, such as  Metadata filtering and Sentence Window Retrieval, can significantly reduce the number  of documents relevant to the LLM context. 3. Improving Accuracy: Engineers have more control over the retrieval component when dealing with RAG. The better the retrieval from the databases, the better the generation.  Techniques, such as Hybrid Retrieval and Reranking, optimize retrieval by combining  multiple methods and reranking results based on relevance. 4. Handling Complex Queries: Complex queries and large datasets pose challenges in managing context and accuracy. Sentence Window Retrieval improves focus by  providing a more targeted context around relevant chunks. Hierarchical Indexing  organizes data into manageable layers, enhancing retrieval efficiency. 5. Improving Cost: Metadata Filtering reduces the number of documents processed by applying metadata-based filters, lowering computational costs. Adjusting Chunk Order  and Compressing the LLM Prompt also contribute to cost savings by ensuring that only  the most relevant information is processed, reducing the computational resources  required. Conclusion The importance of Retrieval-Augmented Generation (RAG) cannot be overstated in modern  information retrieval tasks. With tools available, anyone can create a RAG system. Despite its potential, a basic RAG system is frequently hindered by several challenges. These  include inaccurate query matching, handling large datasets, and managing computational  costs. This article explores architectures for RAG that offer improved performance and lower costs.  HyDE, Metadata Filtering, and Sentence Window Retrieval address cross-domain asymmetry  and reduce computational costs. "
            },
            {
                "page_number": 13,
                "text": "Hybrid Retrieval improves accuracy, and Hierarchical Indexing handles complex queries in large  datasets. These advanced RAGs are impressive but are always restricted to the use case of the  problem you are solving. Collaborating with users on such problems is suggested. This is where Athina AI comes in. Athina AI is a platform for AI teams to prototype, experiment, and monitor LLM-powered  applications. Your product becomes fully performant when you act on the user's feedback,  which requires the observability that Athina provides. "
            }
        ],
        "images": [
            "Image_33",
            "Image_40",
            "Image_47",
            "Image_56"
        ]
    },
    {
        "file_name": "rag_primer.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Retrieval Augmented Generation - A Primer Retrieval augmented generation, or RAG, is rapidly gaining traction as the next disruptive  evolution for generative AI models. Promising a reduction in the hallucinations sometimes  experienced by large language models such as GPT-4, greater accuracy of responses, access to  up-to-date information, and the capability to use private or proprietary data without exposing  those same data to actors outside of one’s organisation, RAG models represent a major  increase in capability for over purely generative, pre-trained models such as GPT and LLaMa.  With these potential benefits in mind, and with major industry players like Oracle and MongoDB  bringing their own RAG-enabled solutions to market, now is a great time to get a top-level  understanding of what RAG actually is. In this post, we’re going to walk you through the basics of  RAG and the vector database principles that underpin it, covering why it’s useful, how it works,  and what limitations it may experience. Pure Generative Versus RAG Drawbacks of Current Generative Models Existing generative AI models such as GPT-4 and LLaMa have captured a lot of interest, both  publicly and within the AI community, for their ability to ingest vast quantities of data, learn the  syntactic rules of communication, and then capably interpret input queries to return a  semantically or stylistically accurate output. While powerful for summarising large corpuses of  data in a few paragraphs, or finding a specific piece of information within a sea of data, these  models soon hit a few problems on close inspection. Principal among these problems is the tendency for ‘hallucinations’, when the pre-trained  model doesn’t know the answer to an input query and, instead of responding that it doesn’t  know, fabricates a convincing-sounding but inaccurate answer. It is primarily this behaviour that  led to swift action from Stack Overflow to ban AI-generated answers to user questions, with  similar moves from other institutions regarding the usage and trust of Chat-GPT for commercial  or official purposes. Secondly, a pre-trained model only knows about the data on which it was trained. In other  words, it rapidly becomes stale when trying to train it to understand current events. The freely  available instance of Chat-GPT, for example, uses GPT-3.5, trained on data up to January 2022,  and therefore it is unable to answer queries about any events beyond that date. This is obviously  problematic when seeking answers about fields which are in ongoing development or see fast- moving change, such as AI, features of programming languages and libraries, legislation and so  on. While models can be re-trained, the time taken and compute power required to do this make  it prohibitive to perform frequently, and it is only ever a temporary fix. Finally, pre-trained models do not have access to your organisation's private or proprietary data.  This is obviously a good thing for the most part; no one's private information should be publicly  exposed via a generative AI model. However, this does mean that the power of a generative  model cannot be utilised on private or commercially sensitive information, or that a locked- down instance of the model must be trained on those data, at the cost of time, compute power,  and maintenance. We can really summarise these problems as the issue of hallucinations and two major causes  thereof, which combine to become a major limitation of purely generative models. It is in  addressing this limitation that RAG demonstrates its value. "
            },
            {
                "page_number": 2,
                "text": "Retrieval Augmentation In an impressive feat of appropriate nomenclature, retrieval augmented generation models  make use of an information retrieval architecture to augment the generation process. In simple  terms, it’s a generative model which uses a search engine, pointed towards a vector database of  known-good data, to provide the information and context to answer the query being posed,  rather than relying on the information on which it was trained. The consequences of this simple  addition are far reaching; by backing the generative model using an arbitrary and private  database of information, we can leverage the semantic understanding given by a large language  model (LLM) to both interpret the query and construct an answer, while ensuring that we always  have the required information available in a non-stale, non-public repository. The basic process is as follows: 1. The user inputs their prompt to the model 2. The LLM interprets the query (embedding) 3. The retrieval architecture searches the database and returns the relevant information (vector search) 4. The LLM uses that information to generate an answer Basic RAG data flow. The technical terms included here are explained later in this article In principle, RAG is similar to fine-tuning a LLM, but much more powerful. As a reminder, fine- tuning is the insertion of context into the prompts or queries passed to a model; in this way,  we’re notionally retraining or providing additional training on top of the base model to include  more topics or more private or sensitive data, but this is limited to how much information we  can squeeze into the prompt and will be sensitive to the exact wording used. RAG, on the other "
            },
            {
                "page_number": 3,
                "text": "hand, includes context from your database at the time of inference, and by constructing the  prompt well we can even specify that only the context in the database should be used to  generate an answer. So how does this address the hallucination, and causes thereof, that we observed with purely  generative models? With RAG, we can ensure that sensitive and up-to-date information is always available to our  model by simply inserting new data into our vector database. We can then tell the model via our  input prompt to use only the information in the database to answer our query, with a strict  requirement to say ‘I don’t know' if the answer is not explicitly included within that context. This  mitigates against hallucinations and allows our model to access new data without costly  retraining and without the actual model learning that information. Private data are thereby  not made publicly visible via the model, and we improve security and compartmentalisation  between users; a user’s credentials and the database management system control which data  are visible and queryable by the user, and the model generates a response based on what they  are permitted to access. RAG Pre-processing and Architecture RAG clearly has a lot to offer as an evolution of generative AI, but doing so requires a few extra  pieces of pre-processing and architecture to support the vector database and the data held  within. Let’s take a look at what they are and how they work. Data Chunking Constructing a vector database for RAG starts, as many things do, with a large volume of  unstructured data. This may be a corpus of very many documents, a library of millions of  pictures, thousands of hours of video or something else entirely depending on the application.  Without loss of generality, we’ll focus on the textual data case, but the principles apply equally  to other forms of unstructured data. A strategy is needed for storing our huge volume of raw data in a way which allows it to be  searched for pieces of relevant information, and that strategy is called chunking. In very literal  terms, the data are divided up into chunks prior to storage, so that each one can be inspected  for relevance to the input query during a search. It is best practice to include some overlap in  these chunks, to avoid information being split between chunk boundaries and thereby lost  during the chunking process. The size and format of these chunks can be very different from application to application, and  it’s always a balancing act of having small enough chunks that they can be quickly compared to  one another, versus large enough chunks that each one contains meaningful information. It’s  also important to consider the total number of chunks; create too many, and execution of the  model will slow down due to the number of comparisons required when performing a search. For structured text formats like HTML and JSON, we may be able to take advantage of the  inherent formatting hints in the document and chunk on paragraph or heading tags. Longer form  prose may simply take a sentence or number of sentences per chunk, and small documents  may be a chunk in and of themselves. Optimising the chunk size for the application and platform  in question is an important step to achieve the required accuracy and speed. Vector Embedding "
            },
            {
                "page_number": 4,
                "text": "To be able to provide answers in a useful timeframe, RAG must solve the problem of how to  rapidly search a database of information on which it was not trained, and return relevant pieces  of information from which to construct an answer. Searches of this kind on any kind of  unstructured data are non-trivial, so we first must first map those unstructured data to  structured numerical vector, in a process known as vector embedding. Vector embedding represents an arbitrary piece of unstructured data as an n-dimensional array  of floats. These numbers may not be inherently meaningful or interpretable, but they provide a  way of comparing two pieces of unstructured data by mapping them to a point in n-dimensional  space. Similar pieces of data will sit close to one another in the vector space, and dissimilar  pieces of data will be further away. Much like deciding on a chunk size, choosing a value for n must be conducted on a case-by- case basis. If n is too low then we will not have enough degrees of freedom to represent the  differences in our data. Too high, and we will erode performance by increasing the complexity of  the embedding and search processes. For text applications, an LLM will typically be used to perform this embedding, and the same  LLM will be used to embed the query at runtime, thereby ensuring that we compare like-for-like  vector representations when performing a search. Distance Metrics There are many ways of measuring the distance between vectors, and the choice of distance  metric depends on the application. The typical choices are:    Euclidean distance - straight line distance between points    Cosine distance - the angle between two points    Dot product - a projection of one vector onto another, which combines the magnitude  and angle between two vectors (effectively a combination of Euclidean and Cosine  distances)    Manhattan distance - Distance when movement direction is constrained to be parallel to  one axis at a time (i.e. no diagonal lines, as if following roads in the Manhattan grid  system) Euclidean distance is best for applications in which exact matching is important, like pixel  values when comparing two images, while cosine distance is better for sentiment analysis,  where very different combinations of letters and words can encode similar sentiments. Deciding  on a distance metric requires an understanding of the data domain as well as the underlying  problem, and since different metrics can yield very different answers to whether a pair of points  are close or distant, this choice can make a lot of difference to the model's performance. "
            },
            {
                "page_number": 5,
                "text": "Examples of how two different distance metrics can yield similar or different results for the  same pair of points. Vector Search Vector search methods are ways of finding the closest match in a vector database to some input  vector, e.g. the vector embedded query passed to our RAG model. This is a reasonably  commonplace operation to perform with vector data, and standard k-nearest-neighbours (KNN)  methods may be applied to find the relevant pieces of data. However, in application the retrieval  architecture in a RAG will need to be able to return a result in a constrained timeframe of a few  seconds, or maybe a few minutes for particularly patient users or complicated queries. If the  vector database is large, perhaps holding millions of embedded data chunks, then finding the  KNN can become prohibitively time-consuming. Fortunately, for large databases, it is sufficient to return the approximate nearest  neighbours (ANN) instead of the literal closest matches. While imperfect, enough context and  information can generally be gleaned from this approach to answer the input query, and  performance time at scale is much better. A major ANN method in RAG is the hierarchical navigable small worlds method (HNSW), which  arranges vectors as nodes on layers of connected graphs. Higher layers contain smaller subsets  of the data set to be searched, and if a node is present on layer m, then it will be present on all  layers lower than m. Searching begins on the top layer, with the search moving from node-to- node to minimise a distance metric between the node and the input query vector. If a closer  match cannot be found on the current layer, it drops to the layer below and keeps going, until  the closest match on the bottom level is reached. "
            },
            {
                "page_number": 6,
                "text": "Visualisation of the hierarchical navigable small worlds (HNSW) method of finding an  approximate nearest neighbour (ANN) to an input vector A full treatment of HNSW is worthy of its own post, and there are other approximate nearest  neighbour search methods which can be applicable. Ultimately, the vector search step needs  only to return a large enough number of sufficiently close matching results for the LLM, i.e. the  generative model, to construct a meaningful and accurate answer. Vector Databases Vector databases are databases designed and optimised to handle vector data, as opposed to  the tabular data stored by traditional relational databases. They provide efficient storage,  indexing and querying mechanisms, optimised for high-dimensional and variable-length  vectors, and thereby allow for flexible data storage and retrieval. Many vector databases are  designed to scale horizontally, allowing for efficient distributed storage and querying across  multiple nodes or clusters, and use techniques such as k-d trees, ball tres or locality-sensitive  hashing to enable fast and efficient search operations. Of particular use is the ability to associate metadata with each vector. For RAG applications,  this allows a linkage between the raw, human-readable tokens of a text corpus to the abstract  vector representation of a data chunk. This is of great utility in helping to debug and optimise a  model, by letting the user interpret which results were found to be similar to an input vector and  where the information for the returned query came from. At the heart of RAG, vector databases can be thought of as a long term memory on top of a LLM,  avoiding the need to retrain a model or rerun a dataset through a model before making a query. "
            },
            {
                "page_number": 7,
                "text": "LLM Interface Framework With the vector database in place and populated, it’s necessary to have some way of interfacing  it with a LLM, so that queries can be passed in and context can be returned. Two such  interfacing frameworks currently available for use are LangChain and LlamaIndex, and either  can be used to create a RAG model from a vector database and a LLM. Both are extensively  documented by their developers and widely used in the AI community. Platforms Since RAG models lean heavily on the use of a vector database to enable their retrieval stage,  platforms which support vector databases and enable vector search methods are required for  deployment. Existing solutions are available in the form of MongoDB’s Atlas with Atlas Vector Search  and Pinecone’s Pinecone Vector Database with Vector Search, and open-source options such  as Weaviate. Others are close to market, such as Oracle’s AI Vector Search, which will be  included in a future update to Oracle Database 23c. Benefits and Limitations of RAG RAG offers a lot of potential improvement over purely generative models, at the cost of  additional model complexity, up-front work and infrastructure. To help clarify this, we’ve  summarised the main benefits and limitations of RAG models below. Benefits of RAG 1. Reduced Hallucinations and Staleness The vector database for retrieval can be kept up-to-date and made more complete than the  stale training set of a pure generative model, leading to fewer hallucinations and decreasing the  need to re-train the model. 2. Contextual Consistency By retrieving data from a curated and maintained source database only, RAG helps to ensure  that the output is contextually relevant and consistent with the input. The generative model will  be focused on the data which answer the question, and not on the potentially irrelevant data on  which it was trained. 3. Improved Accuracy Similarly to the above, by grabbing data from a managed database of reliable information, the  accuracy of responses is increased versus a pre-trained model alone. 4. Controllability Using an appropriate retrieval database to provide ground truth to the generative model, and  excluding any other sources, allows developers to control the scope of generated responses to  the set of reliable, relevant and safe data. 5. Privacy Rather than training an LLM on private data, it is instead possible to train an LLM generically and  then point it towards private or proprietary data in a vector database as the context for a query. "
            },
            {
                "page_number": 8,
                "text": "The data source acts as the LLM’s memory, and the model will not learn personally identifying or  proprietary information. 6. Reduced Generation Bias Retrieved context mitigates some of the generation biases observed in pure generative models,  since biases from the training set will be superseded by the context from the vector database.  While this does not prevent replication of biases which are extant in the vector database, it gives  developers control over sources of bias, such that they can be corrected. Limitations of RAG 1. Complexity Joining two diverse models, a retrieval model and a generative model, is naturally going to  increase the complexity over using a LLM alone. This means that more compute power is  required and debugging is more complicated. 2. Data Preparation As with any ML model, the RAG model requires clean, non-redundant and accurate data on  which to base its answers. It then requires an effective chunking strategy to achieve the required  efficiency and accuracy, and appropriate embedding to represent the data. 3. Prompt Engineering Despite the capacity to search for relevant information, questions still need to be asked in the  right way to yield a good answer. This can include a need to specify that only information in the  retrieved context should be used to answer, and an effective wording of the search criteria. 4. Performance The two-step process of searching followed by generation increases the real-time latency  experienced from asking a question to receiving an answer, compared to using a generative  model alone. This leads to a necessary trade-off between depth-of-search in the retrieval stage  and overall response time. Summary That was a quick tour of RAG, covering the core concepts, the underlying technologies, and  some of the options available for deploying a RAG model today. At its heart, RAG is a way of  focusing a pre-trained generative model on relevant or private data without the risk of private  data leakage, and with robustness against hallucinations, at the cost of a little complexity and  infrastructure. As with any AI or ML architecture, it’s not a one-size-fits-all solution, nor is it a  cure-all for the limitations of generative AI (for instance, the ethical problems of generative AI  (e.g. a base LLM) being trained on copyrighted works), but it represents a major step forwards  over purely generative models, and can be utilised for powerful time savings and efficiency  gains. "
            }
        ],
        "images": [
            "Image_21",
            "Image_33",
            "Image_36"
        ]
    },
    {
        "file_name": "rag_what_is_it.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Retrieval Augmented Generation (RAG) What is retrieval augmented generation (RAG)? Retrieval augmented generation (RAG) is an architecture that provides the most relevant and  contextually-important proprietary, private or dynamic data to your Generative AI application's  large language model (LLM) when it is performing tasks to enhance its accuracy and  performance. What is RAG in AI / LLM? Retrieval Augmented Generation (RAG) in AI is a technique that leverages a database to fetch  the most contextually relevant results that match the user's query at generation time. Products built on top of Large Language Models (LLMs) such as OpenAI's ChatGPT and  Anthropic's Claude are brilliant yet flawed. Powerful as they are, current LLMs suffer from  several drawbacks: 1. They are static - LLMs are “frozen in time” and lack up-to-date information. It is not feasible to update their gigantic training datasets. 2. They lack domain-specific knowledge - LLMs are trained for generalized tasks, meaning they do not know your company’s private data. 3. They function as “black boxes” - it’s not easy to understand which sources an LLM was considering when they arrived at their conclusions. 4. They are inefficient and costly to produce - Few organizations have the financial and human resources to produce and deploy foundation models. Unfortunately, these issues affect the accuracy of GenAI applications leveraging LLMs. For any  business application with requirements more demanding than your average chatbot demo,  LLMs used “out of the box” with no modifications aside from prompting will perform poorly at  context-dependent tasks, such as helping customers book their next flight. This post will examine why Retrieval Augmented Generation is the preferred method for  addressing these drawbacks, provide a deep dive into how RAG works, and explain why most  companies use RAG to boost their GenAI applications’ performance. LLMs are “stuck” at a particular time, but RAG can bring them into the present. ChatGPT’s training data “cutoff point” was September 2021. If you ask ChatGPT about  something that occurred last month, it will not only fail to answer your question factually; it will  likely dream up a very convincing-sounding outright lie. We commonly refer to this behavior as  “hallucination.” "
            },
            {
                "page_number": 2,
                "text": "The LLM lacks domain-specific information about the Volvo XC60 in the above example.  Although the LLM has no idea how to turn off reverse braking for that car model, it performs its  generative task to the best of its ability anyway, producing an answer that sounds grammatically  solid - but is unfortunately flatly incorrect. The reason LLMs like ChatGPT feel so bright is that they've seen an immense amount of human  creative output - entire companies’ worth of open source code, libraries worth of books,  lifetimes of conversations, scientific datasets, etc., but, critically, this core training data is  static. After OpenAI finished training the foundation model (FM) behind ChatGPT in 2021, it received no  additional updated information about the world; it remains oblivious to significant world events,  weather systems, new laws, the outcomes of sporting events, and, as demonstrated in the  image above, the operating procedures for the latest automobiles. RAG provides up-to-date information about the world and domain-specific data to your  GenAI applications. Retrieval Augmented Generation means fetching up-to-date or context-specific data from an  external database and making it available to an LLM when asking it to generate a response,  solving this problem. You can store proprietary business data or information about the world  and have your application fetch it for the LLM at generation time, reducing the likelihood of  hallucinations. The result is a noticeable boost in the performance and accuracy of your GenAI  application. "
            },
            {
                "page_number": 3,
                "text": "LLMs lack context from private data - leading to hallucinations when asked domain or  company-specific questions. The second major drawback of current LLMs is that, although their base corpus of knowledge is  impressive, they do not know the specifics of your business, your requirements, your customer  base, or the context your application is running in - such as your e-commerce store. RAG addresses this second issue by providing extra context and factual information to your  GenAI application’s LLM at generation time: anything from customer records to paragraphs of  dialogue in a play, to product specifications and current stock, to audio such as voice or songs.  The LLM uses this provided content to generate an informed answer. Retrieval Augmented Generation allows GenAI to cite its sources and improves  auditability. In addition to addressing the recency and domain-specific data issues, RAG also allows GenAI  applications to provide their sources, much like research papers will provide citations for where  they obtained an essential piece of data used in their findings. Imagine a GenAI application serving the legal industry by helping lawyers prepare arguments.  The GenAI application will ultimately present its recommendations as final outputs. Still, RAG  enables it to provide citations of the legal precedents, local laws, and the evidence it used when  arriving at its proposals. RAG makes the inner workings of GenAI applications easier to audit and understand. It allows  end users to jump straight into the same source documents the LLM used when creating its  answers. Why is RAG the preferred approach from a cost-efficacy perspective? There are three main options for improving the performance of your GenAI application other  than RAG. Let’s examine them to understand why RAG remains the main path most companies  take today. Create your own foundation model OpenAI’s Sam Altman estimated it cost around $100 million to train the foundation model  behind ChatGPT. Not every company or model will require such a significant investment, but ChatGPT’s price tag  underscores that cost is a real challenge in producing sophisticated models with today’s  techniques. In addition to raw compute costs, you’ll also face the scarce talent issue: you need specialized  teams of machine learning PhDs, top-notch systems engineers, and highly skilled operations  folks to tackle the many technical challenges of producing such a model and every other AI  company in the world is angling for the same rare talent. Another challenge is obtaining, sanitizing, and labeling the datasets required to produce a  capable foundation model. For example, suppose you’re a legal discovery company considering  training your model to answer questions about legal documents. In that case, you’ll also need  legal experts to spend many hours labeling training data. "
            },
            {
                "page_number": 4,
                "text": "Even if you have access to sufficient capital, can assemble the right team, obtain and label  adequate datasets and overcome the many technical hurdles to hosting your model in  production, there’s no guarantee of success. The industry has seen several ambitious AI  startups come and go, and we expect to see more failures. Fine-tuning: adapting a foundation model to your domain’s data. Fine-tuning is the process of retraining a foundation model on new data. It can certainly be  cheaper than building a foundation model from scratch. Still, this approach suffers from many  of the same downsides of creating a foundation model: you need rare and deep expertise and  sufficient data, and the costs and technical complexity of hosting your model in production  don’t go away. Fine-tuning is not a practical approach now that LLMs are pairable with vector databases for  context retrieval. Some LLM providers, such as OpenAI, no longer support fine-tuning for their  latest-generation models. Fine-tuning is an outdated method of improving LLM outputs. It required recurring, costly, and  time-intensive labeling work by subject-matter experts and constant monitoring for quality drift,  undesirable deviations in the accuracy of a model due to a lack of regular updates, or changes  in data distribution. If your data changes over time, even a fine-tuned model’s accuracy can drop, requiring more  costly and time-intensive data labeling, constant quality monitoring, and repeated fine-tuning. Imagine updating your model every time you sell a car so your GenAI application has the most  recent inventory data. Prompt engineering is insufficient for reducing hallucinations. Prompt engineering means testing and tweaking the instructions you provide your model to  attempt to coax it to do what you want. It’s also the cheapest option to improve the accuracy of your GenAI application because you  can quickly update the instructions provided to your GenAI application’s LLM with a few code  changes. It refines the responses your LLMs return but cannot provide them with any new or dynamic  context, so your GenAI application will still lack up-to-date context and be susceptible to  hallucination. Let’s now take a deeper dive into how Retrieval Augmented Generation works. We’ve discussed how RAG passes additional relevant content from your domain-specific  database to an LLM at generation time, alongside the original prompt or question, through a  “context window'. An LLM’s context window is its field of vision at a given moment. RAG is like holding up a cue  card containing the critical points for your LLM to see, helping it produce more accurate  responses incorporating essential data. To understand RAG, we must first understand semantic search, which attempts to find the true  meaning of the user’s query and retrieve relevant information instead of simply matching "
            },
            {
                "page_number": 5,
                "text": "keywords in the user’s query. Semantic search aims to deliver results that better fit the user’s  intent, not just their exact words. Creating a vector database from your domain-specific proprietary data using an embedding  model. This diagram shows how you make a vector database from your domain-specific, proprietary  data. To create your vector database, you convert your data into vectors by running it through an  embedding model. An embedding model is a type of LLM that converts data into vectors: arrays, or groups, of  numbers. In the above example, we’re converting user manuals containing the ground truth for  operating the latest Volvo vehicle, but your data could be text, images, video, or audio. The most important thing to understand is that a vector represents the meaning of the input text,  the same way another human would understand the essence if you spoke the text aloud. We  convert our data to vectors so that computers can search for semantically similar items based  on the numerical representation of the stored data. Next, you put the vectors into a vector database, like Pinecone. Pinecone’s vector database can  search billions of items for similar matches in under a second. Remember that you can create vectors, ingest the vectors into the database, and update the  index in real-time, solving the recency problem for the LLMs in your GenAI applications. For example, you can write code that automatically creates vectors for your latest product  offering and then upserts them in your index each time you launch a new product. Your  company’s support chatbot application can then use RAG to retrieve up-to-date information  about product availability and data about the current customer it’s chatting with. Vector databases allow you to query data using natural language, which is ideal for chat  interfaces. Now that your vector database contains numerical representations of your target data, you can  perform a semantic search. Vector databases shine in semantic search use cases because end  users form queries with ambiguous natural language. "
            },
            {
                "page_number": 6,
                "text": "Semantic search works by converting the user’s query into embeddings and using a vector  database to search for similar entries. Let's see what happens to our retrieval augmented generation example when we are able to  inject context dynamically: Retrieval Augmented Generation (RAG) uses semantic search to retrieve relevant and timely  context that LLMs use to produce more accurate responses. You originally converted your proprietary data into embeddings. When the user issues a query or  question, you translate their natural language search terms into embeddings. You send these embeddings to the vector database. The database performs a “nearest  neighbor” search, finding the vectors that most closely resemble the user’s intent. When the  vector database returns the relevant results, your application provides them to the LLM via its  context window, prompting it to perform its generative task. "
            },
            {
                "page_number": 7,
                "text": "Retrieval Augmented Generation reduces the likelihood of hallucinations by providing domain- specific information through an LLM's context window. Since the LLM now has access to the most pertinent and grounding facts from your vector  database, your rag application can provide an accurate answer for your user. RAG reduces the  likelihood of hallucination. Vector databases can support even more advanced search functionality Semantic search is powerful, but it’s posble to go even further. For example, Pinecone’s vector  database supports hybrid search functionality, a retrieval system that considers the query's  semantics and keywords. RAG is the most cost-effective, easy to implement, and lowest-risk path to higher  performance for GenAI applications. Semantic search and Retrieval Augmented Generation provide more relevant GenAI responses,  translating to a superior experience for end-users. Unlike building your foundation model, fine- tuning an existing model, or solely performing prompt engineering, RAG simultaneously  addresses recency and context-specific issues cost-effectively and with lower risk than  alternative approaches. Its primary purpose is to provide context-sensitive, detailed answers to questions that require  access to private data to answer correctly. Pinecone enables you to integrate RAG within minutes. Check out our examples repository on  GitHub for runnable examples, such as this RAG Jupyter Notebook. "
            }
        ],
        "images": [
            "Image_25",
            "Image_37",
            "Image_41",
            "Image_49"
        ]
    },
    {
        "file_name": "retrival techniques.pdf",
        "title": "BM-25",
        "pages": [
            {
                "page_number": 1,
                "text": "Retrieval Techniques - Sparse, Dense, and Hybrid Representations Introduction Information retrieval is the task of finding documents that satisfy an information need from a  large collection of documents. Given the vast amount of data, efficient information retrieval  techniques are essential to a number of applications from web search to recommendations to  conversational chatbots. In a typical retrieval system, the information need is represented by a user query. The user query  is efficiently compared to all the available documents and the most relevant documents are  returned to the user. A crucial task in information retrieval is the representation of the  documents and the query. There are two major representations - sparse and dense. Sparse Representations Sparse retrieval methods represent text by the counts of each word in the vocabulary. Since the  vocabulary is generally very large and documents typically contain only a small subset of the  vocabulary, most of the counts are zero, resulting in a sparse vector. A major advantage of sparse representation is the efficiency of retrieval using an inverted index.  An inverted index keeps track of the documents containing each word or phrase. Given a query,  an inverted index can quickly retrieve all documents containing the query terms. The retrieved  documents can then be scored based on the frequency of the query terms. However, not all  terms are equally important, and their significance varies. Term Frequency - Inverse Document Frequency (TF-IDF) Some words appear in nearly all documents, diminishing their utility for distinguishing relevant  documents. To improve retriever performance, TF-IDF assigns weights to terms based on their  frequency across documents, counting the words while attenuating the influence of terms with  high document frequency. BM-25 A large document will inherently have a higher TF-IDF similarity with a query than a smaller one,  potentially skewing results. BM-25 addresses this by incorporating document length  normalization into TF-IDF, adjusting term weights to ensure fairer comparisons. BM-25 is a  robust baseline retrieval method, performing competitively across various benchmarks. Dense Representations Sparse retrieval methods are efficient but suffer from the vocabulary mismatch problem— lexical terms present in the query but not in a relevant document, and vice versa. Dense retrieval  methods alleviate this problem by representing text in continuous vector spaces, allowing for  better semantic matching even when exact terms differ. Most of the common dense representations are based on some form of the transformer  architecture. A general distinction between the types of dense representations is based on  whether the collection of documents can be encoded independent of the query or not. Bi-Encoders Usually, the collection of documents from which we retrieve relevant documents is quite large.  Bi-Encoder-based representations encode the documents independently of the queries. Since "
            },
            {
                "page_number": 2,
                "text": "encoding with a transformer is expensive, documents are encoded once and stored in a  database. During a search, the query is encoded, and an approximate nearest neighbour search  is performed to find the most relevant documents based on cosine similarity between the query  vector and the document vectors. Bi-Encoder retrieval system - the documents are encoded once for all queries. Figure source [1] The majority of stage-1 retrieval systems use the bi-encoder architecture. Some of the common  bi-encoder systems are given below.    Dense Passage Retrieval (DPR) and Sentence BERT (SBERT): Both of these techniques  use a supervised training set of a (query, positive document, negative document) to  contrastively train a transformer encoder model [2].    Contriever: Similar to the above techniques contriever uses contrastive training.  However, contriever uses self-supervised learning, i.e., it generates the (query, positive  document, negative document) tuple from general text. This allows contriever to train on  much larger datasets and thus performing better than supervised techniques [3]. "
            },
            {
                "page_number": 3,
                "text": "   ColBERT (Contextualized Late Interaction): The previous techniques encode the  document tokens and pool the resulting embeddings to generate a single embedding  vector. ColBERT, in contrast, encodes the documents with a pre-trained BERT and keep  the embeddings of all the tokens. During search, the query tokens are embedded and a  score is calculated using a MaxSim operation involving all tokens from both the  document and the query. ColBERT stores the embeddings of all the document tokens and perform late interaction with  query at search time. Figure source [1] Cross-Encoder A cross-encoder architecture encodes both the document and the query using a single encoder  simultaneously to get the similarity score between the query and each document. Since this  process is very expensive, cross-encoders are used as stage 2 re-rankers where the number of  documents have been reduced to a few dozens instead of hundreds of millions. "
            },
            {
                "page_number": 4,
                "text": "Cross Encoder architecture encoding both the query and the document to compute a similarity  score. Figure source [1] Common cross-encoders include the following    BERT: In a pre-trained BERT can directly be used as a cross-encoder, feeding both the  query and each document seperated by the [SEP] token and using the embedding of  [CLS] token to compute the similarity score.    T5: Similar to BERT, T5 can be used to concatenate the query and each document to  compute the score of each document for retrieval. Hybrid Techniques Sparse techniques are efficient, and dense techniques alleviate the vocabulary mismatch  problem. Hybrid techniques attempt to unify the best of both worlds using document  expansion. One of the most popular hybrid techniques is the SParse Lexical AnD Expansion  (SPLADE) model [4]. "
            },
            {
                "page_number": 5,
                "text": "Hybrid techniques retain the efficiency of sparse methods by focusing on exact term matches  while using dense embeddings to identify related terms for expansion. In SPLADE, a sparse  representation of the document is expanded with additional related terms identified through  dense embeddings, addressing the vocabulary mismatch problem. For example, consider a one-hot vector for the query 'car': q = [0,1,0,0, ... ,0] Here, the second position corresponds to 'car,' and all other positions are zero. SPLADE uses  dense embeddings to expand the document representation to include related terms like  'vehicle' and 'automobile,' resulting in: q = [0,1,0,1, ... ,1] This ensures the query matches documents containing these related terms, improving recall  and relevance. References [1]: Khattab and Zaharia 2020 ColBERT: Efficient and Effective Passage Search via  Contextualized Late Interaction over BERT [2] Karpukhin et al. 2020 Dense Passage Retrieval for Open-Domain Question Answering [3] Izacard et al. 2022 Unsupervised Dense Information Retrieval with Contrastive Learning [4] Formal et al. 2021 SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking "
            }
        ],
        "images": [
            "Image_25",
            "Image_28",
            "Image_31"
        ]
    },
    {
        "file_name": "sparse_and_dense.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Understanding the Difference between Sparse and Dense Information Retrieval In the world of machine learning, information retrieval plays a crucial role in various  applications. One of the key aspects of information retrieval is the representation of documents.  Two popular methods that are often employed are Sparse Retrieval (SR) and Dense Retrieval  (DR). While both aim to capture the essence of a document, they differ in their approach and  underlying techniques. Dense Retrieval involves encoding documents as dense vectors using pre-trained language  models like BERT, T5, or GPT. These models leverage deep learning techniques to generate high- dimensional representations of the text. However, this poses a challenge when it comes to  traditional methods like inverted indexes, which rely on the sparsity of word occurrences. To  address this, Approximate Nearest Neighbor search methods, such as FAISS, are used to find  document embeddings similar to the query. On the other hand, Sparse Retrieval focuses on projecting documents into sparse vectors that  align with the vocabulary of the document's language. Traditional methods like TF-IDF or BM25  are commonly used for this purpose. However, with the rise of transformer-based models, even  sparse retrieval methods have evolved. Approaches like SPLADE utilize neural models to infer  relevant vocabulary terms for a document, even if they are not explicitly present. This addresses  the lexical gap, where a term may be highly relevant to a document without being mentioned  verbatim. Both sparse and dense retrieval methods have their own advantages and limitations. Sparse  retrieval excels in capturing explicit word occurrences and can be efficient for large-scale  indexing. However, it may struggle with capturing implicit or contextual information. On the  other hand, dense retrieval is better suited for understanding the semantic meaning of  documents. It can capture complex relationships and context that may not be captured by  sparse methods. However, dense retrieval requires more computational resources and is often  slower than its sparse counterpart. In the field of Language Model (LM) research, there are ongoing challenges that researchers are  actively working on. One of these challenges is reducing hallucination, where the LM generates  responses that are not grounded in the given context. To address this, researchers have  proposed techniques such as adding more context to the prompt, emphasizing chain-of- thought responses, promoting self-consistency, or training the model to be concise in its  responses. Another challenge in LM research is optimizing the retrieval process itself. Techniques like the  Retrieval-Augmented Generation (RAG) framework have been developed to improve the  efficiency and accuracy of information retrieval. RAG works in two phases: chunking/indexing  and querying. In the chunking phase, the documents are divided into smaller chunks, and their  embeddings are stored in a vector database. In the querying phase, when a user sends a query,  it is converted into an embedding. The vector database then retrieves the most similar chunks to  the query embedding. This approach enables faster retrieval by focusing on relevant chunks  rather than searching through the entire document index. To make the retrieval process even more effective, it has been observed that models tend to  perform better at understanding information at the beginning and end of the index compared to  the middle. This insight can be leveraged to optimize the retrieval system by placing important  or frequently accessed documents at the extremes of the index. "
            },
            {
                "page_number": 2,
                "text": "In conclusion, understanding the difference between sparse and dense information retrieval is  crucial in the field of machine learning. While sparse retrieval focuses on explicit word  occurrences and utilizes traditional methods like TF-IDF, dense retrieval leverages deep  learning models to capture semantic meaning. Both methods have their own advantages and  limitations, and researchers are actively working on addressing challenges such as  hallucination and optimizing the retrieval process. To improve information retrieval,  incorporating techniques like RAG and leveraging the distribution of important documents  within the index can lead to more efficient and accurate retrieval systems. Actionable Advice:    1. When implementing information retrieval systems, consider the nature of your data  and the specific requirements of your application to determine whether sparse or dense  retrieval will be more suitable.    2. Experiment with different retrieval techniques such as FAISS for dense retrieval or  traditional methods like TF-IDF for sparse retrieval to find the approach that yields the  best results for your specific use case.    3. When working with language models, focus on reducing hallucination by providing  additional context, encouraging self-consistency, or training the model to generate  concise responses. This can significantly improve the quality and relevance of the  generated output. Resource: 1. r/MachineLearning - [D] Difference between sparse and dense information retrieval (Glasp) 2. Open challenges in LLM research (Glasp) "
            }
        ],
        "images": []
    },
    {
        "file_name": "supervised_vs_unsupervised.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Supervised vs. unsupervised learning: What's the difference? Supervised and unsupervised learning are the two primary approaches in artificial  intelligence and machine learning. The simplest way to differentiate between supervised and  unsupervised learning is how the models are trained and the type of training data the algorithms  use. However, there are some other differences between supervised learning and unsupervised  learning, which make certain techniques better suited for helping organizations accomplish  their specific goals and business objectives. Here, we’ll cover the key differences between supervised and unsupervised machine learning to  help you understand which approaches best suit your needs. The difference between supervised and unsupervised learning The biggest difference between supervised and unsupervised machine learning is the type of  data used. Supervised learning uses labeled training data, and unsupervised learning does not. More simply, supervised learning models have a baseline understanding of what the correct  output values should be. With supervised learning, an algorithm uses a sample dataset to train itself to make predictions,  iteratively adjusting itself to minimize error. These datasets are labeled for context, providing the  desired output values to enable a model to give a “correct” answer. In contrast, unsupervised learning algorithms work independently to learn the data's inherent  structure without any specific guidance or instruction. You simply provide unlabeled input data  and let the algorithm identify any naturally occurring patterns in the dataset. While the type of data is the easiest way to differentiate between these two approaches, they  each have different goals and applications that also set them apart from each other. Supervised learning models are more focused on learning the relationships between input and  output data. For example, a supervised model might be used to predict flight times based on  specific parameters, such as weather conditions, airport traffic, peak flight hours, and more. On the other hand, unsupervised learning is more helpful for discovering new patterns and  relationships in raw, unlabeled data. Unsupervised learning models, for instance, might be used  to identify buyer groups that purchase related products together to provide suggestions for  other items to recommend to similar customers. As a result, supervised and unsupervised machine learning are deployed to solve different types  of problems. Supervised machine learning is suited for classification and regression tasks, such  as weather forecasting, pricing changes, sentiment analysis, and spam detection. While  unsupervised learning is more commonly used for exploratory data analysis and clustering  tasks, such as anomaly detection, big data visualization, or customer segmentation. How to choose between supervised and unsupervised learning Now that you understand the differences between supervised and unsupervised learning, which  approach is right for you? "
            },
            {
                "page_number": 2,
                "text": "Choosing the right approach depends on your overall goals and requirements, the use cases  you wish to solve, and your team’s overall approach to analyzing, processing, and managing  data. Generally, you’ll need to consider the following things when deciding which option works best  for your organization.    Is your data labeled or unlabeled? Supervised learning requires labeled datasets.  You’ll need to assess whether your organization has the time, resources, and expertise  to validate and label data.    What are your goals? It’s important to consider the type of problem you’re trying to  solve and whether you are trying to create a prediction model or looking to discover new  insights or hidden patterns in data.    What types of algorithms do you need? When deciding what approach is best suited  for your organization, it’s also important to evaluate if there are algorithms that can  support the volume of data and match the required dimensions, such as the number of  features and attributes. The choice between supervised vs. unsupervised learning comes down to the specific problem  you want to solve, the data you have available, and whether you have the tools and experience  to build and manage your models. What is semi-supervised learning? Not sure that either of these options is the right fit? You could also consider a third approach:  semi-supervised learning. Semi-supervised learning combines aspects of both supervised learning and unsupervised  learning. Machine learning techniques that fall under this category utilize both labeled and  unlabeled data to train a predictive model. Semi-supervised learning uses a small amount of labeled data to train an initial model, which  can be used to predict labels on a larger amount of unlabeled data. The model is then applied  iteratively to both originally labeled data and data with predicted labels (pseudo-labels). After,  you will add your most accurate predictions to the labeled dataset and repeat the process again  to continue improving the performance of your model. "
            }
        ],
        "images": []
    },
    {
        "file_name": "suprovised.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "What is supervised learning? Supervised learning, also known as supervised machine learning, is a subcategory of machine  learning and artificial intelligence. It is defined by its use of labeled data sets to train algorithms  that to classify data or predict outcomes accurately. As input data is fed into the model, it adjusts its weights until the model has been fitted  appropriately, which occurs as part of the cross validation process. Supervised learning helps  organizations solve for a variety of real-world problems at scale, such as classifying spam in a  separate folder from your inbox. It can be used to build highly accurate machine learning  models. How supervised learning works Supervised learning uses a training set to teach models to yield the desired output. This training  dataset includes inputs and correct outputs, which allow the model to learn over time. The  algorithm measures its accuracy through the loss function, adjusting until the error has been  sufficiently minimized. Supervised learning can be separated into two types of problems when data mining— classification and regression:    Classification uses an algorithm to accurately assign test data into specific categories. It  recognizes specific entities within the dataset and attempts to draw some conclusions  on how those entities should be labeled or defined. Common classification algorithms  are linear classifiers, support vector machines (SVM), decision trees, k-nearest  neighbor, and random forest, which are described in more detail below.    Regression is used to understand the relationship between dependent and independent  variables. It is commonly used to make projections, such as for sales revenue for a given  business. Linear regression, logistical regression, and polynomial regression are popular  regression algorithms. Supervised learning algorithms Various algorithms and computations techniques are used in supervised machine learning  processes. Below are brief explanations of some of the most commonly used learning methods,  typically calculated through use of programs like R or Python: Neural networks: Primarily leveraged for deep learning algorithms, neural networks process  training data by mimicking the interconnectivity of the human brain through layers of nodes.  Each node is made up of inputs, weights, a bias (or threshold), and an output. If that output  value exceeds a given threshold, it “fires” or activates the node, passing data to the next layer in  the network. Neural networks learn this mapping function through supervised learning,  adjusting based on the loss function through the process of gradient descent. When the cost  function is at or near zero, we can be confident in the model’s accuracy to yield the correct  answer. Naive bayes: Naive Bayes is classification approach that adopts the principle of class  conditional independence from the Bayes Theorem. This means that the presence of one  feature does not impact the presence of another in the probability of a given outcome, and each  predictor has an equal effect on that result. There are three types of Naïve Bayes classifiers: "
            },
            {
                "page_number": 2,
                "text": "Multinomial Naïve Bayes, Bernoulli Naïve Bayes, and Gaussian Naïve Bayes. This technique is  primarily used in text classification, spam identification, and recommendation systems. Linear regression: Linear regression is used to identify the relationship between a dependent  variable and one or more independent variables and is typically leveraged to make predictions  about future outcomes. When there is only one independent variable and one dependent  variable, it is known as simple linear regression. As the number of independent variables  increases, it is referred to as multiple linear regression. For each type of linear regression, it  seeks to plot a line of best fit, which is calculated through the method of least squares.  However, unlike other regression models, this line is straight when plotted on a graph. Logistic regression: While linear regression is leveraged when dependent variables are  continuous, logistic regression is selected when the dependent variable is categorical, meaning  they have binary outputs, such as 'true' and 'false' or 'yes' and 'no.' While both regression  models seek to understand relationships between data inputs, logistic regression is mainly  used to solve binary classification problems, such as spam identification. Support vector machines (SVM): A support vector machine is a popular supervised learning  model developed by Vladimir Vapnik, used for both data classification and regression. That  said, it is typically leveraged for classification problems, constructing a hyperplane where the  distance between two classes of data points is at its maximum. This hyperplane is known as the  decision boundary, separating the classes of data points (e.g., oranges vs. apples) on either side  of the plane. K-nearest neighbor: K-nearest neighbor, also known as the KNN algorithm, is a non- parametric algorithm that classifies data points based on their proximity and association to  other available data. This algorithm assumes that similar data points can be found near each  other. As a result, it seeks to calculate the distance between data points, usually through  Euclidean distance, and then it assigns a category based on the most frequent category or  average. Its ease of use and low calculation time make it a preferred algorithm by data  scientists, but as the test dataset grows, the processing time lengthens, making it less  appealing for classification tasks. KNN is typically used for recommendation engines and image  recognition. Random forest: Random forest is another flexible supervised machine learning algorithm  used for both classification and regression purposes. The 'forest' references a collection of  uncorrelated decision trees, which are then merged together to reduce variance and create  more accurate data predictions. Unsupervised vs. supervised vs. semi-supervised learning Unsupervised machine learning and supervised machine learning are frequently discussed  together. Unlike supervised learning, unsupervised learning uses unlabeled data. From that  data, it discovers patterns that help solve for clustering or association problems. This is  particularly useful when subject matter experts are unsure of common properties within a data  set. Common clustering algorithms are hierarchical, k-means, and Gaussian mixture models. Semi-supervised learning occurs when only part of the given input data has been labeled.  Unsupervised and semi-supervised learning can be more appealing alternatives as it can be  time-consuming and costly to rely on domain expertise to label data appropriately for  supervised learning. "
            },
            {
                "page_number": 3,
                "text": "For a deep dive into the differences between these approaches, check out 'Supervised vs.  Unsupervised Learning: What's the Difference?' Supervised learning examples Supervised learning models can be used to build and advance a number of business  applications, including the following:    Image- and object-recognition: Supervised learning algorithms can be used to locate,  isolate, and categorize objects out of videos or images, making them useful when  applied to various computer vision techniques and imagery analysis.    Predictive analytics: A widespread use case for supervised learning models is in creating  predictive analytics systems to provide deep insights into various business data points.  This allows enterprises to anticipate certain results based on a given output variable,  helping business leaders justify decisions or pivot for the benefit of the organization.    Customer sentiment analysis: Using supervised machine learning algorithms,  organizations can extract and classify important pieces of information from large  volumes of data—including context, emotion, and intent—with very little human  intervention. This can be incredibly useful when gaining a better understanding of  customer interactions and can be used to improve brand engagement efforts.      Spam detection: Spam detection is another example of a supervised learning model.  Using supervised classification algorithms, organizations can train databases to  recognize patterns or anomalies in new data to organize spam and non-spam-related  correspondences effectively. Challenges of supervised learning Although supervised learning can offer businesses advantages, such as deep data insights  and improved automation, there are some challenges when building sustainable supervised  learning models. The following are some of these challenges:    Supervised learning models can require certain levels of expertise to structure  accurately.    Training supervised learning models can be very time intensive.    Datasets can have a higher likelihood of human error, resulting in algorithms learning  incorrectly.    Unlike unsupervised learning models, supervised learning cannot cluster or classify  data on its own. "
            }
        ],
        "images": [
            "Image_26",
            "Image_31",
            "Image_35",
            "Image_31",
            "Image_37",
            "Image_39"
        ]
    },
    {
        "file_name": "tokenation1.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "What is Tokenization? Tokenization breaks text into smaller parts for easier machine analysis, helping machines  understand human language. Tokenization, in the realm of Natural Language Processing (NLP) and machine learning, refers to  the process of converting a sequence of text into smaller parts, known as tokens. These tokens  can be as small as characters or as long as words. The primary reason this process matters is  that it helps machines understand human language by breaking it down into bite-sized pieces,  which are easier to analyze. Tokenization Explained Imagine you're trying to teach a child to read. Instead of diving straight into complex paragraphs,  you'd start by introducing them to individual letters, then syllables, and finally, whole words. In a  similar vein, tokenization breaks down vast stretches of text into more digestible and  understandable units for machines. The primary goal of tokenization is to represent text in a manner that's meaningful for machines  without losing its context. By converting text into tokens, algorithms can more easily identify  patterns. This pattern recognition is crucial because it makes it possible for machines to  understand and respond to human input. For instance, when a machine encounters the word  'running', it doesn't see it as a singular entity but rather as a combination of tokens that it can  analyze and derive meaning from. To delve deeper into the mechanics, consider the sentence, 'Chatbots are helpful.' When we  tokenize this sentence by words, it transforms into an array of individual words: ['Chatbots', 'are', 'helpful']. This is a straightforward approach where spaces typically dictate the boundaries of tokens.  However, if we were to tokenize by characters, the sentence would fragment into: ['C', 'h', 'a', 't', 'b', 'o', 't', 's', ' ', 'a', 'r', 'e', ' ', 'h', 'e', 'l', 'p', 'f', 'u', 'l']. This character-level breakdown is more granular and can be especially useful for certain  languages or specific NLP tasks. In essence, tokenization is akin to dissecting a sentence to understand its anatomy. Just as  doctors study individual cells to understand an organ, NLP practitioners use tokenization to  dissect and understand the structure and meaning of text. It's worth noting that while our discussion centers on tokenization in the context of language  processing, the term 'tokenization' is also used in the realms of security and privacy,  particularly in data protection practices like credit card tokenization. In such scenarios,  sensitive data elements are replaced with non-sensitive equivalents, called tokens. This  distinction is crucial to prevent any confusion between the two contexts. Types of Tokenization Tokenization methods vary based on the granularity of the text breakdown and the specific  requirements of the task at hand. These methods can range from dissecting text into individual "
            },
            {
                "page_number": 2,
                "text": "words to breaking them down into characters or even smaller units. Here's a closer look at the  different types:    Word tokenization. This method breaks text down into individual words. It's the most  common approach and is particularly effective for languages with clear word boundaries  like English.    Character tokenization. Here, the text is segmented into individual characters. This  method is beneficial for languages that lack clear word boundaries or for tasks that  require a granular analysis, such as spelling correction.    Subword tokenization. Striking a balance between word and character tokenization,  this method breaks text into units that might be larger than a single character but  smaller than a full word. For instance, 'Chatbots' could be tokenized into 'Chat' and  'bots'. This approach is especially useful for languages that form meaning by combining  smaller units or when dealing with out-of-vocabulary words in NLP tasks. Here's a table explaining the differences: Type  Description  Use Cases Word  Tokenization  Breaks text into individual words. Effective for languages with clear word boundaries like English. Character  Tokenization Segments text into individual  characters. Useful for languages without clear word  boundaries or tasks requiring granular  analysis. Subword  Tokenization Breaks text into units larger than  characters but smaller than  words. Beneficial for languages with complex  morphology or handling out-of-vocabulary  words. Tokenization Use Cases Tokenization serves as the backbone for a myriad of applications in the digital realm, enabling  machines to process and understand vast amounts of text data. By breaking down text into  manageable chunks, tokenization facilitates more efficient and accurate data analysis. Here are  some prominent use cases, along with real-world applications: Search engines When you type a query into a search engine like Google, it employs tokenization to dissect your  input. This breakdown helps the engine sift through billions of documents to present you with  the most relevant results. Machine translation Tools such as Google Translate utilize tokenization to segment sentences in the source  language. Once tokenized, these segments can be translated and then reconstructed in the  target language, ensuring the translation retains the original context. Speech recognition "
            },
            {
                "page_number": 3,
                "text": "Voice-activated assistants like Siri or Alexa rely heavily on tokenization. When you pose a  question or command, your spoken words are first converted into text. This text is then  tokenized, allowing the system to process and act upon your request. Sentiment analysis in reviews Tokenization plays a crucial role in extracting insights from user-generated content, such as  product reviews or social media posts. For instance, a sentiment analysis system for e- commerce platforms might tokenize user reviews to determine whether customers are  expressing positive, neutral, or negative sentiments. For example:    The review: 'This product is amazing, but the delivery was late.'    After tokenization: ['This', 'product', 'is', 'amazing', ',', 'but', 'the', 'delivery', 'was',  'late', '.'] The tokens 'amazing' and 'late' can then be processed by the sentiment model to assign mixed  sentiment labels, providing actionable insights for businesses. Chatbots and virtual assistants Tokenization enables chatbots to understand and respond to user inputs effectively. For  example, a customer service chatbot might tokenize the query: 'I need to reset my password but can't find the link.' Which is tokenized as: ['I', 'need', 'to', 'reset', 'my', 'password', 'but', 'can't', 'find', 'the',  'link']. This breakdown helps the chatbot identify the user's intent ('reset password') and respond  appropriately, such as by providing a link or instructions. Tokenization Challenges Navigating the intricacies of human language, with its nuances and ambiguities, presents a set  of unique challenges for tokenization. Here's a deeper dive into some of these obstacles, along  with recent advancements that address them: Ambiguity Language is inherently ambiguous. Consider the sentence 'Flying planes can be dangerous.'  Depending on how it's tokenized and interpreted, it could mean that the act of piloting planes is  risky or that planes in flight pose a danger. Such ambiguities can lead to vastly different  interpretations. Languages without clear boundaries Some languages, like Chinese, Japanese, or Thai, lack clear spaces between words, making  tokenization more complex. Determining where one word ends and another begins is a  significant challenge in these languages. To address this, advancements in multilingual tokenization models have made significant  strides. For instance: "
            },
            {
                "page_number": 4,
                "text": "   XLM-R (Cross-lingual Language Model - RoBERTa) uses subword tokenization and  large-scale pretraining to handle over 100 languages effectively, including those without  clear word boundaries.    mBERT (Multilingual BERT) employs WordPiece tokenization and has shown strong  performance across a variety of languages, excelling in understanding syntactic and  semantic structures even in low-resource languages. These models not only tokenize text effectively but also leverage shared subword vocabularies  across languages, improving tokenization for scripts that are typically harder to process. Handling special characters Texts often contain more than just words. Email addresses, URLs, or special symbols can be  tricky to tokenize. For instance, should 'john.doe@email.com' be treated as a single token or  split at the period or the '@' symbol? Advanced tokenization models now incorporate rules and  learned patterns to ensure consistent handling of such cases. Implementing Tokenization The landscape of Natural Language Processing offers many tools, each tailored to specific  needs and complexities. Here's a guide to some of the most prominent tools and methodologies  available for tokenization:    NLTK (Natural Language Toolkit). A stalwart in the NLP community, NLTK is a  comprehensive Python library that caters to a wide range of linguistic needs. It offers  both word and sentence tokenization functionalities, making it a versatile choice for  beginners and seasoned practitioners alike.    Spacy. A modern and efficient alternative to NLTK, Spacy is another Python-based NLP  library. It boasts speed and supports multiple languages, making it a favorite for large- scale applications.    BERT tokenizer. Emerging from the BERT pre-trained model, this tokenizer excels in  context-aware tokenization. It's adept at handling the nuances and ambiguities of  language, making it a top choice for advanced NLP projects (see this tutorial on NLP with  BERT).    Advanced techniques. o  Byte-Pair Encoding (BPE). An adaptive tokenization method, BPE tokenizes  based on the most frequent byte pairs in a text. It's particularly effective for  languages that form meaning by combining smaller units. o  SentencePiece. An unsupervised text tokenizer and detokenizer mainly for  Neural Network-based text generation tasks. It handles multiple languages with  a single model and can tokenize text into subwords, making it versatile for  various NLP tasks. Hugging Face Transformers One of the most popular tools for NLP tasks, the Hugging Face Transformers library provides a  seamless integration with PyTorch, making it ideal for both research and production. This library "
            },
            {
                "page_number": 5,
                "text": "includes advanced tokenizers designed to work with state-of-the-art transformer models like  BERT, GPT, and RoBERTa. Key features include:    Fast tokenizers: Built using Rust, these tokenizers offer significant speed  improvements, enabling faster pre-processing for large datasets.    Support for subword tokenization: The library supports Byte-Pair Encoding (BPE),  WordPiece, and Unigram tokenization, ensuring efficient handling of out-of-vocabulary  words and complex languages.    Built-in pretrained tokenizers: Each model in the Hugging Face Transformers library  comes with a corresponding pretrained tokenizer, ensuring compatibility and ease of  use. For instance, the BERT tokenizer splits text into subwords, making it adept at  handling language nuances. Your choice of tool should align with the specific requirements of your project. For those taking  their initial steps in NLP, NLTK or Spacy might offer a more approachable learning curve.  However, for projects demanding a deeper understanding of context and nuance, the Hugging  Face Transformers and BERT tokenizer stand out as robust options. How I Used Tokenization for a Rating Classifier Project I gained my initial experience with text tokenization while working on a portfolio project three  years ago. The project involved a dataset containing user reviews and ratings, which I used to  develop a deep-learning text classification model. I used `word_tokenize` from NLTK to clean  up the text and `Tokenizer` from Keras to preprocess it. Let's explore how I used tokenizers in the project: 1. When working with NLP data, tokenizers are commonly used to process and clean the text dataset. The aim is to eliminate stop words, punctuation, and other irrelevant  information from the text. Tokenizers transform the text into a list of words, which can be  cleaned using a text-cleaning function. 2. Afterward, I used the Keras Tokenizer method to transform the text into an array for analysis and to prepare the tokens for the deep learning model. In this case, I used the  Bidirectional LSTM model, which produced the most favorable outcomes. 3. Next, I converted tokens into a sequence by using the `texts_to_sequences` function. 4. Before feeding the sequence to the model, I had to add padding to make the sequence of numbers the same length. 5. Finally, I split the dataset into training and testing sets, trained the model on the training set, and evaluated it on the testing set. Tokenizer has many benefits in the field of natural language processing where it is used to clean,  process, and analyze text data. Focusing on text processing can improve model performance. I recommend taking the Introduction to Natural Language Processing in Python course to learn  more about the preprocessing techniques and dive deep into the world of tokenizers. "
            }
        ],
        "images": []
    },
    {
        "file_name": "tokenization2.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Understanding Tokenization in NLP: A Beginner’s Guide to Text Processing Tokenization is a critical yet often overlooked component of natural language processing (NLP).  In this guide, we’ll explain tokenization, its use cases, pros and cons, and why it’s involved in  almost every large language model (LLM). Table of contents    What is tokenization in NLP?    Types of tokenization    How tokenization works    Tokenization applications    Benefits of tokenization    Challenges of tokenization What is tokenization in NLP? Tokenization is an NLP method that converts text into numerical formats that machine learning  (ML) models can use. When you send your prompt to an LLM such as Anthropic’s Claude,  Google’s Gemini, or a member of OpenAI’s GPT series, the model does not directly read your  text. These models can only take numbers as inputs, so the text must first be converted into a  sequence of numbers using a tokenizer. One way a tokenizer may tokenize text would be to split it into separate words and assign a  number to each unique word: “Grammarly loves grammar and ML and writing” might become: Each word (and its associated number) is a token. An ML model can use the sequence of  tokens—[7,102], [37], [564], [2], [9,763], [2], [231]—to run its operations and produce its output.  This output is usually a number, which is converted back into text using the reverse of this same  tokenization process. In practice, this word-by-word tokenization is great as an example but is  rarely used in industry for reasons we will see later. One final thing to note is that tokenizers have vocabularies—the complete set of tokens they  can handle. A tokenizer that knows basic English words but not company names may not have  “Grammarly” as a token in its vocabulary, leading to tokenization failure. Types of tokenization In general, tokenization is turning a chunk of text into a sequence of numbers. Though it’s  natural to think of tokenization at the word level, there are many other tokenization methods,  one of which—subword tokenization—is the industry standard. Word tokenization Word tokenization is the example we saw before, where text is split by each word and by  punctuation. "
            },
            {
                "page_number": 2,
                "text": "Word tokenization’s main benefit is that it’s easy to understand and visualize. However, it has a  few shortcomings:    Punctuation, if present, is attached to the words, as with “writing.”    Novel or uncommon words (such as “Grammarly”) take up a whole token. As a result, word tokenization can create vocabularies with hundreds of thousands of tokens.  The problem with large vocabularies is they make training and inference much less efficient— the matrix needed to convert between text and numbers would need to be huge. Additionally, there would be many infrequently used words, and the NLP models wouldn’t have  enough relevant training data to return accurate responses for those infrequent words. If a new  word was invented tomorrow, an LLM using word tokenization would need to be retrained to  incorporate this word. Subword tokenization Subword tokenization splits text into chunks smaller than or equal to words. There is no fixed  size for each token; each token (and its length) is determined by the training process. Subword  tokenization is the industry standard for LLMs. Below is an example, with tokenization done by  the GPT-4o tokenizer: Here, the uncommon word “Grammarly” gets broken down into three tokens: “Gr,” “amm,” and  “arly.” Meanwhile, the other words are common enough in text that they form their own tokens. Subword tokenization allows for smaller vocabularies, meaning more efficient and cheaper  training and inference. Subword tokenizers can also break down rare or novel words into  combinations of smaller, existing tokens. For these reasons, many NLP models use subword  tokenization. Character tokenization Character tokenization splits text into individual characters. Here’s how our example would  look: Every single unique character becomes its own token. This actually requires the smallest  vocabulary since there are only 52 letters in the alphabet (uppercase and lowercase are  regarded as different) and several punctuation marks. Since any English word must be formed  from these characters, character tokenization can work with any new or rare word. However, by standard LLM benchmarks, character tokenization doesn’t perform as well as  subword tokenization in practice. The subword token “car” contains much more information  than the character token “c,” so the attention mechanism in transformers has more information  to run on. Sentence tokenization "
            },
            {
                "page_number": 3,
                "text": "Sentence tokenization turns each sentence in the text into its own token. Our example would  look like: The benefit is that each token contains a ton of information. However, there are several  drawbacks. There are infinite ways to combine words to write sentences. So, the vocabulary  would need to be infinite as well. Additionally, each sentence itself would be pretty rare since even minute differences (such as  “as well” instead of “and”) would mean a different token despite having the same meaning.  Training and inference would be a nightmare. Sentence tokenization is used in specialized use  cases such as sentence sentiment analysis, but otherwise, it’s a rare sight. Tokenization tradeoff: efficiency vs. performance Choosing the right granularity of tokenization for a model is really a complex relationship  between efficiency and performance. With very large tokens (e.g., at the sentence level), the  vocabulary becomes massive. The model’s training efficiency drops because the matrix to hold  all these tokens is huge. Performance plummets since there isn’t enough training data for all the  unique tokens to meaningfully learn relationships. On the other end, with small tokens, the vocabulary becomes small. Training becomes efficient,  but performance may plummet since each token doesn’t contain enough information for the  model to learn token-token relationships. Subword tokenization is right in the middle. Each token has enough information for models to  learn relationships, but the vocabulary is not so large that training becomes inefficient. How tokenization works Tokenization revolves around the training and use of tokenizers. Tokenizers convert text into  tokens and tokens back into text. We’ll discuss subword tokenizers here since they are the most  popular type. Subword tokenizers must be trained to split text effectively. Why is it that “Grammarly” gets split into “Gr,” “amm,” and “arly”? Couldn’t “Gram,” “mar,”  and “ly” also work? To a human eye, it definitely could, but the tokenizer, which has presumably  learned the most efficient representation, thinks differently. A common training algorithm  (though not used in GPT-4o) employed to learn this representation is byte-pair encoding (BPE).  We’ll explain BPE in the next section. Tokenizer training To train a good tokenizer, you need a massive corpus of text to train on. Running BPE on this  corpus works as follows: 1. Split all the text in the corpus into individual characters. Set these as the starting tokens in the vocabulary. "
            },
            {
                "page_number": 4,
                "text": "2. Merge the two most frequently adjacent tokens from the text into one new token and add it to the vocabulary (without deleting the old tokens—this is important). 3. Repeat this process until there are no remaining frequently occurring pairs of adjacent tokens, or the maximum vocabulary size has been reached. As an example, assume that our entire training corpus consists of the text “abc abcd”: 1. The text would be split into [“a”, “b”, “c”, “ ”, “a”, “b”, “c”, “d”]. Note that the fourth entry in that list is a space character. Our vocabulary would then be [“a”, “b”, “c”, “ ”,  “d”]. 2. “a” and “b” most frequently occur next to each other in the text (tied with “b” and “c” but “a” and “b” win alphabetically). So, we combine them into one token, “ab”. The  vocabulary now looks like [“a”, “b”, “c”, “ ”, “d”, “ab”], and the updated text (with the  “ab” token merge applied) looks like [“ab”, “c”, “ ”, “ab”, “c”, “d”]. 3. Now, “ab” and “c” occur most frequently together in the text. We merge them into the token “abc”. The vocabulary then looks like [“a”, “b”, “c”, “ ”, “d”, “ab”, “abc”], and the  updated text looks like [“abc”, “ ”, “abc”, “d”]. 4. We end the process here since each adjacent token pair now only occurs once. Merging tokens further would make the resulting model perform worse on other texts. In  practice, the vocabulary size limit is the limiting factor. With our new vocabulary set, we can map between text and tokens. Even text that we haven’t  seen before, like “cab,” can be tokenized because we didn’t discard the single-character  tokens. We can also return token numbers by simply seeing the position of the token within the  vocabulary. Good tokenizer training requires extremely high volumes of data and a lot of computing—more  than most companies can afford. Companies get around this by skipping the training of their  own tokenizer. Instead, they just use a pre-trained tokenizer (such as the GPT-4o tokenizer  linked above) to save time and money with minimal, if any, loss in model performance. Using the tokenizer So, we have this subword tokenizer trained on a massive corpus using BPE. Now, how do we use  it on a new piece of text? We apply the merge rules we determined in the tokenizer training process. We first split the  input text into characters. Then, we do token merges in the same order as in training. To illustrate, we’ll use a slightly different input text of “dc abc”: 1. We split it into characters [“d”, “c”, “ ”, “a”, “b”, “c”]. 2. The first merge we did in training was “ab” so we do that here: [“d”, “c”, “ ”, “ab”, “c”]. 3. The second merge we did was “abc” so we do that: [“d”, “c”, “ ”, “abc”]. 4. Those are the only merge rules we have, so we are done tokenizing, and we can return the token IDs. If we have a bunch of token IDs and we want to convert this into text, we can simply look up  each token ID in the list and return its associated text. LLMs do this to turn the embeddings "
            },
            {
                "page_number": 5,
                "text": "(vectors of numbers that capture the meaning of tokens by looking at the surrounding tokens)  they work with back into human-readable text. Tokenization applications Tokenization is always the first step in all NLP. Turning text into forms that ML models (and  computers) can work with requires tokenization. Tokenization in LLMs Tokenization is usually the first and last part of every LLM call. The text is turned into tokens first,  then the tokens are converted to embeddings to capture each token’s meaning and passed into  the main parts of the model (the transformer blocks). After the transformer blocks run, the  embeddings are converted back into tokens. Finally, the just-returned token is added to the  input and passed back into the model, repeating the process again. LLMs use subword  tokenization to balance performance and efficiency. Tokenization in search engines Search engines tokenize user queries to standardize them and to better understand user intent.  Search engine tokenization might involve splitting text into words, removing filler words (such as  “the” or “and”), turning uppercase into lowercase, and dealing with characters like hyphens.  Subword tokenization usually isn’t necessary here since performance and efficiency are less  dependent on vocabulary size. Tokenization in machine translation Machine translation tokenization is interesting since the input and output languages are  different. As a result, there will be two tokenizers, one for each language. Subword tokenization  usually works best since it balances the trade-off between model efficiency and model  performance. But some languages, such as Chinese, don’t have a linguistic component smaller  than a word. There, word tokenization is called for. Benefits of tokenization Tokenization is a must-have for any NLP model. Good tokenization lets ML models work  efficiently with text and handle new words well. Tokenization lets models work with text Internally, ML models only work with numbers. The algorithm behind ML models relies entirely  on computation, which itself requires numbers to compute. So, text must be turned into  numbers before ML models can work with them. After tokenization, techniques like attention or  embedding can be run on the numbers. Tokenization generalizes to new and rare text Or more accurately, good tokenization generalizes to new and rare text. With subword and  character tokenization, new texts can be decomposed into sequences of existing tokens. So,  pasting an article with gibberish words into ChatGPT won’t cause it to break (though it may not  give a very coherent response either). Good generalization also allows models to learn  relationships among rare words, based on the relationships in the subtokens. Challenges of tokenization "
            },
            {
                "page_number": 6,
                "text": "Tokenization depends on the training corpus and the algorithm, so results can vary. This can  affect LLMs’ reasoning abilities and their input and output length. Tokenization affects reasoning abilities of LLMs An easy problem that often stumps LLMs is counting the occurrences of the letter “r” in the  word “strawberry.” The model would incorrectly say there were two, though the answer is really  three. This error may partially have been because of tokenization. The subword tokenizer split  “strawberry” into “st,” “raw,” and “berry.” So, the model may not have been able to connect the  one “r” in the middle token to the two “r”s in the last token. The tokenization algorithm chosen  directly affects how words get tokenized and how each token relates to the others. Tokenization affects LLM input and output length LLMs are mostly built on the transformer architecture, which relies on the attention mechanism  to contextualize each token. However, as the number of tokens increases, the time needed for  attention goes up quadratically. So, a text with four tokens will take 16 units of time but a text  with eight tokens will take 64 units of time. This confines LLMs to input and output limits of a few  hundred thousand tokens. With smaller tokens, this can really limit the amount of text you can  feed into the model, reducing the number of tasks you can use it for. "
            }
        ],
        "images": [
            "Image_42",
            "Image_50",
            "Image_52",
            "Image_54",
            "Image_59",
            "Image_52"
        ]
    },
    {
        "file_name": "ToolsandFrameworks1.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Advanced RAG on Hugging Face documentation using LangChain This notebook demonstrates how you can build an advanced RAG (Retrieval Augmented Generation) for answering a user’s question about a specific knowledge base (here, the HuggingFace documentation), using LangChain. For an introduction to RAG, you can check this other cookbook! RAG systems are complex, with many moving parts: here is a RAG diagram, where we noted in blue all possibilities for system enhancement: 💡As you can see, there are many steps to tune in this architecture: tuning the system properly will yield significant performance gains. In this notebook, we will take a look into many of these blue notes to see how to tune your RAG system and get the best performance. "
            },
            {
                "page_number": 2,
                "text": "Let’s dig into the model building! First, we install the required model dependancies. !pip install -q torch transformers transformers accelerate bitsandbytes langchain sentence-transformers faiss-cpu openpyxl pacmap datasets langchain-community ragatouille from tqdm.notebook import tqdm import pandas as pd from typing import Optional, List, Tuple from datasets import Dataset import matplotlib.pyplot as plt pd.set_option('display.max_colwidth', None) # This will be helpful when visualizing retriever outputs Load your knowledge base import datasets ds = datasets.load_dataset('m-ric/huggingface_doc', split='train') from langchain.docstore.document import Document as LangchainDocument RAW_KNOWLEDGE_BASE = [ LangchainDocument(page_content=doc['text'], metadata={'source': doc['source']}) for doc in tqdm(ds) ] 1. Retriever - embeddings 🗂️ The retriever acts like an internal search engine: given the user query, it returns a few relevant snippets from your knowledge base. These snippets will then be fed to the Reader Model to help it generate its answer. So our objective here is, given a user question, to find the most relevant snippets from our knowledge base to answer that question. "
            },
            {
                "page_number": 3,
                "text": "This is a wide objective, it leaves open some questions. How many snippets should we retrieve? This parameter will be named top_k. How long should these snippets be? This is called the chunk size. There’s no one-size-fits-all answers, but here are a few elements:  🔀Your chunk size is allowed to vary from one snippet to the other.  Since there will always be some noise in your retrieval, increasing the top_k increases the chance to get relevant elements in your retrieved snippets. 🎯Shooting more arrows increases your probability of hitting your target.  Meanwhile, the summed length of your retrieved documents should not be too high: for instance, for most current models 16k tokens will probably drown your Reader model in information due to Lost-in-the-middle phenomenon. 🎯Give your reader model only the most relevant insights, not a huge pile of books! In this notebook, we use Langchain library since it offers a huge variety of options for vector databases and allows us to keep document metadata throughout the processing. 1.1 Split the documents into chunks  In this part, we split the documents from our knowledge base into smaller chunks which will be the snippets on which the reader LLM will base its answer.  The goal is to prepare a collection of semantically relevant snippets. So their size should be adapted to precise ideas: too small will truncate ideas, and too large will dilute them. 💡Many options exist for text splitting: splitting on words, on sentence boundaries, recursive chunking that processes documents in a tree-like way to preserve structure information… To learn more about chunking, I recommend you read this great notebook by Greg Kamradt.  Recursive chunking breaks down the text into smaller parts step by step using a given list of separators sorted from the most important to the least important separator. If the first split doesn’t give the right size or shape of chunks, the method repeats itself on the new chunks using a different separator. For instance with the list of separators ['', '', '.', '']: "
            },
            {
                "page_number": 4,
                "text": " The method will first break down the document wherever there is a double line break ''.  Resulting documents will be split again on simple line breaks '', then on sentence ends '.'.  Finally, if some chunks are still too big, they will be split whenever they overflow the maximum size.  With this method, the global structure is well preserved, at the expense of getting slight variations in chunk size. This space lets you visualize how different splitting options affect the chunks you get. 🔬Let’s experiment a bit with chunk sizes, beginning with an arbitrary size, and see how splits work. We use Langchain’s implementation of recursive chunking with RecursiveCharacterTextSplitter.  Parameter chunk_size controls the length of individual chunks: this length is counted by default as the number of characters in the chunk.  Parameter chunk_overlap lets adjacent chunks get a bit of overlap on each other. This reduces the probability that an idea could be cut in half by the split between two adjacent chunks. We ~arbitrarily set this to 1/10th of the chunk size, you could try different values! from langchain.text_splitter import RecursiveCharacterTextSplitter # We use a hierarchical list of separators specifically tailored for splitting Markdown documents # This list is taken from LangChain's MarkdownTextSplitter class MARKDOWN_SEPARATORS = [ '#{1,6} ', '```', '+', '---+', '___+', '', '', ' ', '', ] text_splitter = RecursiveCharacterTextSplitter( chunk_size=1000, # The maximum number of characters in a chunk: we selected this value arbitrarily chunk_overlap=100, # The number of characters to overlap between chunks add_start_index=True, # If `True`, includes chunk's start index in metadata strip_whitespace=True, # If `True`, strips whitespace from the start and end of every document "
            },
            {
                "page_number": 5,
                "text": "separators=MARKDOWN_SEPARATORS, ) docs_processed = [] for doc in RAW_KNOWLEDGE_BASE: docs_processed += text_splitter.split_documents([doc]) We also have to keep in mind that when embedding documents, we will use an embedding model that accepts a certain maximum sequence length max_seq_length. So we should make sure that our chunk sizes are below this limit because any longer chunk will be truncated before processing, thus losing relevancy. from sentence_transformers import SentenceTransformer # To get the value of the max sequence_length, we will query the underlying `SentenceTransformer` object used in the RecursiveCharacterTextSplitter print(f'Model's maximum sequence length: {SentenceTransformer('thenlper/gte-small').max_seq_length}') from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained('thenlper/gte-small') lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)] # Plot the distribution of document lengths, counted as the number of tokens fig = pd.Series(lengths).hist() plt.title('Distribution of document lengths in the knowledge base (in count of tokens)') plt.show() Model's maximum sequence length: 512 👀As you can see, the chunk lengths are not aligned with our limit of 512 tokens, and some documents are above the limit, thus some part of them will be lost in truncation!  So we should change the RecursiveCharacterTextSplitter class to count length in number of tokens instead of number of characters.  Then we can choose a specific chunk size, here we would choose a lower threshold than 512: "
            },
            {
                "page_number": 6,
                "text": " smaller documents could allow the split to focus more on specific ideas.  But too small chunks would split sentences in half, thus losing meaning again: the proper tuning is a matter of balance. from langchain.text_splitter import RecursiveCharacterTextSplitter from transformers import AutoTokenizer EMBEDDING_MODEL_NAME = 'thenlper/gte-small' def split_documents( chunk_size: int, knowledge_base: List[LangchainDocument], tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME, ) -> List[LangchainDocument]: ''' Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents. ''' text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer( AutoTokenizer.from_pretrained(tokenizer_name), chunk_size=chunk_size, chunk_overlap=int(chunk_size / 10), add_start_index=True, strip_whitespace=True, separators=MARKDOWN_SEPARATORS, ) docs_processed = [] for doc in knowledge_base: docs_processed += text_splitter.split_documents([doc]) # Remove duplicates unique_texts = {} docs_processed_unique = [] for doc in docs_processed: if doc.page_content not in unique_texts: unique_texts[doc.page_content] = True docs_processed_unique.append(doc) return docs_processed_unique docs_processed = split_documents( 512, # We choose a chunk size adapted to our model RAW_KNOWLEDGE_BASE, tokenizer_name=EMBEDDING_MODEL_NAME, ) # Let's visualize the chunk sizes we would have in tokens from a common model from transformers import AutoTokenizer "
            },
            {
                "page_number": 7,
                "text": "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME) lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)] fig = pd.Series(lengths).hist() plt.title('Distribution of document lengths in the knowledge base (in count of tokens)') plt.show() ➡️Now the chunk length distribution looks better! 1.2 Building the vector database We want to compute the embeddings for all the chunks of our knowledge base: to learn more about sentence embeddings, we recommend reading this guide. How does retrieval work? Once the chunks are all embedded, we store them in a vector database. When the user types in a query, it gets embedded by the same model previously used, and a similarity search returns the closest documents from the vector database. The technical challenge is thus, given a query vector, to quickly find the nearest neighbors of this vector in the vector database. To do this, we need to choose two "
            },
            {
                "page_number": 8,
                "text": "things: a distance, and a search algorithm to find the nearest neighbors quickly within a database of thousands of records. Nearest Neighbor search algorithm There are plentiful choices for the nearest neighbor search algorithm: we go with Facebook’s FAISS since FAISS is performant enough for most use cases, and it is well known and thus widely implemented. Distances Regarding distances, you can find a good guide here. In short:  Cosine similarity computes the similarity between two vectors as the cosinus of their relative angle: it allows us to compare vector directions regardless of their magnitude. Using it requires normalizing all vectors, to rescale them into unit norm.  Dot product takes into account magnitude, with the sometimes undesirable effect that increasing a vector’s length will make it more similar to all others.  Euclidean distance is the distance between the ends of vectors. You can try this small exercise to check your understanding of these concepts. But once vectors are normalized, the choice of a specific distance does not matter much. Our particular model works well with cosine similarity, so choose this distance, and we set it up both in the Embedding model, and in the distance_strategy argument of our FAISS index. With cosine similarity, we have to normalize our embeddings. 🚨👇The cell below takes a few minutes to run on A10G! from langchain.vectorstores import FAISS from langchain_community.embeddings import HuggingFaceEmbeddings from langchain_community.vectorstores.utils import DistanceStrategy embedding_model = HuggingFaceEmbeddings( model_name=EMBEDDING_MODEL_NAME, multi_process=True, model_kwargs={'device': 'cuda'}, encode_kwargs={'normalize_embeddings': True}, # Set `True` for cosine similarity ) "
            },
            {
                "page_number": 9,
                "text": "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents( docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE ) 👀To visualize the search for the closest documents, let’s project our embeddings from 384 dimensions down to 2 dimensions using PaCMAP. 💡We chose PaCMAP rather than other techniques such as t-SNE or UMAP, since it is efficient (preserves local and global structure), robust to initialization parameters and fast. # Embed a user query in the same space user_query = 'How to create a pipeline object?' query_vector = embedding_model.embed_query(user_query) import pacmap import numpy as np import plotly.express as px embedding_projector = pacmap.PaCMAP(n_components=2, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, random_state=1) embeddings_2d = [ list(KNOWLEDGE_VECTOR_DATABASE.index.reconstruct_n(idx, 1)[0]) for idx in range(len(docs_processed)) ] + [query_vector] # Fit the data (the index of transformed data corresponds to the index of the original data) documents_projected = embedding_projector.fit_transform(np.array(embeddings_2d), init='pca') df = pd.DataFrame.from_dict( [ { 'x': documents_projected[i, 0], 'y': documents_projected[i, 1], 'source': docs_processed[i].metadata['source'].split('/')[1], 'extract': docs_processed[i].page_content[:100] + '...', 'symbol': 'circle', 'size_col': 4, } for i in range(len(docs_processed)) ] + [ { "
            },
            {
                "page_number": 10,
                "text": "'x': documents_projected[-1, 0], 'y': documents_projected[-1, 1], 'source': 'User query', 'extract': user_query, 'size_col': 100, 'symbol': 'star', } ] ) # Visualize the embedding fig = px.scatter( df, x='x', y='y', color='source', hover_data='extract', size='size_col', symbol='symbol', color_discrete_map={'User query': 'black'}, width=1000, height=700, ) fig.update_traces( marker=dict(opacity=1, line=dict(width=0, color='DarkSlateGrey')), selector=dict(mode='markers'), ) fig.update_layout( legend_title_text='<b>Chunk source</b>', title='<b>2D Projection of Chunk Embeddings via PaCMAP</b>', ) fig.show() "
            },
            {
                "page_number": 11,
                "text": "➡️On the graph above, you can see a spatial representation of the knowledge base documents. As the vector embeddings represent the document’s meaning, their closeness in meaning should be reflected in their embedding’s closeness. The user query’s embedding is also shown: we want to find the k documents that have the closest meaning, thus we pick the kclosest vectors. In the LangChain vector database implementation, this search operation is performed by the method vector_database.similarity_search(query). Here is the result: print(f'Starting retrieval for {user_query=}...') retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=5) print('==================================Top document==================================') print(retrieved_docs[0].page_content) print('==================================Metadata============================ ======') print(retrieved_docs[0].metadata) "
            },
            {
                "page_number": 12,
                "text": "Starting retrieval for user_query='How to create a pipeline object?'... ==================================Top document================================== ``` ## Available Pipelines: ==================================Metadata================================== &#123;'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/deepfloyd_if.md ', 'start_index': 16887} 2. Reader - LLM 💬 In this part, the LLM Reader reads the retrieved context to formulate its answer. There are substeps that can all be tuned: 1. The content of the retrieved documents is aggregated together into the “context”, with many processing options like prompt compression. 2. The context and the user query are aggregated into a prompt and then given to the LLM to generate its answer. 2.1. Reader model The choice of a reader model is important in a few aspects:  the reader model’s max_seq_length must accommodate our prompt, which includes the context output by the retriever call: the context consists of 5 documents of 512 tokens each, so we aim for a context length of 4k tokens at least.  the reader model For this example, we chose HuggingFaceH4/zephyr-7b-beta, a small but powerful model. With many models being released every week, you may want to substitute this model to the latest and greatest. The best way to keep track of open source LLMs is to check the Open-source LLM leaderboard. "
            },
            {
                "page_number": 13,
                "text": "To make inference faster, we will load the quantized version of the model: from transformers import pipeline import torch from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig READER_MODEL_NAME = 'HuggingFaceH4/zephyr-7b-beta' bnb_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type='nf4', bnb_4bit_compute_dtype=torch.bfloat16, ) model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, quantization_config=bnb_config) tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME) READER_LLM = pipeline( model=model, tokenizer=tokenizer, task='text-generation', do_sample=True, temperature=0.2, repetition_penalty=1.1, return_full_text=False, max_new_tokens=500, ) READER_LLM('What is 4+4? Answer:') 2.2. Prompt The RAG prompt template below is what we will feed to the Reader LLM: it is important to have it formatted in the Reader LLM’s chat template. We give it our context and the user’s question. prompt_in_chat_format = [ { 'role': 'system', 'content': '''Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide the number of the source document when relevant. If the answer cannot be deduced from the context, do not give an answer.''', }, { 'role': 'user', "
            },
            {
                "page_number": 14,
                "text": "'content': '''Context: {context} --- Now here is the question you need to answer. Question: {question}''', }, ] RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template( prompt_in_chat_format, tokenize=False, add_generation_prompt=True ) print(RAG_PROMPT_TEMPLATE) <|system|> Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide the number of the source document when relevant. If the answer cannot be deduced from the context, do not give an answer. <|user|> Context: &#123;context} --- Now here is the question you need to answer. Question: &#123;question} <|assistant|> Let’s test our Reader on our previously retrieved documents! retrieved_docs_text = [doc.page_content for doc in retrieved_docs] # We only need the text of the documents context = 'Extracted documents:' context += ''.join([f'Document {str(i)}:::' + doc for i, doc in enumerate(retrieved_docs_text)]) final_prompt = RAG_PROMPT_TEMPLATE.format(question='How to create a pipeline object?', context=context) # Redact an answer answer = READER_LLM(final_prompt)[0]['generated_text'] print(answer) To create a pipeline object, follow these steps: 1. Define the inputs and outputs of your pipeline. These could be strings, dictionaries, or any other format that best suits your use case. "
            },
            {
                "page_number": 15,
                "text": "2. Inherit the `Pipeline` class from the `transformers` module and implement the following methods: - `preprocess`: This method takes the raw inputs and returns a preprocessed dictionary that can be passed to the model. - `_forward`: This method performs the actual inference using the model and returns the output tensor. - `postprocess`: This method takes the output tensor and returns the final output in the desired format. - `_sanitize_parameters`: This method is used to sanitize the input parameters before passing them to the model. 3. Load the necessary components, such as the model and scheduler, into the pipeline object. 4. Instantiate the pipeline object and return it. Here's an example implementation based on the given context: ```python from transformers import Pipeline import torch from diffusers import StableDiffusionPipeline class MyPipeline(Pipeline): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) self.pipe = StableDiffusionPipeline.from_pretrained('my_model') def preprocess(self, inputs): # Preprocess the inputs as needed return &#123;'input_ids':...} def _forward(self, inputs): # Run the forward pass of the model return self.pipe(**inputs).images[0] def postprocess(self, outputs): # Postprocess the outputs as needed return outputs['sample'] def _sanitize_parameters(self, params): # Sanitize the input parameters return params my_pipeline = MyPipeline() result = my_pipeline('My input string') print(result) ``` "
            },
            {
                "page_number": 16,
                "text": "Note that this implementation assumes that the model and scheduler are already loaded into memory. If they need to be loaded dynamically, you can modify the `__init__` method accordingly. 2.3. Reranking A good option for RAG is to retrieve more documents than you want in the end, then rerank the results with a more powerful retrieval model before keeping only the top_k. For this, Colbertv2 is a great choice: instead of a bi-encoder like our classical embedding models, it is a cross-encoder that computes more fine-grained interactions between the query tokens and each document’s tokens. It is easily usable thanks to the RAGatouille library. from ragatouille import RAGPretrainedModel RERANKER = RAGPretrainedModel.from_pretrained('colbert-ir/colbertv2.0') 3. Assembling it all! from transformers import Pipeline def answer_with_rag( question: str, llm: Pipeline, knowledge_index: FAISS, reranker: Optional[RAGPretrainedModel] = None, num_retrieved_docs: int = 30, num_docs_final: int = 5, ) -> Tuple[str, List[LangchainDocument]]: # Gather documents with retriever print('=> Retrieving documents...') relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs) relevant_docs = [doc.page_content for doc in relevant_docs] # Keep only the text # Optionally rerank results if reranker: print('=> Reranking documents...') relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final) relevant_docs = [doc['content'] for doc in relevant_docs] "
            },
            {
                "page_number": 17,
                "text": "relevant_docs = relevant_docs[:num_docs_final] # Build the final prompt context = 'Extracted documents:' context += ''.join([f'Document {str(i)}:::' + doc for i, doc in enumerate(relevant_docs)]) final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context) # Redact an answer print('=> Generating answer...') answer = llm(final_prompt)[0]['generated_text'] return answer, relevant_docs Let’s see how our RAG pipeline answers a user query. question = 'how to create a pipeline object?' answer, relevant_docs = answer_with_rag(question, READER_LLM, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER) => Retrieving documents... print('==================================Answer============================== ====') print(f'{answer}') print('==================================Source docs==================================') for i, doc in enumerate(relevant_docs): print(f'Document {i}------------------------------------------------------------') print(doc) ==================================Answer================================== To create a pipeline object, follow these steps: 1. Import the `pipeline` function from the `transformers` module: ```python from transformers import pipeline ``` 2. Choose the task you want to perform, such as object detection, sentiment analysis, or image generation, and pass it as an argument to the `pipeline` function: - For object detection: "
            },
            {
                "page_number": 18,
                "text": "```python >>> object_detector = pipeline('object-detection') >>> object_detector(image) [&#123;'score': 0.9982201457023621, 'label':'remote', 'box': &#123;'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}}, ...] ``` - For sentiment analysis: ```python >>> classifier = pipeline('sentiment-analysis') >>> classifier('This is a great product!') &#123;'labels': ['POSITIVE'],'scores': tensor([0.9999], device='cpu', dtype=torch.float32)} ``` - For image generation: ```python >>> image = pipeline( ... 'stained glass of darth vader, backlight, centered composition, masterpiece, photorealistic, 8k' ... ).images[0] >>> image PILImage mode RGB size 7680x4320 at 0 DPI ``` Note that the exact syntax may vary depending on the specific pipeline being used. Refer to the documentation for more details on how to use each pipeline. In general, the process involves importing the necessary modules, selecting the desired pipeline task, and passing it to the `pipeline` function along with any required arguments. The resulting pipeline object can then be used to perform the selected task on input data. ==================================Source docs================================== Document 0------------------------------------------------------------ # Allocate a pipeline for object detection >>> object_detector = pipeline('object-detection') >>> object_detector(image) [&#123;'score': 0.9982201457023621, 'label': 'remote', 'box': &#123;'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}}, &#123;'score': 0.9960021376609802, 'label': 'remote', 'box': &#123;'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}}, &#123;'score': 0.9954745173454285, 'label': 'couch', 'box': &#123;'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}}, &#123;'score': 0.9988006353378296, 'label': 'cat', 'box': &#123;'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}}, "
            },
            {
                "page_number": 19,
                "text": "&#123;'score': 0.9986783862113953, 'label': 'cat', 'box': &#123;'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}] Document 1------------------------------------------------------------ # Allocate a pipeline for object detection >>> object_detector = pipeline('object_detection') >>> object_detector(image) [&#123;'score': 0.9982201457023621, 'label': 'remote', 'box': &#123;'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}}, &#123;'score': 0.9960021376609802, 'label': 'remote', 'box': &#123;'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}}, &#123;'score': 0.9954745173454285, 'label': 'couch', 'box': &#123;'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}}, &#123;'score': 0.9988006353378296, 'label': 'cat', 'box': &#123;'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}}, &#123;'score': 0.9986783862113953, 'label': 'cat', 'box': &#123;'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}] Document 2------------------------------------------------------------ Start by creating an instance of [`pipeline`] and specifying a task you want to use it for. In this guide, you'll use the [`pipeline`] for sentiment analysis as an example: ```py >>> from transformers import pipeline >>> classifier = pipeline('sentiment-analysis') Document 3------------------------------------------------------------ ``` ## Add the pipeline to 🤗Transformers If you want to contribute your pipeline to 🤗Transformers, you will need to add a new module in the `pipelines` submodule with the code of your pipeline, then add it to the list of tasks defined in `pipelines/__init__.py`. Then you will need to add tests. Create a new file `tests/test_pipelines_MY_PIPELINE.py` with examples of the other tests. The `run_pipeline_test` function will be very generic and run on small random models on every possible architecture as defined by `model_mapping` and `tf_model_mapping`. This is very important to test future compatibility, meaning if someone adds a new model for `XXXForQuestionAnswering` then the pipeline test will attempt to run on it. Because the models are random it's impossible to check for actual values, that's why there is a helper `ANY` that will simply attempt to match the output of the pipeline TYPE. "
            },
            {
                "page_number": 20,
                "text": "You also *need* to implement 2 (ideally 4) tests. - `test_small_model_pt` : Define 1 small model for this pipeline (doesn't matter if the results don't make sense) and test the pipeline outputs. The results should be the same as `test_small_model_tf`. - `test_small_model_tf` : Define 1 small model for this pipeline (doesn't matter if the results don't make sense) and test the pipeline outputs. The results should be the same as `test_small_model_pt`. - `test_large_model_pt` (`optional`): Tests the pipeline on a real pipeline where the results are supposed to make sense. These tests are slow and should be marked as such. Here the goal is to showcase the pipeline and to make sure there is no drift in future releases. - `test_large_model_tf` (`optional`): Tests the pipeline on a real pipeline where the results are supposed to make sense. These tests are slow and should be marked as such. Here the goal is to showcase the pipeline and to make sure there is no drift in future releases. Document 4------------------------------------------------------------ ``` 2. Pass a prompt to the pipeline to generate an image: ```py image = pipeline( 'stained glass of darth vader, backlight, centered composition, masterpiece, photorealistic, 8k' ).images[0] image ✅We now have a fully functional, performant RAG system. That’s it for today! Congratulations for making it to the end 🥳 To go further 🗺️ This is not the end of the journey! You can try many steps to improve your RAG system. We recommend doing so in an iterative way: bring small changes to the system and see what improves performance. Setting up an evaluation pipeline  💬“You cannot improve the model performance that you do not measure”, said Gandhi… or at least Llama2 told me he said it. Anyway, you should absolutely start by measuring performance: this means building a small "
            },
            {
                "page_number": 21,
                "text": "evaluation dataset, and then monitor the performance of your RAG system on this evaluation dataset. Improving the retriever 🛠️You can use these options to tune the results:  Tune the chunking method:  Size of the chunks  Method: split on different separators, use semantic chunking…  Change the embedding model 👷‍♀️More could be considered:  Try another chunking method, like semantic chunking  Change the index used (here, FAISS)  Query expansion: reformulate the user query in slightly different ways to retrieve more documents. Improving the reader 🛠️Here you can try the following options to improve results:  Tune the prompt  Switch reranking on/off  Choose a more powerful reader model 💡Many options could be considered here to further improve the results:  Compress the retrieved context to keep only the most relevant parts to answer the query.  Extend the RAG system to make it more user-friendly:  cite source  make conversational "
            }
        ],
        "images": [
            "Image_9",
            "Image_49",
            "Image_66"
        ]
    },
    {
        "file_name": "ToolsandFrameworks2.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "The Top 16 AI Frameworks and Libraries: A Beginner's Guide Explore the best AI frameworks and libraries and their basics in this ultimate guide for junior data practitioners starting their professional careers. This article is a valued contribution from our community and has been edited for clarity and accuracy by DataCamp. Interested in sharing your own expertise? We’d love to hear from you! Feel free to submit your articles or ideas through our Community Contribution Form. Artificial Intelligence has become essential for software development in today’s rapidly evolving tech landscape. It enables applications to perform tasks that were once considered the exclusive domain of humans, such as understanding and generating human language, recognizing patterns in data, and making intelligent decisions. From chatbots and recommendation systems to image recognition and natural language processing, AI is transforming how we build software. As a junior data practitioner with a solid technical foundation, you might wonder how to harness AI’s power effectively. That's why this guide will explore why and how to choose AI frameworks, what frameworks and libraries you should explore, and many other essentials. Why Choose an AI Framework? These days, developers rely on corresponding frameworks and libraries more and more to save money, time, and resources while creating AI-driven software. Let’s consider what AI frameworks and libraries are — a collection of pre-built tools and resources that simplify the process of building applications on the basis of Artificial Intelligence. These frameworks provide a foundation for implementing machine learning and deep learning algorithms, making it easier to develop intelligent software. Now, let’s discover more about why AI frameworks are used for boosting software development flow. They are cost-effective for IT companies AI frameworks provide businesses with a cost-effective way to develop custom software applications. By eliminating the need for manual coding and allowing developers to use pre-built components, frameworks can significantly help businesses reduce their development costs. Using frameworks also allows businesses to quickly build sophisticated applications tailored to their specific needs. By leveraging AI and ML power, companies can develop more efficient software solutions than traditional software development methods. They facilitate app development flow When selecting frameworks, it's important to consider their ability to streamline the development process. These frameworks come with pre-implemented algorithms, data "
            },
            {
                "page_number": 2,
                "text": "handling utilities, and optimization techniques, which enable developers to focus on solving the specific problem at hand rather than getting caught up in the technicalities of AI implementation. They are a time-saving opportunity AI frameworks are a tremendous timesaver for modern software development because they simplify creating, testing, and deploying applications. They also provide a complete development environment with debugging tools, testing harnesses, and data visualization capabilities. This speeds up the development process as developers can act quickly without having to manually compile and debug each section of their code. Additionally, AI frameworks offer a wide variety of pre-built models. These are the most common reasons for implementing AI in business from the very steps of software development for enterprises. Now, let’s consider the key factors when choosing an Artificial Intelligence framework for an IT project. How to Choose an AI Framework That Fits Your Business Needs When choosing an AI framework, it's essential to consider several key factors to ensure it aligns with your project's requirements and your level of expertise. Performance Performance should be a top priority when looking at frameworks. Opting for a framework that can handle data efficiently and provides rapid training and inference times is recommended. The performance of different frameworks can be assessed by evaluating benchmarks and real-world use cases. Community support An active and engaged community is an absolute necessity for the development of AI frameworks. It provides unlimited access to an extensive array of resources, tutorials, and community-driven plugins, along with unwavering support for implementing ongoing enhancements and updates to the framework. Flexibility When working on AI projects, flexibility is of utmost importance. The best AI framework that provides the ability to experiment with various algorithms is essential to ensure success. Additionally, the framework should be able to adapt to different data types, such as text, images, and audio, and integrate with other technologies without any issues. Thus, you can ensure that your AI projects are efficient, accurate, and effective. Ease of learning "
            },
            {
                "page_number": 3,
                "text": "It's important to choose a framework that matches your current skill level when starting out as a junior data practitioner. Pay attention to the frameworks that are more beginner-friendly, offering detailed documentation and tutorials. Moreover, consider your learning style and the available resources to aid you in getting started. When selecting an AI framework, it is crucial to consider various factors, including performance, community support, flexibility, and ease of learning. By doing so, you can ensure that the framework meets the requirements of your project and is compatible with your skill level. Considering these factors will help you find the ideal framework for your data projects. Open-Source vs. Commercial AI Frameworks: Benefits and Drawbacks When it comes to choosing the best AI framework for your development project, you have two main options: open-source and commercial frameworks. Each option has its own set of advantages and disadvantages. Understanding the differences between them before making a decision is vital. Open-source AI frameworks Open-source frameworks are those released under an open-source license, which grants users the opportunity to operate with the software for any purpose. Why are open-source frameworks beneficial? They're typically free to use, making them budget-friendly for small projects and startups. They often have a strong and active community, which can be used as a valuable resource for learning and troubleshooting. You can inspect the source code of open-source frameworks, giving you greater control over your AI implementations. What are the drawbacks of the open-source AI frameworks? Limited support. While community support is helpful, it may not be as responsive or comprehensive as commercial support. Complexity. Some open-source frameworks can be complex and challenging for beginners to grasp fully. Commercial AI frameworks "
            },
            {
                "page_number": 4,
                "text": "Commercial frameworks are developed by companies that release their software under proprietary licenses. This means that users of these frameworks are limited in what they can do with the software and also may be subject to additional fees. However, users of commercial frameworks may benefit from extra features and support from the vendor. The benefits of commercial AI frameworks Commercial frameworks typically come with dedicated support teams, ensuring prompt assistance when issues arise. They often focus on user-friendliness, making them more accessible to developers of all skill levels. Moreover, you may find advanced features and optimizations in commercial frameworks that cater to specific use cases. When figuring out How to learn AI, you need to consider both the positives and negatives of different frameworks. The drawbacks of commercial AI frameworks They can be expensive, which may be prohibitive for small or bootstrapped projects. Using a commercial framework may tie you to a specific vendor and limit your flexibility. Which to choose? The response relies on your particular project needs and requirements. Moreover, when selecting a framework, you should take into account financial resources, personal expertise, and other factors. The Top AI Frameworks and Libraries Software is an important component of streamlining business operations through AI frameworks and libraries. By using software, businesses can automate tasks, reduce manual labor, improve accuracy, save time and money, create insights from data, and more. Popular AI frameworks such as TensorFlow and PyTorch are used for developing machine learning models. These frameworks provide a comprehensive set of tools that enable developers to easily create and deploy ML models. Other useful AI libraries include Scikit-Learn, Keras, and Caffe. These libraries provide a set of APIs that enable developers to quickly develop applications without needing to write an entire code base from scratch. PyTorch Torch is an open-source machine learning library known for its dynamic computational graph and is favored by researchers. The framework is excellent for prototyping and "
            },
            {
                "page_number": 5,
                "text": "experimentation. Moreover, it's empowered by growing community support, with tools like PyTorch being built on the library. PyTorch has swiftly become one of the most widely used frameworks out there, useful in all kinds of applications. Scikit-Learn Scikit-Learn is a Python library for machine learning. It is an open-source and beginner-friendly tool that offers data mining and machine learning capabilities, as well as comprehensive documentation and tutorials. Scikit-Learn is well-suited for smaller projects and quick model prototyping but may not be the best choice for deep learning tasks. TensorFlow TensorFlow is an open-source deep learning framework developed by Google. It's renowned for its flexibility and scalability, making it suitable for many AI applications. This framework has a large and active community and is equipped with extensive documentation and tutorials. It also supports deployment on various platforms. However, the learning curve of TensorFlow can be steep for beginners. Keras Keras is an open-source high-level neural networks API that runs on top of TensorFlow or other frameworks. It is user-friendly and easy to learn, simplifying the process of operating with deep learning models. Moreover, it's ideal for quick prototyping. You should only keep in mind that Keras may lack some advanced features for complex tasks. LangChain LangChain has recently gained popularity as a framework for large language model (LLM) applications. It allows developers to build applications using LLMs with features like model I/O, data connections, chains, memory, agents, and callbacks. LangChain integrates with various tools, including OpenAI and Hugging Face Transformers, and is used for diverse applications like chatbots, document summarization, and interacting with APIs. Hugging Face Hugging Face specializes in easy-to-use AI tools, mainly known for their 'Transformers' library, which helps in advanced machine learning tasks like language processing and creating chatbots. They also provide tools for generating images and sounds, efficient ways to handle data in AI models, and simple methods to update large AI models. Additionally, they offer web-friendly versions of these tools, making it easier for beginners and experts alike to experiment with AI in various fields, including natural language processing and computer vision. OpenNN "
            },
            {
                "page_number": 6,
                "text": "OpenNN is a tool used for creating neural networks, a type of AI that mimics how the human brain works. It's written in C++ and is known for being fast and efficient. OpenNN is used mainly for research and creating AI that can learn and make decisions based on data. OpenAI OpenAI provides a range of tools for different AI tasks, including making images or converting text to speech. It's known for its powerful GPT language models that can understand and generate human-like text. OpenAI's platform is user-friendly, making it easier for people to use advanced AI in their own projects, especially for creating AI assistants or tools that interact with users in natural language. It's worth noting that several of the features required a paid premium subscription. PyBrain PyBrain is an open-source ML library for Python. It provides a simple and flexible environment for experimenting with various machine learning algorithms and is perfect for researchers, educators, and developers looking for a lightweight Python-based framework for exploring machine learning concepts. It is lightweight and easy to use for experimentation, supporting a wide range of machine learning algorithms. Moreover, PyBrain’s AI library is good for educational purposes and rapid prototyping. However, you should take into account that PyBrain has limited documentation and a smaller community compared to mainstream libraries. It may also lack some advanced features found in other frameworks. IBM Watson IBM Watson is a suite of AI and machine learning services provided by IBM. It offers tools and solutions for building and deploying AI-powered applications, including natural language processing, computer vision, and predictive analytics. It can be easily integrated with IBM Cloud for seamless deployment. What is more, robust AI capabilities in the IBM Watson suite are backed by IBM's expertise. However, the pricing may be a concern for smaller businesses seeking comprehensive AI solutions and consulting services. Microsoft Cognitive Toolkit (CNTK) The Microsoft Cognitive Toolkit, or CNTK, is a free and open-source deep learning AI framework developed by Microsoft. It's known for its efficiency, especially on multi-GPU systems, and is suitable for both research and production deployments. It is preferred by many researchers, data scientists, and developers working on deep learning projects with access to powerful hardware because it's highly efficient, particularly for training large models. It also supports multiple neural network types, including feedforward and recurrent networks; additionally, it provides a Python API for ease of use. "
            },
            {
                "page_number": 7,
                "text": "But you should be aware that Microsoft CNTK may possess a steeper learning curve compared to more beginner-friendly frameworks. DL4J (Deeplearning4j) Deeplearning4j, often abbreviated as DL4J, implies an open-source deep learning framework specifically designed for Java and Scala developers. It provides a comprehensive set of tools for building and deploying deep neural networks in Java-based applications. DL4J is designed for Java and Scala, making it suitable for enterprise-level applications. The framework also offers support for distributed computing, enabling scalability. The platform includes a wide range of neural network types and pre-processing tools. However, it has a smaller community compared to Python-based frameworks. More AI frameworks and libraries to consider Theano Theano is an open-source numerical computation AI library for Python. While it's no longer actively developed, it played a significant role in the early days of deep learning. Why so? For starters, it had an efficient symbolic mathematics library. Theano was also suitable for educational purposes. Although some existing projects may still use it, it's no longer actively maintained or updated. MXNet MXNet is an open-source deep learning framework known for its efficiency and scalability. Additionally, MXNet is efficient for both research and production. It has growing community and industry support, but its community is smaller compared to TensorFlow and PyTorch. "
            },
            {
                "page_number": 8,
                "text": "Caffe Caffe is an open-source deep learning framework. It's known for its speed and efficiency in computer vision tasks, supporting a variety of deep learning architectures. Caffe is optimized for computer vision applications and excellent for deploying on edge devices. But when choosing it, you should consider its limited flexibility for non-vision tasks. XGBoost This is an open-source gradient boosting framework known for its efficiency and performance. Data practitioners working with structured data and classification/regression problems often choose it. This AI framework excels in structured data tasks and is widely used in data science competitions. XGBoost is known for its exceptional performance for tabular data. The framework supports various programming languages being well-maintained and actively developed. However, you should understand that XGBoost is not designed for deep learning tasks. Conclusion We’ve covered some of the most popular AI frameworks, libraries, and other tools for you to consider. You can also check out our list of the Top 5 AI tools for data science to know more about how to boost the productivity of your data processing workflows. As a junior data practitioner, choosing one of the right AI frameworks or libraries is crucial for your professional growth and project success. While there's no one-size-fits-all solution, consider your project's requirements, your familiarity with the framework, and the resources available in the community. In summary, PyTorch and TensorFlow are excellent choices for deep learning projects, with TensorFlow offering scalability and PyTorch emphasizing flexibility. Scikit-Learn is a go-to for traditional machine learning tasks, while Keras provides a user-friendly entry point to deep learning. XGBoost excels in structured data problems, Caffe is a top choice for computer vision, and MXNet offers efficiency and scalability. Remember that Theano, while no longer actively developed, may still find its use in certain educational contexts. More recent tools such as LangChain and OpenAI give excellent options for LLMs, while Hugging Face is useful for natural language processing in general. Ultimately, the best AI framework or library is the one that aligns with your specific needs and helps you achieve your goals as a data practitioner. So, roll up your sleeves, dive into the world of AI, and start building smarter software with the right tools. "
            }
        ],
        "images": [
            "Image_34"
        ]
    },
    {
        "file_name": "ToolsandFrameworks3.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Top AI Frameworks in 2024: Comparison of Artificial Intelligence Frameworks Key takeaways  An AI framework is a set of code components that allow engineers to architect, build, and deploy AI functionality in software products.  TensorFlow, PyTorch, Keras, Scikit-Learn, and spaCy are the most popular AI frameworks.  LangChain, LlamaIndex, and Haystack allow developers to use the capabilities of large language models in software products.  Enterprises such as Hugging Face, IBM, and Amazon provide subscription-based access to off-the-shelf AI toolsets, while OpenAI provides APIs to access its famous GPT-based tools.  To choose the right solution for your business, you should clearly understand why you need to implement AI in your product, and you should have a powerful engineering team to make it happen. The future that visionaries wrote about is here. We can enjoy recommendation engines, automate manual operations, generate content, and get incredible results from simple prompts thanks to the use of artificial intelligence (AI) and its subfields — deep learning (DL) and machine learning (ML). In 2022, we were astonished by ChatGPT, a product by OpenAI that demonstrates sky-high performance and precision when responding to users’ queries. While ChatGPT was the first tool of its kind with this degree of capabilities, today we have various tools based on large language models (LLMs). They are extending the borders of the digital world, offering software products that are more powerful, useful, and creative than ever. What’s even more important these days is that you can connect your software with any AI tool, boost its efficiency, improve your whole business, meet more customer needs, and achieve business goals. In this article, we consider major types of artificial intelligence frameworks, highlight the most popular, and help you choose the right AI framework for your particular use case. "
            },
            {
                "page_number": 2,
                "text": "What is an AI framework? Say you need a powerful neural network to improve the recommendation engine on your real estate platform. Does this mean you need to build and train a neural network from scratch, then develop new software for your users based on it? Not necessarily. You can use AI frameworks to strengthen your product with AI functionality. “ AI frameworks provide data scientists, AI developers, and researchers with the code components to architect, train, validate, and deploy intelligent functionality through a programming interface. AI frameworks, which include deep learning and machine learning frameworks, are sets of components that help your development team integrate smart algorithms into your product. They simplify AI product development and speed up the product launch. Why would you want to use an AI framework? "
            },
            {
                "page_number": 3,
                "text": "Developers use AI frameworks for three main reasons:  Ready-made infrastructure. AI frameworks provide the instruments developers need to build an AI product. No need to build them from scratch.  Code standardization. While working on the same project, developers may generate different ideas on how to solve similar tasks. Frameworks have requirements and standards that allow you to implement a unified approach to coding and decision-making and improve code quality.  Resource optimization. Frameworks help to save developers’ time and your budget. Popular AI frameworks have a powerful community of developers behind them. When you start using an AI framework for your product, you get access to community expertise and an opportunity to use development best practices for your application. Common features of the most powerful AI frameworks Before we dive into key differences between major AI frameworks, let’s have a quick look at features they share:  Simplicity. AI frameworks provide a simple interface for working with complex mathematical operations and algorithms. This permits developers to work with AI models without needing to build them from scratch.  Efficiency. Most AI frameworks are optimized for high-performance computing and can use GPUs (graphics processing units) and TPUs (tensor processing units) to accelerate training and inference.  Flexibility and customization. AI frameworks are flexible, allowing you to customize and experiment with various characteristics of your AI model, including neural network architecture, loss functions (that describe the difference between predicted and real values in AI models), and optimization algorithms. "
            },
            {
                "page_number": 4,
                "text": " Compatibility. Many AI frameworks are compatible with popular programming languages like Python and JavaScript, making it easy to integrate AI models into existing software systems.  Scalability. AI frameworks can handle both small-scale and large-scale tasks; thus, along with, for example, SaaS app development, they are suitable for academic research.  Model zoos. Many frameworks have model repositories, often called model zoos, where you can find pretrained models for various tasks. These can save a significant amount of time and computational resources.  Interoperability. Some frameworks can work together, allowing you to combine the strengths of multiple frameworks for different aspects of your project.  Ecosystem. Popular AI frameworks have large and active communities of developers, researchers, and users. This means that you can find extensive documentation, tutorials, and pre-trained models, making it easier to get started and troubleshoot issues. Now, let’s find out what types of frameworks you can consider for your project, how they differ, and how to make the right choice. Major types of frameworks you can use in AI product development As you research AI frameworks, you may find open-source and commercial solutions, frameworks based on deep learning and machine learning algorithms, training and inference frameworks, and many others. The classifications of AI frameworks are many and complex. In our article, we show you two major types of frameworks: "
            },
            {
                "page_number": 5,
                "text": " AI frameworks are widely used for developing machine learning and deep learning models.  LLM orchestration frameworks are used for development and training of large language models. We’ve also decided to consider subscription-based enterprise AI toolkits, which may be confused with AI frameworks. With the help of our article, you can enrich your knowledge of cutting-edge frameworks, tap into a source of their similarities and differences, find out about tools developed by Amazon, Meta, and IBM, and decide on the right technologies for your project. AI frameworks Among the abundance of open-source AI frameworks, we’ve picked the five most widely used: TensorFlow, PyTorch, Keras, scikit-learn, and spaCy. TensorFlow TensorFlow, a widely used open-source machine learning library, is a free tool coded in Python. It is purpose-built for training and prediction tasks with neural networks. TensorFlow is based on deep learning inference, which involves pretrained deep neural network models to make predictions on new data. TensorFlow empowers developers to create expansive neural networks with multiple layers using data flow graphs. Its superpower is flexibility: the framework enables easy deployment across a variety of platforms (CPUs, GPUs, TPUs) with minimal or no code changes. TensorFlow processes data in the form of multidimensional arrays, housing vectors, and matrices known as tensors. What products benefit from TensorFlow? Google released the initial version of TensorFlow in 2015 with the goal of bringing machine learning to everyone. Almost nine years later, TensorFlow may be handy for a variety of products and industries. It solves challenges related to the development of AI healthcare solutions, social "
            },
            {
                "page_number": 6,
                "text": "networks, and eCommerce apps. Online marketplaces may also benefit from TensorFlow: for example, Airbnb uses the TensorFlow framework for image classification. Read also:How To Create an AI SaaS Product You may use TensorFlow to:  Build and retrain deep, complex, large-scale neural networks  Develop projects based on natural language processing (NLP)  Create functionality for complex calculations When deciding on TensorFlow for your next project, get ready for a steep learning curve. To make it work for your business, extensive programming and ML knowledge is required. If you are familiar with other Google engineering products, you may notice another common disadvantage: they are too complex, with lots of unnecessary modules and overly complicated components. Another thing worth mentioning is that Google is working on a big AI framework called JAX (“Just After eXecution”), and chances are, the team is investing more effort in JAX, which may lead to TensorFlow’s possible obsolescence in the future. PyTorch On lists of top AI frameworks, TensorFlow and PyTorch go side by side. With an extensive list of tools and libraries, PyTorch is a leader in the AI development league. PyTorch is an open-source machine learning framework developed by Facebook’s AI Research lab. Known for its dynamic computation graph mode in development environments, PyTorch is widely used for academic purposes. PyTorch stands out for data parallelism — scenarios where operations take place concurrently on different elements of the system, distributing computational work across multiple CPU or GPU cores. "
            },
            {
                "page_number": 7,
                "text": "In 2023, Meta released PyTorch 2.0, which they describe as “faster, more Pythonic and as dynamic as ever.” This new version of the framework demonstrates better performance than its predecessor. The development team claims to have lowered the barrier to entry for AI developers and reached better flexibility in dynamic shapes. What products benefit from PyTorch? PyTorch serves the needs of numerous industries, including advertising and marketing, finance, and healthcare. One of the most fascinating use cases is by Tesla, which uses PyTorch for self-driving capabilities. This framework is also great for technology and academic projects. PyTorch is the tool that enables AI on mobile devices — for example, Instagram uses it to create engaging experiences and interactive filters. Stanford University uses PyTorch capabilities to explore new algorithmic methods. You may use PyTorch to:  Research and experiment with prototypes  Develop systems that require data parallelism  Create NLP-based apps PyTorch is a powerful Python-based AI framework suitable for research tasks. It allows you to view changes in code on the go, yet it has some disadvantages. Compared to TensorFlow, PyTorch is praised for its ease of use. However, the community and ecosystem are slightly smaller, and the framework may be less convenient to deploy models in production environments. Still, TensorFlow and PyTorch share multiple features, and both may fit your project’s needs. Keras Keras is an open-source machine learning framework that offers an intuitive interface for building and training deep learning models with a focus on simplicity and consistency. It requires a powerful AI back end and creates a powerful duo with TensorFlow. When it comes to the most developer-oriented API, Keras may be an absolute leader. The official website claims that Keras was built for human beings, not "
            },
            {
                "page_number": 8,
                "text": "machines; it minimizes the number of developers' actions and provides good documentation and clear error messages. Keras streamlines AI model development by offering optimization algorithms, loss functions, and evaluation metrics for seamless integration into the training process. Keras also simplifies and accelerates model training. Compared to PyTorch and TensorFlow, Keras allows developers to build a smaller, more elegant, and more readable codebase that is easier to support and extend. Developers can integrate Keras with TensorFlow or PyTorch, which may increase software efficiency and performance. Keras 3.0 was launched in 2023. The new version provides several critical benefits:  Higher AI model performance compared to the previous version  Model and data parallelism – a feature that allows for splitting models and data across multiple GPUs  Ability to use data from any source What products benefit from Keras? Keras is used for a variety of tasks, including image classification and segmentation, object detection, NLP, and data generation. For example, Revy — a shared micromobility company — uses Keras to forecast the customer conversion rate. You may use Keras to:  Strengthen your existing TensorFlow-based model  Experiment with different model architectures and hyperparameters In a nutshell, Keras is more like an interface that should be used on top of a powerful AI back end. You may notice Keras’s limitations the moment you start looking for use cases. Compared to PyTorch and TensorFlow, fewer projects use Keras. The community is smaller, too. The framework demonstrates limited flexibility for complex models, provides less control over the training process, and may be less suitable for research "
            },
            {
                "page_number": 9,
                "text": "purposes. Besides, if you use any framework other than TensorFlow, it may be challenging for you to integrate it into your Keras project. At the same time, Keras relies heavily on a backend framework, so limitations of the back end impact Keras’s performance as well. Scikit-learn Back in 2018, scikit-learn was used by almost 40% of GitHub projects. These days, it’s still quite popular and keeps extending its capabilities. Scikit-learn is a Python-based ML library with a focus on predictive data analysis. Some core algorithms behind this library are written in Cython – a type of Python that can reach levels of code performance comparable to C. Thus, the performance of scikit-learn is high. Scikit-learn uses three key libraries:  NumPy for working with data arrays and mathematical algorithms  SciPy for scientific computing  Matplotlib for data visualization Scikit-learn provides developers with numerous APIs for data clustering, probability calculations, feature selection and extraction, and more. What products benefit from scikit-learn? As with most ML frameworks, scikit-learn is used to solve tasks related to image classification, sentiment analysis, recommendation system development, and so on. Spotify uses this library for music recommendations, and according to Erik Bernhardsson, Engineering Manager at Spotify, it’s the most well-designed ML package so far. You may use scikit-learn to:  Start your experience with AI project development  Build fraud detection functionality  Implement predictive analytics in your product "
            },
            {
                "page_number": 10,
                "text": "Parallel processing operations with scikit-learn are not as efficient as with other top AI frameworks. It’s not suited for complex tasks and heavy calculations. spaCy “I think small companies are terrible at natural language processing,” said Matthew Honnibal, founder of Explosion AI, in 2015 when introducing spaCy. More than eight years ago, Matthew focused on building a simple NLP framework for founders of small and medium companies, and to this day, spaCy serves the needs of businesses worldwide. In 2021, Explosion AI rolled out spaCy 3.0. Based on the transformer architecture and trained on 73 languages, spaCy demonstrates high performance and efficiency – the tool is perfect for processing large volumes of text data. What products benefit from spaCy? Any project that has to do with NLP and text analysis may benefit from this tool. On their blog, Explosion shares the use case of a reviews analysis system based on spaCy. You may use spaCy to:  Get started with NLP  Try different model architectures According to the Explosion development team, spaCy may not be the right choice for research purposes, and it’s not designed for chatbots. Talking about spaCy, we’ve come closer to the concept of a transformer architecture: a DL encoder–decoder architecture based on the attention mechanism. This architecture is the basis of ChatGPT, LLaMA, Mistral, Claude, and other large language models. Inspired by amazing use cases and the human-like intelligence of LLMs, you may want to connect one to your software product. Let’s see what LLM orchestration frameworks you can use to make your whole business more intelligent. "
            },
            {
                "page_number": 11,
                "text": "LLM orchestration frameworks With LLM integration, you can strengthen your product with numerous capabilities like data analysis and content generation, usage pattern identification, image recognition, and prediction analysis. This is why we decided to add this section to our article and increase your awareness of LLM orchestration frameworks that can help you train a language model on your data from scratch Read also:How to Implement Large Language Models in Your Business Orchestration frameworks simplify the development and deployment of applications built around LLMs, enhancing their performance and reliability. LangChain With LangChain, you can craft autonomous agents capable of intelligent decision-making, develop personal assistants that understand and respond to user queries, create chatbots for interactive communication, and implement robust systems that understand code. LangChain offers an open-source library with prebuilt components and chains your development team may use to train your system on relevant data. LangChain has one of the biggest contributor communities on GitHub and includes integrations with major cloud providers: Amazon, Google, and Azure. What products benefit from LangChain? Instacart, Zapier, Dropbox, and numerous other leading companies use LangChain in their AI-driven products. For example, Adyen, an eCommerce company, uses LangChain capabilities for boosting support team efficiency. SaaS startups benefit with this framework, too. You may use LangChain to:  Develop intelligent decision-making agents  Create user-responsive virtual assistants and chatbots "
            },
            {
                "page_number": 12,
                "text": " Enhance code analysis and development processes  Improve your applications with advanced language model capabilities  Tap into generative AI capabilities What about the drawbacks? According to Max Woolf, Data Scientist at Buzzfeed, “the problem with LangChain is that it makes simple things relatively complex, and with that unnecessary complexity creates a tribalism which hurts the up-and-coming AI ecosystem as a whole.” LlamaIndex LlamaIndex is an open-source orchestration framework tailored for connecting an LLM to original data sources. It shares common features with LangChain, including a distributed architecture and robust LLM management capabilities. LlamaIndex is a versatile tool that connects various data sources, such as documents, databases, and websites, making them accessible and searchable by LLMs. There are two key features of LlamaIndex:  Distributed querying: LlamaIndex excels in distributed querying of LLMs, optimizing query processing across a software system.  Efficient indexing: LlamaIndex offers the ability to index LLMs, enhancing their search efficiency. What products benefit from LlamaIndex? This framework may help to create knowledgeable agents — AI chatbots trained on your corporate data, both structured and unstructured. You may use LlamaIndex to:  Get answers from unstructured data sources like PDFs and web pages, legal documents, and research papers  Create chatbots that tap into your knowledge database for personalized responses  Structure data with natural language for insights "
            },
            {
                "page_number": 13,
                "text": "LlamaIndex is good only at tasks related to text processing. Another feature is that the quality of the output depends heavily on the quality of the input, meaning the quality of responses you get with LlamaIndex depends on your embeddings. Haystack Haystack covers many similar tasks: answering questions, searching through documents, extracting data, etc. Haystack is an open-source framework for building and deploying search and question-answering systems powered by large language models. The framework works with multiple platforms (including tools provided by OpenAI and Hugging Face, which we’ll review later), offers integrations with popular vector databases, and provides the Haystack Rest API — an opportunity to deploy your system as an API and access it via web or mobile apps. What products benefit from Haystack? On their website, the Haystack team claims this framework may help users build search systems, data extraction tools, and FAQ software. You may use Haystack to:  Build custom search engines  Create question-answering systems  Extract specific information from text documents  Summarize lengthy content Developers mention that one of Haystack’s disadvantages is the lack of scalability. As you extend your application and your data system grows, Haystack may fail to provide the expected performance and quality. Alternative enterprise toolkits to handle your business tasks "
            },
            {
                "page_number": 14,
                "text": "Open-source frameworks are not your only choice. There are whole packages of solutions with powerful features available right out of the box; just choose a subscription plan and get the functionality you need. Let’s start with the most impressive and affordable one. Hugging Face Hugging Face is an entire AI community, both a platform and a framework. It provides AI models, spaces, and datasets for creating ML applications with great functionality and performance. Hugging Face provides a comprehensive library of tools for working with pretrained NLP models, making it an essential resource for developers and researchers in the AI and NLP fields. The Hugging Face Transformers library is widely used for building, fine-tuning, and deploying transformer-based models, including those for text classification, language generation, and other NLP tasks. HuggingFace provides free access to hosting space, orgs, repos, and open-source tools, and it has paid plans with more advanced features and support. What products benefit from Hugging Face? More than 50,000 organizations use Hugging Face. Grammarly, Microsoft, and Meta are among the most well-known users. You may use Hugging Face to:  Handle text analysis, sentiment analysis, language translation, text generation, and other NLP tasks  Fine-tune models for your unique applications  Develop chatbots and virtual assistants that require natural language understanding and generation  Generate content including articles, reports, and product descriptions  Categorize text data  Develop and deploy entire AI systems "
            },
            {
                "page_number": 15,
                "text": "As you browse through Hugging Face offerings, you may notice the Enterprise Hub — a subscription plan that allows you to access the platform’s features along with complete data security, dedicated customer support, and simple access controls. You can create your private environment on the AI platform, train your models on the provided infrastructure, and deploy your solution to production in just several clicks. Watsonx Watsonx by IBM is an expansive collection of AI and machine learning services. IBM’s long history in the IT field makes Watsonx worth mentioning in top AI frameworks lists. The Watsonx collection includes three major components: a new model development studio, a data store, and an AI toolkit. This suite equips developers and organizations with a diverse set of tools for constructing and launching AI-driven applications, encompassing natural language processing, computer vision, and predictive analytics. One of the advantages of this tool set is its seamless integration with IBM Cloud infrastructure, which enables a straightforward app deployment process. You can access numerous Watsonx features with the free tier, or consider the Standard plan that provides model hosting, prompt tuning, infrastructure management, third-party integrations, and other services. What products benefit from Watsonx? Watsonx is popular among midsize businesses and enterprises. NatWest, Eviden, Samsung SDS, Deloitte, and multiple other famous companies use IBM AI tools. You may use Watsonx to:  Perform medical research  Create enterprise chatbots and virtual assistants  Get insights from your organization’s unstructured data  Detect financial fraud "
            },
            {
                "page_number": 16,
                "text": " Assess and mitigate business risks Amazon SageMaker Amazon SageMaker is a cloud-based machine learning infrastructure provided by Amazon Web Services (AWS). It offers an easy-to-use interface for:  building an ML model from scratch  preparing training data with minimum coding effort  training models  deploying models to production  automating ML workflows  automatically generating ML-based predictions Your team can scale SageMaker to handle large datasets and integrate it with other AWS services. Amazon ML tools may be useful for several specialists within your organization:  Data scientists who know how to code  Data scientists who work with low-code or no-code solutions  AI engineers who develop your products  Business analysts Amazon SageMaker pricing depends on the resources you use, and you can estimate your approximate costs using the AWS Pricing Calculator. What products benefit from AWS SageMaker? Numerous famous companies use AWS machine learning tools to streamline their business operations and transform organizations with the power of AI. Workday, Salesforce, Wix, Canva, and others use SageMaker for a variety of needs. Chris Hausler, head of AI at Zendesk, says that with SageMaker MME, the team built a multi-tenant, SaaS-friendly inference capability to host multiple models per endpoint, reducing inference cost by 90% compared to dedicated endpoints. You may use SageMaker to: "
            },
            {
                "page_number": 17,
                "text": " Boost an existing AWS-based product with AI functionality  Process large datasets and workloads  Detect fraud in real time OpenAI tools OpenAI is the legendary team that released Generative Pre-trained Transformers (GPT) models and Chat GPT; this team also stands behind DALL-E and a multitude of AI research papers, guides, and reports. OpenAI is a phenomenon in the IT industry and was one of the most discussed AI companies in December 2023. The company’s incredibly talented team has forced the AI breakthrough we’re now experiencing. OpenAI provides API services, allowing developers and businesses to harness the power of their AI models in various applications. The company offers the OpenAI API based on GPT-4 to solve your business tasks and ChatGPT subscription plans to enhance your product with fantastic capabilities. OpenAI actively engages in AI research and is known for its commitment to ethics and safety in AI development. They emphasize the responsible and transparent advancement of AI and publish research papers to support and educate the AI community. What products benefit from OpenAI APIs: Two million developers, 92% of Fortune 500 companies, and over 100 million people use ChatGPT weekly. The capabilities of OpenAI products can hardly be exaggerated: by integrating your product with OpenAI APIs, you can boost every business process, improve every product feature, and take a mile-long step towards improving your business. OpenAI products serve a variety of purposes, including creating NLP applications, generating different types of content, analyzing data, and building chatbots, wikis, and search recommendations. OpenAI products also allow you to experiment with the latest web technologies to discover the best use cases by yourself. Duolingo, Stripe, Wix, and numerous healthcare and government projects benefit from OpenAI solutions. "
            },
            {
                "page_number": 18,
                "text": "You may use OpenAI tools to:  Develop chatbots, virtual assistants, translation apps, and recommendation engines  Generate text, images, video, and audio content  Convert text to speech, images, and video  Extract insights from corporate data  Experiment with cutting-edge technologies  Enhance your routine business operations with automation, creativity, and machine intelligence These AI toolkits share a common feature: you need a subscription to use them. Most companies provide free plans, yet these are only suitable to test the waters and will not be enough to handle your business tasks. More advanced plans may impact your overall development budget. It may be easier for you to use off-the-shelf tools than to train a model from scratch, but still, you need to cooperate with an AI development team to use the capabilities of enterprise frameworks. How to choose an AI framework for your needs What AI framework may be best for your business? To answer this question, you should:  Collect information about your current business needs. Without a detailed picture of your product idea, no one can advise you on the best framework.  Consult with an AI app development team. The market is evolving rapidly, and it’s hard to keep track of all the latest technologies while running your "
            },
            {
                "page_number": 19,
                "text": "business. Development service providers may help you choose and implement the most suitable framework. As an alternative, you may consider CTO as a service providers to get an in-depth consultation on most powerful tools. Read also:What Is Software Development Consulting? We analyzed our previous experience with AI app development for our clients to determine the optimized flow for selecting the right framework: 1. Our project discovery team reviews your initial requirements. 2. A business analyst prepares essential project documentation. 3. A software architect develops an architectural approach for your app and suggests the right technologies for it, including the AI framework. 4. The discovery team composes a software requirements specification highlighting solutions tailored to your current needs. We take these steps during the project discovery phase at the very start of the product development process. Conclusion Choosing an AI framework for your project may be frustrating. There are many tools you may use, and not that many explanations on when to choose a particular technology. The number of AI technologies, libraries, and solutions is growing so fast that one can hardly find the time to explore and try all of them. In the meantime, the choice of AI framework is critical.  The right choice of technology may help you validate your idea quickly, implement AI functionality in your product, and start attracting new customers and investors faster than competitors. "
            },
            {
                "page_number": 20,
                "text": " The wrong framework may fail to meet your project’s needs. It may lack flexibility or be too complex for your tasks. After implementing it, you may notice the limitations and decide to go with another solution, but doing so will cost you additional time and money. In the AI market, it’s essential to test ideas quickly, iterate fast, and make the right decisions to get desirable business outcomes. With the right framework, you get one step closer to your goals. "
            }
        ],
        "images": []
    },
    {
        "file_name": "ToolsandFrameworks4.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "Top AI Frameworks for Developers AI framework is a suite of software libraries and tools designed to simplify building, training, and deploying AI applications. It offers a structured environment that manages complex computations, enabling developers to create efficient AI solutions more easily. AI frameworks accelerate AI development by offering ready-to-use tools that simplify complex processes, boost efficiency, and enable rapid experimentation. They make advanced AI accessible to a broader range of developers, including those with limited technical expertise. Geekflare has researched and compiled a list of the top AI frameworks for developers based on key features such as model training capabilities, scalability, compatibility with popular languages, community support, deployment options, pre-trained model availability, and ease of integration with other tools.  Tensorflow – Best for Production Deployment  Pytorch – Best for Research and Academic Use  Scikit-Learn – Best for Classical Machine Learning Models  JAX – Best for High-Performance Numerical Computing  Keras – Best for Prototyping  Hugging Face – Best for NLP and Pre-trained Language Models  OpenAI – Best for Advanced Language Models Tensorflow Best for Production Deployment TensorFlow is an open-source framework created by Google for building and deploying machine learning (ML) models. It’s designed to work on many platforms, like desktops, mobile devices, and the web, making it suitable for beginners and experts. "
            },
            {
                "page_number": 2,
                "text": "TensorFlow provides everything needed to create, train, and launch ML models, all in one place. TensorFlow Extended (TFX) includes tools to track, update, and improve models, making it easier to manage ML projects long-term. The framework allows for distributed training across multiple devices or machines, which is necessary for handling large datasets and speeding up the training process. As an open-source platform, TensorFlow benefits from a vibrant community that contributes to its ecosystem with tools, libraries, and pre-trained models, improving its capabilities and usability Tensorflow Key Features  Eager execution, TensorFlow’s default mode, allows you to run operations instantly, simplifying debugging and enhancing code clarity.  TensorFlow works well with Keras, a user-friendly API that helps you quickly create and train deep learning models.  The simplified API is easy to read and accessible, making TensorFlow beginner-friendly.  With TensorBoard, you can monitor training progress and track key metrics, providing insights into model performance.  Use TensorFlow SavedModel to package your models for easy sharing and deployment across different platforms. TensorFlow Use Cases TensorFlow is used for tasks like image and speech recognition, natural language processing (NLP), and predictive analytics. It helps build deep learning models for applications such as computer vision, recommendation systems, and even robotics, making it suitable for many industries. PROS:  - TensorFlow is optimized for performance on both CPUs and GPUs for faster model training. "
            },
            {
                "page_number": 3,
                "text": "- Works with Python, JavaScript, C++, and Java. - Integrates well with tools like TensorBoard and TFX. CONS:  - Require significant computing power for complex models.  - Present a steep learning curve for beginners in machine learning Scikit-Learn Best for Classical Machine Learning Models Scikit-Learn (or Sklearn) is an open-source Python library for machine learning, built on NumPy, SciPy, and Matplotlib for data visualization and analysis. It offers efficient techniques for feature extraction and selection, empowering users to prepare data effectively for model training. Scikit Learn includes methods that merge models to improve prediction accuracy like bagging and boosting. The library is extensively documented with tutorials and samples making it user-friendly, for newcomers and seasoned professionals alike. Scikit Learn includes tools for data preprocessing, such as scaling, encoding, and transforming data, as well as feature engineering techniques like polynomial features and principal component analysis (PCA). Being part of the broader Python ecosystem, It integrates well with other libraries, such as Pandas for data manipulation and Matplotlib for visualization. Scikit-Learn Key Features  Scikit-Learn offers a large collection of machine learning algorithms, including popular classification methods (e.g., decision trees, support vector machines), regression, clustering, and dimensionality reduction. "
            },
            {
                "page_number": 4,
                "text": " The library provides a uniform and easy-to-understand API across different algorithms, making it simple for users to switch between models and evaluate them.  Scikit-Learn provides tools for model selection and evaluation, including cross-validation, grid search, and performance metrics, which are critical for creating reliable models. Scikit-Learn Use Cases Scikit-learn is commonly used for tasks like spam detection in emails and predicting house prices. It excels in classification, regression, clustering for customer segmentation, and dimensionality reduction to simplify data, making it a valuable tool for data scientists and businesses looking to gain insights from their data. PRONS:  Provides easy-to-use interfaces that integrate well with other Python libraries.  Extensive resources and community support for troubleshooting and new user learning.  Integrates well with NumPy, SciPy, and Matplotlib, enhancing workflow flexibility. CONS:  Lacks deep learning capabilities, making it unsuitable for NLP and neural network tasks.  Performance can lag on very large datasets. JAX Best for High-Performance Numerical Computing JAX is an open-source machine-learning framework developed by Google to boost high-performance numerical computing and machine-learning research. It combines the capabilities of automatic differentiation and just-in-time (JIT) compilation to optimize "
            },
            {
                "page_number": 5,
                "text": "computations, making it particularly suitable for tasks that require efficient gradient computation. JAX introduces the DeviceArray, which allows the execution of code on hardware accelerators like GPUs and TPUs without changing the code structure. It encourages a functional programming approach, promoting immutability and pure functions, which can lead to cleaner and more maintainable code. JAX’s API is designed to be similar to NumPy’s, making it easier for users familiar with NumPy to transition to JAX while benefiting from its advanced features. Jax Key Features  JAX provides a set of composable transformations that can be applied in various combinations to optimize performance and allow complex workflows.  With JIT compilation, JAX can optimize functions for performance by compiling them into highly efficient code using the XLA (Accelerated Linear Algebra) compiler.  The vmap function allows users to automatically vectorize functions, making it easy to apply operations across batches of data without manual looping.  JAX supports parallel execution across multiple devices (GPUs/TPUs) using the pmap function, facilitating efficient computation on large datasets. Jax Use Cases JAX is used for high-performance machine learning and scientific computing. It excels in optimizing models, speeding up calculations with JIT compilation, and handling large datasets. Researchers use it for tasks like protein folding simulations and reinforcement learning. PROS: Allows for greater flexibility in defining custom operations and algorithms. High-performance numerical computing using Just-In-Time (JIT). "
            },
            {
                "page_number": 6,
                "text": "Use existing NumPy codebases with minimal changes, facilitating integration into current workflows. CONS:  Lack of comprehensive high-level API support compared to established frameworks like TensorFlow or PyTorch.  JAX does not provide built-in data loaders. Keras Best for Prototyping Keras framework is well known in the deep learning community for its easy-to-use interface and accessibility to newcomers and professionals alike in deep learning technologies. Its straightforward API simplifies building, training, and evaluating models, allowing developers to be productive with minimal code. Keras has a strong ecosystem of pre-trained models and open-source tools, which can be used for various applications, including image classification, NLP, and time series forecasting. It also retains its autonomy with support for backends such as Theano and Microsoft Cognitive Toolkit (CNT). Keras is designed for easy network creation, supports flexible model structures, and enables fast experimentation with high-level abstractions. It allows quick deployment on both CPUs and GPUs, making it ideal for testing and exploration in research settings. Keras Key Features "
            },
            {
                "page_number": 7,
                "text": " Keras allows users to create custom layers, loss functions, and model architectures, making it adaptable to various deep learning tasks.  Keras operates as a high-level API on popular deep-learning backends like TensorFlow, Theano, and CNTK.  Using TensorFlow’s infrastructure, Keras scales from small experiments to large-scale production applications. It can run on TPUs and large GPU clusters for improved performance. Keras Use Cases Keras is popular for deep learning applications because of its easy-to-use interface. It’s commonly used for image classification and object detection, like analyzing medical images and quality control in manufacturing. Additionally, Keras is applied in NLP for sentiment analysis, machine translation, and time series forecasting to predict stock prices or weather patterns. PROS:  Simplifies deep learning model creation with a user-friendly API. Both Sequential and Functional APIs allow for the creation of simple to complex architectures.  Includes built-in support for various neural network architectures, from CNNs to RNNs.  Supports both TensorFlow and Theano as backend engines, offering flexibility. CONS:  While great for high-level tasks, Keras abstracts away many low-level details. Lacks some advanced debugging features found in other frameworks like PyTorch. Hugging Face Best for NLP and Pre-trained Language Models Hugging Face is a popular open-source Python library designed to simplify the deployment of NLP models. Launched by the Hugging Face community, this library "
            },
            {
                "page_number": 8,
                "text": "supports various machine-learning tasks, particularly for language models, but also extends to other domains like computer vision, Image generation, and audio processing. The Hugging Face ecosystem includes an extensive Model Hub and other resources, such as the Datasets and Tokenizers libraries, which contribute to a user-friendly platform for AI research, deployment, and collaboration for individuals and businesses. Hugging Face Spaces allows users to create and share interactive demos of machine-learning models, simplifying model deployment without extensive coding requirements. Hugging Face Key Features  Offers pre-trained models for tasks like text classification, question answering, translation, image classification, and audio processing.  Allows users to train, fine-tune, and deploy models easily across different frameworks.  Includes optimized tokenizers that convert text into machine-readable formats, supporting multi-language processing and essential NLP functions like padding and truncation.  Provides an extensive collection of datasets covering a wide range of machine-learning tasks. This library simplifies data access for training and testing models. Hugging Face Use Cases Hugging Face Transformers are commonly used in NLP and other applications. Key use cases include improving customer support with chatbots that provide personalized assistance, automating content generation for articles and marketing "
            },
            {
                "page_number": 9,
                "text": "materials, and facilitating language translation for better communication. These tools are essential for modern AI solutions. PROS: Offer extensive pre-trained models for NLP tasks, accelerating development. Reduces costs associated with training models from scratch, providing ready-to-use models. Facilitate integration with popular frameworks like PyTorch and TensorFlow. CONS: Pre-existing frameworks can sometimes constrain model architecture customization. Running large language models locally can still be resource-intensive. OpenAI Best for Advanced Language Models OpenAI Framework is a set of tools, APIs, and libraries designed to help developers and organizations create and integrate artificial intelligence applications. It offers a range of models, including GPT (for text-based tasks), DALL-E (for image generation), Codex (for code generation), and more, all accessible via API. OpenAI’s framework supports various industries, allowing businesses, developers, and researchers to create creative solutions across different applications. The API-first approach allows developers to integrate OpenAI models into various applications, including mobile apps, websites, and enterprise software. OpenAI has implemented mechanisms to minimize harmful outputs, such as content filtering, moderation tools, and ethical guidelines for use, promoting safer and more responsible AI. OpenAI provides comprehensive documentation, sample code, tutorials, and an active community forum for support and collaboration. Key Features of OpenAI "
            },
            {
                "page_number": 10,
                "text": " Users can fine-tune models with domain-specific data to customize their behavior, improving model accuracy and relevance for niche applications.  Through a partnership with Microsoft, OpenAI models can be accessed via Azure, allowing enterprises to use scalable infrastructure, improve security, and comply with compliance standards.  OpenAI’s models, particularly GPT, use in-context learning. This allows users to guide responses by providing examples and instructions without retraining the model. OpenAI Use Cases OpenAI’s framework is used for chatbots in customer service, healthcare diagnostics, and finance fraud detection. It also aids in content creation, personalized learning, and predictive analytics. Tools like Codex help developers generate code from text, while DALL·E creates images from descriptions, improving creativity and efficiency. PROS:  Wide range of tasks covered, including NLP, image generation, and code.  Easy to access and integrate via API with comprehensive documentation. Provides access to AI models like GPT-4 and DALL-E, empowering various industries. CONS: The pricing can be high for frequent or high-volume usage.  Strict filters and ethical constraints may limit use cases. Langchain Best for Building Applications with LLMs "
            },
            {
                "page_number": 11,
                "text": "LangChain is an open-source framework designed to simplify the creation of applications using large language models (LLMs), such as OpenAI’s GPT-4. With built-in integrations and modular components, developers can build AI applications by combining LLM capabilities with external computation and data sources. By providing a user-friendly interface and structured workflow, LangChain empowers developers to implement diverse applications, including chatbots, document summarization tools, code analysis, and more. It supports Python and JavaScript, making it versatile for various development environments. LangChain provides data augmentation tools to generate similar data for training and supports tasks like text classification, summarization, sentiment analysis, and machine translation. The framework supports vector database integration for similarity search, making it easier to fetch relevant information based on vector representations of user inputs. Langchain Key Features  LangChain’s modular components (e.g., LLM Wrappers, Prompt Templates, and Indexes) serve as the basic building blocks for LLM-powered applications, allowing easy information retrieval, prompt management, and text generation.  Chains allow developers to combine multiple components to create end-to-end workflows for specific tasks, like document summarization, code analysis, or question-answering.  Agents allow LLMs to perform specific actions by interacting with external APIs or data sources, improving the functionality and usability of applications.  LangChain is available as a Python and JavaScript package, allowing development flexibility and broadening the range of supported applications. Langchain Use Cases "
            },
            {
                "page_number": 12,
                "text": "LangChain is best suited for building applications that use LLMs for tasks like chatbots, content generation, and data analysis. Its use cases include creating conversational agents, automating customer support, and developing personalized recommendations, making it ideal for improving user interaction and automating complex workflows. XGBoost Best for Gradient Boosting XGBoost is an advanced machine-learning algorithm widely used for its speed, accuracy, and efficiency. It is a type of ensemble model in the family of gradient boosting algorithms, which combines multiple “weak learners” (typically decision trees) to create a stronger predictive model. XGBoost optimizes this process by sequentially training each weak learner to correct the errors of its predecessors, resulting in a highly accurate model that is well-suited for both classification and regression tasks. Uses a depth-first approach with pruning to prevent overgrown trees, optimizing both model performance and interpretability. Speeds up training by approximating split points for large datasets, balancing efficiency and accuracy. XGBoost Key Features  Includes L1 and L2 regularization to prevent overfitting by penalizing complex models.  The learning rate reduces each tree’s influence, making the model more robust and less prone to overfitting.  Automatically accounts for missing data by assigning a default path for missing values during tree splitting. "
            },
            {
                "page_number": 13,
                "text": " Uses parallelism to boost training speed by simultaneously processing multiple data instances or trees.  Optimizes data access by utilizing CPU cache memory effectively, improving computational speed. XGBoost Use Cases XGBoost is best known for its effectiveness in structured data tasks, particularly in classification and regression problems. Its use cases include winning Kaggle competitions, handling large datasets, and improving predictive accuracy in customer churn prediction, fraud detection, and risk assessment applications. Caffe Best for Image Classification Tasks Caffe is an open-source deep-learning framework developed by the Berkeley Vision and Learning Center (BVLC). Designed for efficiency, modularity, and high performance. It provides access to various pre-trained models, such as AlexNet, VGGNet, and GoogleNet, facilitating transfer learning and experimentation. Caffe primarily supports convolutional neural networks (CNNs), making it popular for computer vision tasks like image classification, object detection, and segmentation. Its speed and flexibility make it useful for academic research and industry applications, mainly where fast deployment and low-latency inference are essential. Caffe Key Features  Supports Python and MATLAB APIs, making integration with other workflows seamless and allowing easy customization.  Caffe’s text-based files allow users to define networks modularly, simplifying the creation and modification of neural architectures.  Built-in tools visualize network architecture and track training progress, aiding in model debugging and optimization. "
            },
            {
                "page_number": 14,
                "text": " With a large user base and extensive documentation, users can benefit from various online resources and community support. Caffe Use Cases Caffe is best suited for deep learning tasks, particularly in computer vision applications such as image classification, segmentation, and object detection. Its use cases include developing models for facial recognition, self-driving cars, and image processing tasks, where high performance and efficient computation are critical. "
            }
        ],
        "images": []
    },
    {
        "file_name": "ToolsandFrameworks5.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "11 Top AI Frameworks and Libraries in 2024 In our fresh article, discover the top 11 modern artificial intelligence tools and frameworks to build robust and flexible architectures for your AI-powered apps. In the rapidly evolving landscape of technology, Artificial Intelligence (AI) and Machine Learning (ML) have emerged as revolutionary forces reshaping industries and daily life. These technologies power the behind-the-scenes magic in everything from personal voice assistants to predictive analytics in healthcare. AI Market Size in USD Billion (Source: Fively) As AI and ML continue to burgeon, an array of tools and frameworks have been developed to aid businesses in harnessing their potential. With countless options available, it can be overwhelming to determine which tools are most apt for specific tasks. Here at Fively, we've compiled a concise list of the top 11 AI tools and frameworks to guide you in making informed decisions. "
            },
            {
                "page_number": 2,
                "text": "Let's journey through each of these indispensable tools with our top ML and AI specialistAndrew Oreshko! List of AI Frameworks & Tools TensorFlow It’s an open-source library for dataflow and differentiable programming developed by the Google Brain team. It's primarily used for machine learning applications and deep neural network research. Languages: Python, C++, and Java Platforms: Linux, macOS, Windows, Android, iOS (via TensorFlow Lite) + Highly flexible and modular; + Excellent community support; + Scalable for both small and large applications; - The steeper learning curve for beginners; - Some operations can be less intuitive compared to other frameworks. “TensorFlow, as one of the market leaders, is ideally suited for large projects due to a developed ecosystem of products (TensorFlow Hub, TensorFlow Serving, TensorFlow Lite), great community, detailed documentation, and lots of code examples over the internet.” "
            },
            {
                "page_number": 3,
                "text": "Sci-kit Learn A free machine learning library that integrates seamlessly with the Python language. It's suitable for various machine-learning tasks, including classification, regression, and clustering. Languages: Python Platforms: Linux, macOS, Windows + Comprehensive library for machine learning algorithms; + Strong documentation and community support; + Ease of use and integration; - Not tailored for deep learning; - Doesn't support GPU acceleration. “Ski-kit Learn, just like Pandas, is very convenient, and has a bunch of ready-made packaged solutions.” Pandas A fast, powerful, and flexible open-source data analysis and data manipulation library. Languages: Python Platforms: Linux, macOS X, Windows + Comprehensive data manipulation tools; "
            },
            {
                "page_number": 4,
                "text": "+ Seamless integration with many other data science tools; + Strong community support; - Memory consumption can be high; - Some operations can be slower than pure NumPy. “Pandas attracts with its user-friendly interface. However, it should be noted that it can have problems when working with big data science projects (because it loads all the data it works with into RAM). There is a similar technology called Dask, which can parallelize its work - it’s really interesting to see its further development.” NumPy A library for the Python language, allowing support for large, multi-dimensional arrays and matrices along with a vast collection of high-level mathematical functions. Languages: Python Platforms: Linux, macOS X, Windows + Efficient mathematical operations; + Broad functionality for numerical tasks; + Seamless integration with other libraries; - Not specifically tailored for machine learning. “NumPy does all the computational heavy-lifting for working with vectors and matrices. This is needed both in Pandas (it is built on "
            },
            {
                "page_number": 5,
                "text": "NumPy), and in Scikit-Learn (since it is M-library, here the speed of calculations is important, which is what NumPy gives us).” AI Marketing Automation Tool Fively created a breakthrough AI-based marketing automation software, that helps to anticipate consumer actions and boost KPIs for businesses of all sizes. “Sci-kit Learn, Pandas, Numpy - they are the core of ML projects: convenient both for research and for building production-ready systems, they will continue to be used everywhere and to develop further.” PyTorch "
            },
            {
                "page_number": 6,
                "text": "Developed by Facebook's AI Research lab, it's a dynamic computational graph-based framework often compared to TensorFlow. It allows developers to use GPU acceleration for computations. Languages: Python, C++ Platfroms: Linux, macOS X, Windows + Dynamic computational graph; + Intuitive and more pythonic; + Strong support for GPU acceleration; - Slower than some counterparts due to dynamic computation. “PyTorch, another market leader with a large community and lots of code examples, is ideal for smaller projects and for research.” Keras It is an open-source neural network library written in Python. It's known for being user-friendly and modular, acting as an interface for TensorFlow and Theano. Languages: Python Platforms: Linux, macOS X, Windows (depends on the backend: TensorFlow, Theano, etc.) + User-friendly API; + Modular and extendable; + Supports multiple backend neural computation engines; "
            },
            {
                "page_number": 7,
                "text": "- Performance can sometimes be suboptimal; - Dependency on TensorFlow or Theano as a backend. “Keras can run on top of TensorFlow or Theano. Keras is, in fact, a top-level API for these backends (TensorFlow/Theano), which simplifies model development.” Theano An open-source numerical computation library that lets developers efficiently define, optimize, and evaluate mathematical expressions. Languages: Python Platforms: Linux, macOS X, Windows + Highly efficient for numerical tasks; + GPU acceleration support; + Tight integration with NumPy; - Development has been halted; - Not as beginner-friendly as some other options. Avo HR Automation Tool Development Case Fively helped to create a breakthrough AI-based HR automation tool that boosts employee engagement, performance, and satisfaction in businesses of all sizes. "
            },
            {
                "page_number": 8,
                "text": "Google ML Kit A mobile SDK that brings Google's machine learning capabilities to Android and iOS apps, with powerful, yet easy-to-use solutions. Languages: Java (Android), Swift/Objective-C (iOS) Platforms: Android, iOS + Easy to integrate into mobile apps; + Pre-trained models available; + Supports custom TensorFlow Lite models; - Limited to mobile app development; - Some advanced use cases may need further fine-tuning. "
            },
            {
                "page_number": 9,
                "text": "Caffe 2 Now we move on to deep learning tools and frameworks. The first one is Caffe 2: an open-source deep learning framework with modularity and speed in mind. Developed by Facebook, it's the successor to the Caffe framework. Languages: Python, C++ Platforms: Linux, macOS X, Windows, Android, iOS + Highly modular and performant; + Suitable for mobile deployment; + Strong community and support; - The steeper learning curve for beginners; - Less popular compared to TensorFlow and PyTorch. Need a Project Estimation? Let's calculate the price of your project with Fively. Estimate a project Microsoft CNTK "
            },
            {
                "page_number": 10,
                "text": "The Microsoft Cognitive Toolkit, or CNTK for short, is a deep learning framework developed by Microsoft. It is known for its efficiency at scale. Languages: Python, C++ Platforms: Linux, macOS X, Windows + Efficient for large-scale datasets; + Strong support for recurrent neural networks (RNNs); + High performance and scalability; + Less intuitive API compared to some counterparts; + Limited community support. MxNet A deep learning framework designed for both efficiency and flexibility. It allows developers to mix symbolic and imperative programming. Languages: Python, C++, Julia, R, Scala, Perl Platforms: Linux, macOS X, Windows, iOS, Android + Supports multi-language APIs; + Highly efficient and scalable; + Mix of symbolic and imperative programming; + Lesser community support compared to TensorFlow and PyTorch; "
            },
            {
                "page_number": 11,
                "text": "+ Some parts of the documentation might be lacking. Top AI Frameworks and Libraries Comparison With a plethora of options available, it's crucial to choose the right one that aligns with your project requirements, expertise, and platform preferences. In the table below, we've collated the main aspects of some of the leading tools, providing a side-by-side comparison to aid in your decision-making process. Let's get a birds-eye view of these tools and discern the subtle nuances that set each apart. AI Frameworks Comparison. Source: Fively Embracing the AI and Machine Learning Revolution "
            },
            {
                "page_number": 12,
                "text": "In the realm of Artificial Intelligence, the apprehension surrounding frameworks often stems from the fear of the unknown. Many professionals are wary, thinking that these tools might overshadow or replace their roles. However, AI solutions like GhatGPT or professional artificial intelligence frameworks like we described above in this article, should be perceived as our allies in building great custom software. The only question that arises is how to choose the AI tool that fits your project needs. Here’s how Andrew Oreshko comments on this: "
            }
        ],
        "images": [
            "Image_9",
            "Image_27",
            "Image_42",
            "Image_53"
        ]
    },
    {
        "file_name": "training_data.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "The Essential Guide to Quality Training Data for Machine Learning What You Need to Know About Data Quality and Training the Machine Machine learning models depend on data. Without a foundation of high-quality training data,  even the most performant algorithms can be rendered useless. Indeed, robust machine learning  models can be crippled when they are trained on inadequate, inaccurate, or irrelevant data in  the early stages. When it comes to training data for machine learning, a longstanding premise  remains painfully true: garbage in, garbage out. Accordingly, no element is more essential in machine learning than quality training data.  Training data refers to the initial data that is used to develop a machine learning model, from  which the model creates and refines its rules. The quality of this data has profound implications  for the model’s subsequent development, setting a powerful precedent for all future  applications that use the same training data. If training data is a crucial aspect of any machine learning model, how can you ensure that your  algorithm is absorbing high-quality datasets? For many project teams, the work involved in  acquiring, labeling, and preparing training data is incredibly daunting. Sometimes, they  compromise on the quantity or quality of training data – a choice that leads to significant  problems later. Don’t fall prey to this common pitfall. With the right combination of people, processes, and  technology, you can transform your data operations to produce quality training data,  consistently. To do it requires seamless coordination between your human workforce, your  machine learning project team, and your labeling tools. In this guide to training data, we’ll cover how to create the quality training data inputs your  model craves. First, we’ll explore the idea of training data in more detail, introducing you to a  number of related terms and concepts. From there, we’ll discuss the people, technology, and  processes involved in developing first-rate training data. We’ll also consider the challenges of cleaning and filtering training data, working with teams and  labeling tools, to produce large volumes of high-quality data. Our guide will present the most  productive approaches to these endeavors, illustrating the importance of effective  management, feedback, and communication. As you’ll discover, creating powerful machine  learning models often depends on the expertise and reliability of your human workforce. The Basics:  Training Data and Machine Learning What is training data? In machine learning, training data is the data you use to train a machine learning algorithm or  model. Training data requires some human involvement to analyze or process the data for  machine learning use. How people are involved depends on the type of machine learning  algorithms you are using and the type of problem that they are intended to solve.    With supervised learning, people are involved in choosing the data features to be used  for the model. Training data must be labeled - that is, enriched or annotated - to teach  the machine how to recognize the outcomes your model is designed to detect. "
            },
            {
                "page_number": 2,
                "text": "   Unsupervised learning uses unlabeled data to find patterns in the data, such as  inferences or clustering of data points. There are hybrid machine learning models that  allow you to use a combination of supervised and unsupervised learning. Training data comes in many forms, reflecting the myriad potential applications of machine  learning algorithms. Training datasets can include text (words and numbers), images, video, or  audio. And they can be available to you in many formats, such as a spreadsheet, PDF, HTML, or  JSON. When labeled appropriately, your data can serve as ground truth for developing an  evolving, performant machine-learning formula. What is labeled data? Labeled data is annotated to show the target, which is the outcome you want your machine  learning model to predict. Data labeling is sometimes called data tagging, annotation,  moderation, transcription, or processing. The process of data labeling involves marking a  dataset with key features that will help train your algorithm. Labeled data explicitly calls out  features that you have selected to identify in the data, and that pattern trains the algorithm to  discern the same pattern in unlabeled data. Take, for example, you are using supervised learning to train a machine learning model to review  incoming customer emails and send them to the appropriate department for resolution. One  outcome for your model could involve sentiment analysis - or identifying language that could  indicate a customer has a complaint, so you could decide to label every instance of the words  “problem” or “issue” within each email in your dataset. That, along with other data features you identify in the process of data labeling and model  testing, could help you train the machine to accurately predict which emails to escalate to a  service recovery team. The way data labelers score, or assign weight, to each label and how they manage edge cases  also affects the accuracy of your model. You may need to find labelers with domain expertise  relevant to your use case. As you can imagine, the quality of the data labeling for your training  data can determine the performance of your machine learning model. "
            },
            {
                "page_number": 3,
                "text": "Enter the human in the loop. What is human in the loop? “Human in the loop” applies the judgment of people who work with the data that is used with a  machine learning model. When it comes to data labeling, the humans in the loop are the people  who gather the data and prepare it for use in machine learning. Gathering the data includes getting access to the raw data and choosing the important  attributes of the data that would be good indicators of the outcome you want your machine  learning model to predict. This is an important step because the quality and quantity of data that you gather will determine  how good your predictive model could be. Preparing the data means loading it into a suitable  place and getting it ready to be used in machine learning training. Consider datasets that include point-cloud data from lidar-derived images that must be labeled  to train machine learning models that operate autonomous vehicle (AV) systems. People use  advanced digital tools, such as 3-D cuboid annotation software, to annotate features within that  data, such as the occurrence, location, and size of every stop sign in a single image. This is not a one-and-done approach, because with every test, you will uncover new  opportunities to improve your model. The people who work with your data play a critical role in  the quality of your training data. Every incorrect label can have an effect on your model’s  performance. How is training data used in machine learning? Unlike other kinds of algorithms, which are governed by pre-established parameters that  provide a sort of “recipe,” machine learning algorithms improve through exposure to pertinent  examples in your training data. The features in your training data and the quality of labeled training data will determine how  accurately the machine learns to identify the outcome, or the answer you want your machine  learning model to predict. For example, you could train an algorithm intended to identify suspicious credit card charges  with cardholder transaction data that is accurately labeled for the data features, or attributes,  you decide are key indicators for fraud. "
            },
            {
                "page_number": 4,
                "text": "The quality and quantity of your training data determine the accuracy and performance of your  machine learning model. If you trained your model using training data from 100 transactions, its  performance likely would pale in comparison to that of a model trained on data from 10,000  transactions. When it comes to the diversity and volume of training data, more is usually better –  provided the data is properly labeled. 'As data scientists, our time is best spent fitting models. So we appreciate it when the data is  well structured, labeled with high quality, and ready to be analyzed,” says Lander Analytics  Founder and Chief Data Scientist Jared P. Lander. His full-service consulting firm helps  organizations leverage data science to solve real-world challenges. Training data is used not only to train but to retrain your model throughout the AI development  lifecycle. Training data is not static: as real-world conditions evolve, your initial training dataset  may be less accurate in its representation of ground truth as time goes on, requiring you to  update your training data to reflect those changes and retrain your model. "
            },
            {
                "page_number": 5,
                "text": "What is the difference between training data and testing data? It’s important to differentiate between training and testing data, though both are integral to  improving and validating machine learning models. Whereas training data “teaches” an  algorithm to recognize patterns in a dataset, testing data is used to assess the model’s  accuracy. More specifically, training data is the dataset you use to train your algorithm or model so it can  accurately predict your outcome. Validation data is used to assess and inform your choice of  algorithm and parameters of the model you are building. Test data is used to measure the  accuracy and efficiency of the algorithm used to train the machine - to see how well it can  predict new answers based on its training. Take, for example, a machine learning model intended to determine whether or not a human  being is pictured in an image. In this case, training data would include images, tagged to  indicate the photo includes the presence or absence of a person. After feeding your model this  training data, you would then unleash it on unlabeled test data, including images with and  without people. The algorithm’s performance on test data would then validate your training  approach – or indicate a need for more or different training data. How can I get training data? You can use your own data and label it yourself, whether you use an in-house team,  crowdsourcing, or a data labeling service to do the work for you. You also can purchase training  data that is labeled for the data features you determine are relevant to the machine learning  model you are developing. Auto-labeling features in commercial tools can help speed up your team, but they are not  consistently accurate enough to handle production data pipelines without human  review. Hasty, Dataloop, and V7 Labs have auto-labeling features in their enrichment tools. Your machine learning use case and goals will dictate the kind of data you need and where you  can get it. If you are using natural language processing (NLP) to teach a machine to read, "
            },
            {
                "page_number": 6,
                "text": "understand, and derive meaning from language, you will need a significant amount of text or  audio data to train your algorithm. You would need a different kind of training data if you are working on a computer vision project  to teach a machine to recognize or gain understanding of objects that can be seen with the  human eye. In this case, you would need labeled images or videos to train your machine  learning model to “see” for itself. There are many sources that provide open datasets, such as Google, Kaggle and Data.gov. Many  of these open datasets are maintained by enterprise companies, government agencies, or  academic institutions. How much training data do I need? There’s no clear answer - no magical mathematical equation to answer this question - but more  data is better. The amount of training data you need to create a machine learning model  depends on the complexity of both the problem you seek to solve and the algorithm you develop  to do it. One way to discover how much training data you will need is to build your model with  the data you have and see how it performs. Winning The Race to Quality Data Quality training data is vital when you are creating reliable algorithms. According to research by  analyst firm Cognilytica, more than 80% of artificial intelligence (AI) project time is spent on data  preparation and engineering tasks. Doing the work in-house can be costly and time-consuming. Outsourcing the work can be  challenging, with little to no communication with the people who work with your data, which  results in low quality. Crowdsourcing can cost more because it uses consensus to measure  quality, an approach that requires multiple workers to complete the same task. The correct  answer is the one that comes back from the majority of workers. "
            },
            {
                "page_number": 7,
                "text": "When you are labeling data for machine learning, winning the race to quality data requires a  strategic combination of people, process, and tools. What affects training data quality? There are three main factors that can help you predict the level of quality you can expect from  the people who work with your data - whether your workers are in-house, crowdsourced, or  outsourced teams. 1. People: The selection, development, and management of workers 2. Process: How workers do the work - from onboarding to task instructions to quality control workflows 3. Tools: The technology to access the work, manage workers, and maximize quality and throughput 1) People Quality starts with the people that do the work. Workers’ experience and the training they are  provided significantly impact the level of work they deliver. Worker assessment and selection is  the first opportunity to positively affect data quality. Worker skill assessments help you ensure a  minimum acceptable level of quality. "
            },
            {
                "page_number": 8,
                "text": "Regular training is important. Depending on the difficulty and complexity of a task, customized  training may be required to ensure the continued skill development of the data worker, which  results in higher quality work. For very simple yes-or-no tasks, minimal training may be enough  to deliver sufficient quality levels. However, for tasks with a range of complexity, nuance, or  subjectivity, higher-level training programs may be required to train workers quickly while  ensuring quality results. This is where crowdsourcing can fall short, as workers can change from day to day - so you  aren’t able to capture the value of the domain knowledge they gain from working with your data. 2) Process The best process for labeling quality training data is built for scale, with tight quality controls  and clear parameters for task precision. Communication and collaboration are important  because with machine learning, you will want the ability to quickly iterate the work to meet  evolving business goals. If you’ve already been doing the work in-house, you want to make sure that a potential data  labeling partner is willing and able to take your pre-existing processes, review them for best  practices, and if necessary, modify them to work for their workforce. If you’re just getting started  or want to update your process, it’s important to find a partner that can help you design a  process from scratch. Ideally, you’d find a partner that can do both. You will need to allow room for iteration along the way; in fact, it is a vital best practice. To  iterate quickly requires direct communication with the people who work with your data and  quick, clear feedback from them so you can make rapid improvements in your process. 3) Tools Like any project or task, without the proper tools, you simply can’t do a good job. If you’re stuck  with outdated or incompatible technologies, productivity decreases and quality suffers. Poor-fit  tooling can dramatically slow down and even stop the most innovative projects - or make the  process more costly. Implementing effective tools can improve outcomes, increase speed, and  reduce project costs. In an example of a simple tool enhancement to improve efficiency, one CloudFactory client  increased throughput 3x by embedding a Google Maps view into its tooling screen, rather than  requiring data labelers to open Google Maps in a separate browser tab. The goal with tooling is flexibility. If you are using a partner to gather and prepare your training  data, avoid tying your workforce to your tool. Be sure to consider how important it will be to  make changes in tooling features, whether you want to own the tool as intellectual property, or  whether the tool will be able to keep up as your tasks and use cases progress. Scaling Quality Training Data If machine learning models are only as good as their training data, and great training data  requires the right humans in the loop, we find ourselves at an ironic conclusion: Machine  learning success depends on your human workforce. How do you develop a large enough team that has expertise in your use case and is agile enough  to evolve your process along the way? How can you keep costs down while maintaining high  quality? You might need a training data labeling service. "
            },
            {
                "page_number": 9,
                "text": "Why is a managed team better than crowdsourcing? Many organizations crowdsource the development of their training data, entrusting this crucial  work to hundreds or thousands of anonymous workers. This approach can be problematic  because you will shoulder the management burden, and you may not have access to gather  feedback from the team or provide it to them. At CloudFactory, we outsmarted outsourcing and crowdsourcing by introducing a managed- team approach to data processing, preparation, and enrichment. A managed team provides the  agility you need to make changes, the flexibility to scale your team up or down, and technology  that puts you in direct communication with your data workers via a leader who works alongside  the team. In essence, a CloudFactory team is an extension of your own. Research shows the managed team approach results in higher quality. A study by data science  tech developer Hivemind showed managed teams were more effective in data labeling than  crowdsourced workers, and they also worked faster. In the study, the managed team provided  higher quality work and was only slightly more expensive than the crowdsourced workers. The managed team approach is an effective way to scale your training data operations. Your  training data is too essential to throw it to an anonymous crowd. When you need humans in the  loop, it’s important to know they can produce high quality work. Questions for a Data Labeling Partner About Quality Here are questions you can ask data labeling providers to assess their ability to deliver the  quality training data you want: 1. Do you offer dedicated success managers and project managers? How will our team communicate with your data labeling team? 2. How do you screen and select your workforce? Will we work with the same data labelers over time? If workers change, who trains new team members? Describe how you  transfer context and domain expertise as team members transition on and off the data  labeling team. 3. Is your data labeling process flexible? How will you manage changes or iterations from our team that affect data features for labeling? 4. How do you manage quality assurance? How do you share quality metrics with our team? What happens when quality measures aren’t met? How involved in quality control  (QC) will my team need to be? Quality Data = Better Outcomes:  The CloudFactory Advantage We understand the essential role that people play in the iterative development of machine  learning models. We work hard - so you don’t have to. Our people, processes, tools, and team  model work together to deliver the high quality work you would do yourself - if you had the time. We screen our workers for character and skills, and we make investments in their professional  and personal development. Our teams are actively supported and managed, allowing for  accountability, oversight, and maximum efficiency - all in the service of your business rules and  goals. "
            },
            {
                "page_number": 10,
                "text": "We are tool-agnostic, so we can share our experience with available tools, offer you access to  our best-of-breed industry partners, or use tools you develop and maintain yourself. We can  improve the quantity and quality of your training data, no matter how you are getting the work  done today. Our expertise has helped companies across diverse industries conquer their training data  challenges. Are you ready to get the high quality training data you want for your machine learning model?  Find out what we can do for you. "
            }
        ],
        "images": [
            "Image_32",
            "Image_38",
            "Image_44",
            "Image_52",
            "Image_55"
        ]
    },
    {
        "file_name": "training_data2.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "What Is Training Data? Complete Guide Training data is used in the field of Artificial Intelligence (AI) to advance machine learning (ML)  models. This data is essential for ML algorithms to improve their learning abilities and drive  progress in AI technology. Training Datasets in Machine Learning The concept of AI training data Training data consists of input-output pairs, where the input, known as “features” or  “predictors”, is matched with an output referred to as a “label” or “target.” These datasets  contain various types of information – from images and text to numerical values and sensor  readings. For instance, in image classification, training data contains images paired with labels that  describe objects or scenes depicted in the pictures. How is training data employed to teach ML and AI models? Machine learning algorithms use training data to learn to analyze patterns within the datasets  and improve their understanding of real-world situations. Throughout this process, the "
            },
            {
                "page_number": 2,
                "text": "algorithm adjusts its parameters to minimize the gap between its predictions and the actual  labels in the training data. This continuous process enhances the model’s accuracy in making predictions. As the  algorithm dives deeper into the dataset’s specifics, it gradually builds a representation of the  underlying patterns. The purpose of training is to equip the system with the ability to apply its  knowledge to new data, allowing it to predict or classify instances it hasn’t encountered before. In essence, training data shapes algorithms by providing them with knowledge and insights  needed to handle complexities in real-world data. Without an accurate and representative  training dataset, machine learning models may be susceptible to biases and oversights that  could restrict their capacity to generalize to unfamiliar examples. What’s the difference between training and testing data? Training data and testing data serve different purposes and are incorporated during different  stages of the ML process. "
            },
            {
                "page_number": 3,
                "text": "1. Training data remains accessible throughout model development, and it’s typically larger in volume to enable effective pattern learning. 2. Conversely, testing data is used to evaluate how well the model performs: it’s employed after the training process to gauge its ability to handle unseen data  accurately. The testing dataset must mirror real-world scenarios closely. 3. While training data undergoes preprocessing steps like normalization and feature engineering before being utilized for training purposes, testing data also goes through  similar processes to ensure consistency in input format and feature representation. In short, training data teaches the model, while testing data evaluates its effectiveness. Both  datasets are integrated to uphold the quality of the machine learning process. Types of Training Data Training data comes in different forms – each has its own distinctive characteristics and  applications. Let’s explore the three main training data types: structured, unstructured, and  semi-structured. Structured data Structured data is typically arranged in a specific format – usually in columns and rows – to aid  ML learning models in data processing and analysis. This type of training data adheres to a  defined structure outlined by a schema, making it easy for machines to organize and analyze  data. Some basic examples of structured data include databases, spreadsheets, and tables. Imagine  an Excel spreadsheet with rows and columns filled with numbers or a database that stores  customer information neatly categorized by name, address, and phone number – that’s  structured training data. "
            },
            {
                "page_number": 4,
                "text": "Common sources of structured datasets include databases, customer relationship  management (CRM) systems, and financial records. These sources provide data for training  machine learning models in tasks such as predictive analytics, customer segmentation, and  fraud detection. Other sources of this data are APIs, Web Services (like JSON or XML), and Internet of Things (IoT)  Devices that generate data in specific formats suitable for analysis across various fields (smart  homes, healthcare, and industrial automation). Structured data finds applications across different industries. In finance, structured data is  used in analyzing stock market trends and forecasting market changes. In the field of  healthcare, electronic health records (EHRs) provide data that can assist in decision-making  and patient diagnoses. By incorporating microdata tags within the HTML code of a webpage, search engines receive  detailed information to enhance the site’s SEO features. Unstructured data Unstructured data refers to information that lacks a predefined format or organization. Unlike  structured data, which is typically stored in databases with a specific schema, unstructured  data doesn’t follow a fixed structure. It includes various types of content – text, images, videos,  audio recordings, social media posts, and more. Since unstructured data is often qualitative in nature, it may not neatly fit into rows and columns  like structured data does. Examples of unstructured data can include text documents,  customer reviews, sensor readings, and multimedia files. "
            },
            {
                "page_number": 5,
                "text": "Due to its formats and lack of organization, analyzing and processing this type of data using  traditional methods can be challenging. However, advancements in technologies like NLP and  ML have enabled an effective extraction of insights from unstructured datasets. Key sources of unstructured training data consist of social media platforms, news articles,  online forums, open-ended survey responses, emails, etc. These platforms generate volumes of  information that can be used for gaining insights through NLP techniques, computer vision, and  other AI approaches. Unorganized data serves purposes in various fields: in marketing, analyzing social media  sentiments helps companies understand customer opinions and preferences. In healthcare, it  plays a role in research and managing patient records, clinical notes, and medical images. Financial institutions use unstructured data from news articles and social media to evaluate  risks and make investment decisions. In manufacturing and industrial IoT sectors, sensor data  and equipment logs are utilized to enhance processes and predict failures. Semi-structured data Semi-structured data falls between the two previous data types and offers a mix of organization  and flexibility. This type of training data possesses a defined structure but still allows for some  degree of variation in format. Sources of semi-structured training material include web applications with user interactions,  API responses, and log files. Since semi-structured data is a combination of the two data types  mentioned above, its key sources also encompass Internet of Things Devices and social media  platforms. Semi-structured data can also be found in legal documents and surveys. Semi-structured data is applied in real-time analysis of social media feeds or website traffic – it  helps identify and correct problems quickly. It’s also used for customer personalization by  providing recommendations based on the buyer’s preferences and past behavior. In scientific "
            },
            {
                "page_number": 6,
                "text": "research, the flexibility of semi-structured training data is used to analyze complex information  – e.g., gene sequences or results of an experiment. Comparing training data types Aspect  Structured Data  Unstructured Data  Semi-Structured Data Overview  Highly organized and  follows a clear format Lacks a predefined  structure Has a defined structure  but allows for some  variability in format Organization Data is organized into  rows and columns,  making it easy to sort and  analyze Data lacks a predefined  structure, making it  challenging to analyze  without advanced  techniques Data has a defined  structure but may  contain elements that  don’t conform to the  structure Examples  Databases,  spreadsheets, tables Social media posts,  emails, multimedia  content XML files, JSON  documents, web log  files Common  Sources Transactional databases,  CRM systems, financial  records Social media platforms,  news articles, emails, etc Web pages, sensor  data streams, log files Applications Predictive analytics,  customer segmentation,  financial analysis Sentiment analysis, image  recognition, speech  recognition Web analytics, IoT data  analysis, content  management Analysis  Techniques Easily analyzed using SQL  queries and statistical  tools Requires advanced  techniques such as NLP  and computer vision Requires parsing  techniques and may  involve extracting  structured data from  unstructured sources Benefits – Easy to organize and  analyze – Well-suited for  traditional statistical  analysis- Enables  efficient querying and  retrieval of information – Captures a wide range of  data types – Offers  valuable insights from  diverse sources – Combines structure  with flexibility – Allows  for easy integration of  new data sources Disadvantages – Limited flexibility for  capturing complex  relationships – May not  accommodate all types of  data  – Requires  predefined schemas – Difficult to analyze  without specialized  tools  – Requires extensive  preprocessing – May  contain noise and  irrelevant information – Complexity in  handling variations in  data format – Potential  for inconsistencies in  data structure Characteristics of High-Quality Training Data in Machine Learning "
            },
            {
                "page_number": 7,
                "text": "In the field of machine learning, it’s crucial to grasp the different aspects of training data to  create dependable models. Accuracy Precision plays an important role in creating top-notch training data – it allows for error-free  forecasts and insights by trained ML models. According to Gartner, issues with data quality cost  companies an average of $15 million per year. Investing in high-quality training data from the  start can save significant resources in the long run. Ensuring data accuracy involves conducting validation and verification processes. For example,  in the healthcare industry, where precision is especially important, methods such as cross- referencing with records and expert evaluations are indispensable. Similarly, in the financial sector, where errors could result in economic loss, vigorous validation  through audits and compliance checks are conducted regularly. Relevance It’s crucial to guarantee that the data used to train machine learning models aligns closely with  the task or problem these models aim to solve. To maintain data relevance, professionals need to structure training datasets that reflect real- world scenarios and use cases. For instance, in retail, where understanding customer behavior  is vital for business success, relevant training data might include purchase history, website  interactions, and demographic details. In cybersecurity, relevant training data could comprise  network traffic logs, malware samples, and security alerts. It’s important to note that data relevance extends beyond just the content of the data – it also  encompasses the context in which it was collected. For example, when dealing with predictive  maintenance for manufacturing equipment, relevant training data should consider factors like  conditions, maintenance logs, and environmental variables. Diversity Diversity in training data ensures that ML models generalize well to unfamiliar examples. It also  reduces bias, enhances adaptability, and contributes to the handling of edge cases.    When models are exposed to a wide range of training data, they can better understand real- world scenarios and approach problems from multiple perspectives. In the field of natural  language processing, where subtle language nuances play an important role, incorporating  training data that includes different dialects, accents, and languages can significantly enhance  a model’s performance. Similarly, in computer vision tasks, using training data with varying lighting conditions, camera  angles, and backgrounds aids in object recognition across various environments. Ensuring diversity in training data is also an ethical issue. Professionals must strive for  representation across different demographics, regions, minorities, etc to prevent biases and  promote inclusivity. Techniques like data augmentation, synthetic data creation, and transfer learning can help  expand training datasets and introduce diversity to the training data. "
            },
            {
                "page_number": 8,
                "text": "Challenges in Creating Training Data Data collection The process of collecting data can be quite daunting. Here’re some of the most common  challenges in data collection. Difficulties in sourcing and collecting training data Managing data quality Raw data often contains errors and discrepancies despite efforts to maintain accuracy during  collection. Data profiling is essential to identify issues, while data cleansing helps resolve them. Finding relevant data The process of gathering data itself presents a complex task for data scientists. Implementing  data curation techniques such as creating a data catalog and searchable indexes can simplify  data discovery and accessibility. Choosing data for collection Deciding which data to collect initially and for specific purposes is crucial. Collecting wrong  data can increase time and costs, while omitting information may diminish the dataset’s value  and affect analytic outcomes. Dealing with Big Data Managing volumes of unstructured and semi-structured data in Big Data environments adds  complexity during collection and processing phases. Data scientists often need to navigate  through data stored in a data lake (a centralized repository) to extract relevant information. Ethical and legal data collection practices When collecting training data, one must comply with privacy regulations like GDPR in Europe or  CCPA in California to safeguard individuals’ rights and avoid legal repercussions. Ethical considerations come into play when handling sensitive information. It’s crucial to obtain  informed consent from individuals and anonymize or pseudonymize data to protect privacy.  According to a report from Gartner, in 2023 around 65% of the population has its personal data  covered under modern privacy regulations, up from 10% in 2018. This highlights the increasing  significance of ethical and lawful data gathering methods in today’s age of data protection. Data Annotation Once data is collected, it needs to undergo annotation – it’s a process of labeling data with  relevant tags to make it more comprehensive for computers. However, data annotation  presents its own set of challenges. Challenges in annotating training data for ML models Quality and consistency Any mistakes or biases introduced by the annotators can significantly impact the performance  of machine learning models. Developing clear annotation guidelines and conducting regular  training sessions for annotators can save the day.. "
            },
            {
                "page_number": 9,
                "text": "Scalability Scaling the annotation process while preserving quality poses a challenge. This is due to the  amount of data required for machine learning models to learn effectively. Using automated  annotation tools can really help with scalability – read more about it here. Domain-specific knowledge When it comes to annotating medical images or legal documents, having annotators with  expertise in these fields is vital. However, recruiting and retaining these experts can be both  challenging and costly. Costs Annotation typically involves an expensive process for tasks demanding high precision. Striking  a balance between cost and annotation quality is always an endeavor – automated annotation  can help with this problem too. Data bias Data bias occurs when training data is limited in some way, painting an inaccurate  representation of the issue at hand, or failing to tell the full story. It’s essential to tackle data  bias to uphold fairness and equality in the realm of machine learning applications. The impact of data bias on machine learning models The conventional machine learning method frequently overlooks the importance of edge cases  and bias reduction. This results in models that may perform well at common tasks but ignore  rare scenarios or inherent data biases. Data bias occurs when certain groups are either underrepresented or overrepresented in the  training data, resulting in biased predictions and unfair treatment of individuals from  underrepresented groups. According to a study by MIT Technology Review, facial recognition systems developed by IBM,  Microsoft, and Face++ show higher error rates when identifying individuals with darker skin  tones. This demonstrates how data bias can affect the accuracy of ML models. Strategies for Identifying and mitigating bias in training Data Addressing bias in training data requires careful analysis and proactive measures. Methods like  utilizing bias detection algorithms and fairness-aware machine learning are often employed to  battle this issue. Moreover, using supervised or semi-supervised data annotation methods can help in reducing  bias manually, with human intervention. Human-in-the-Loop method (HITL), with its emphasis  on oversight conducted by human annotators, ensures that data bias is detected and  eliminated, leading to fairer, more impartial outcomes. Tips for Creating High-Quality Data for Machine Learning and Computer Vision Projects Data augmentation techniques Data augmentation is a technique used in ML that allows an increase in the diversity of training  data without actually collecting new data. This is achieved by applying various transformations  to existing data to create altered versions of it, thus expanding the dataset. "
            },
            {
                "page_number": 10,
                "text": "The employment of data augmentation contributes to improving the robustness and diversity of  training datasets in machine learning models. For example, a study in the “Journal of Machine  Learning Research” found that data augmentation can significantly improve the accuracy of  image classification models, in some cases by up to 20%. A range of data augmentation methods are available to introduce variations into existing data  points, replicating real-world data that models may face during deployment. These techniques  include image rotation, flipping, scaling, cropping, and adding noise to datasets to increase  their adaptability. For instance, in object detection tasks, techniques like cropping and rotation  can mimic changes in object size and orientation, allowing models to better adapt to real-world  scenarios. Similarly, within natural language processing tasks, methods like synonym  replacement and word dropout introduce variability into text data to enhance model resilience. In addition, the recent advancements in data augmentation methods such as generative  adversarial networks (GANs) have enabled the creation of realistic data samples that blur the  line between artificial and real-world datasets. Augmentation methods based on GANs have  proven the ability to expand the diversity of training datasets. Quality assurance and validation Stringent quality assurance procedures involve validating, verifying, and cleaning data to  identify and rectify errors, inconsistencies, and biases in the training dataset. Techniques like outlier identification, missing value imputation, and duplicate removal are  employed to eliminate irregularities in training data and guarantee its reliability. Moreover,  establishing validation criteria for data and conducting verification checks help maintain high  data quality throughout the machine learning process. Validation methods cross-validation and holdout validation play a crucial role in evaluating  model performance and identifying potential issues at an early stage. By assessing model  accuracy, precision, recall rates, and other performance metrics, data scientists can  continually enhance their models to boost their effectiveness. Micro models Micro models, also referred to as small-scale models or sub-models, provide a solution to  handling specific tasks or components within larger machine learning systems. These compact  models are trained on specialized subsets of data, they are optimized for efficiency and speed –  this makes them well-suited for resource-constrained environments. The idea behind micro models aligns with the concepts of modularization and scalability,  allowing developers to break tasks into manageable parts. By breaking down machine learning  systems into smaller units, data scientists can simplify the development process and  encourage iterative testing. For example, in image classification projects, micro models can be trained to identify objects or  features within images – e.g., facial expressions. These specialized models can then be  incorporated into larger systems to carry out tasks such as sentiment analysis or content  moderation. Conclusion "
            },
            {
                "page_number": 11,
                "text": "The progress of ML and AI greatly depends on the quality and variety of training data. Training  data enhances the learning abilities of ML models and ensures their relevance in different real- life situations. Ensuring the accuracy, relevance, and diversity of training data is of great importance for the  development of AI systems that are robust, fair, and effective. "
            }
        ],
        "images": [
            "Image_16",
            "Image_25",
            "Image_33",
            "Image_36"
        ]
    },
    {
        "file_name": "transformer_model1.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "What Is a Transformer Model? A transformer model is a neural network that learns context and thus meaning by tracking  relationships in sequential data like the words in this sentence. If you want to ride the next big wave in AI, grab a transformer. They’re not the shape-shifting toy robots on TV or the trash-can-sized tubs on telephone poles. So, What’s a Transformer Model? A transformer model is a neural network that learns context and thus meaning by tracking  relationships in sequential data like the words in this sentence. Transformer models apply an evolving set of mathematical techniques, called attention or self- attention, to detect subtle ways even distant data elements in a series influence and depend on  each other. First described in a 2017 paper from Google, transformers are among the newest and one of the  most powerful classes of models invented to date. They’re driving a wave of advances in  machine learning some have dubbed transformer AI. Stanford researchers called transformers “foundation models” in an August 2021 paper  because they see them driving a paradigm shift in AI. The “sheer scale and scope of foundation  models over the last few years have stretched our imagination of what is possible,” they wrote. What Can Transformer Models Do? Transformers are translating text and speech in near real-time, opening meetings and  classrooms to diverse and hearing-impaired attendees. They’re helping researchers understand the chains of genes in DNA and amino acids in proteins  in ways that can speed drug design. Transformers can detect trends and anomalies to prevent fraud, streamline manufacturing,  make online recommendations or improve healthcare. People use transformers every time they search on Google or Microsoft Bing. The Virtuous Cycle of Transformer AI Any application using sequential text, image or video data is a candidate for transformer  models. That enables these models to ride a virtuous cycle in transformer AI. Created with large  datasets, transformers make accurate predictions that drive their wider use, generating more  data that can be used to create even better models. “Transformers made self-supervised learning possible, and AI jumped to warp speed,” said  NVIDIA founder and CEO Jensen Huang in his keynote address this week at GTC. Transformers Replace CNNs, RNNs Transformers are in many cases replacing convolutional and recurrent neural networks (CNNs  and RNNs), the most popular types of deep learning models just five years ago. "
            },
            {
                "page_number": 2,
                "text": "Indeed, 70 percent of arXiv papers on AI posted in the last two years mention transformers.  That’s a radical shift from a 2017 IEEE study that reported RNNs and CNNs were the most  popular models for pattern recognition. No Labels, More Performance Before transformers arrived, users had to train neural networks with large, labeled datasets that  were costly and time-consuming to produce. By finding patterns between elements  mathematically, transformers eliminate that need, making available the trillions of images and  petabytes of text data on the web and in corporate databases. In addition, the math that transformers use lends itself to parallel processing, so these models  can run fast. Transformers now dominate popular performance leaderboards like SuperGLUE, a benchmark  developed in 2019 for language-processing systems. How Transformers Pay Attention Like most neural networks, transformer models are basically large encoder/decoder blocks that  process data. Small but strategic additions to these blocks (shown in the diagram below) make transformers  uniquely powerful. A look under the hood from a presentation by Aidan Gomez, one of eight co-authors of the 2017  paper that defined transformers. Transformers use positional encoders to tag data elements coming in and out of the network.  Attention units follow these tags, calculating a kind of algebraic map of how each element  relates to the others. Attention queries are typically executed in parallel by calculating a matrix of equations in what’s  called multi-headed attention. "
            },
            {
                "page_number": 3,
                "text": "With these tools, computers can see the same patterns humans see. Self-Attention Finds Meaning For example, in the sentence: She poured water from the pitcher to the cup until it was full. We know “it” refers to the cup, while in the sentence: She poured water from the pitcher to the cup until it was empty. We know “it” refers to the pitcher. “Meaning is a result of relationships between things, and self-attention is a general way of  learning relationships,” said Ashish Vaswani, a former senior staff research scientist at Google  Brain who led work on the seminal 2017 paper. “Machine translation was a good vehicle to validate self-attention because you needed short-  and long-distance relationships among words,” said Vaswani. “Now we see self-attention is a powerful, flexible tool for learning,” he added. How Transformers Got Their Name Attention is so key to transformers the Google researchers almost used the term as the name  for their 2017 model. Almost. “Attention Net didn’t sound very exciting,” said Vaswani, who started working with neural nets in  2011. .Jakob Uszkoreit, a senior software engineer on the team, came up with the name Transformer. “I argued we were transforming representations, but that was just playing semantics,” Vaswani  said. The Birth of Transformers In the paper for the 2017 NeurIPS conference, the Google team described their transformer and  the accuracy records it set for machine translation. Thanks to a basket of techniques, they trained their model in just 3.5 days on eight NVIDIA  GPUs, a small fraction of the time and cost of training prior models. They trained it on datasets  with up to a billion pairs of words. “It was an intense three-month sprint to the paper submission date,” recalled Aidan Gomez, a  Google intern in 2017 who contributed to the work. “The night we were submitting, Ashish and I pulled an all-nighter at Google,” he said. “I caught a  couple hours sleep in one of the small conference rooms, and I woke up just in time for the  submission when someone coming in early to work opened the door and hit my head.” It was a wakeup call in more ways than one. “Ashish told me that night he was convinced this was going to be a huge deal, something game  changing. I wasn’t convinced, I thought it would be a modest gain on a benchmark, but it turned "
            },
            {
                "page_number": 4,
                "text": "out he was very right,” said Gomez, now CEO of startup Cohere that’s providing a language  processing service based on transformers. A Moment for Machine Learning Vaswani recalls the excitement of seeing the results surpass similar work published by a  Facebook team using CNNs. “I could see this would likely be an important moment in machine learning,” he said. A year later, another Google team tried processing text sequences both forward and backward  with a transformer. That helped capture more relationships among words, improving the  model’s ability to understand the meaning of a sentence. Their Bidirectional Encoder Representations from Transformers (BERT) model set 11 new  records and became part of the algorithm behind Google search. Within weeks, researchers around the world were adapting BERT for use cases across many  languages and industries “because text is one of the most common data types companies  have,” said Anders Arpteg, a 20-year veteran of machine learning research. Putting Transformers to Work Soon transformer models were being adapted for science and healthcare. DeepMind, in London, advanced the understanding of proteins, the building blocks of life, using  a transformer called AlphaFold2, described in a recent Nature article. It processed amino acid  chains like text strings to set a new watermark for describing how proteins fold, work that could  speed drug discovery. AstraZeneca and NVIDIA developed MegaMolBART, a transformer tailored for drug discovery.  It’s a version of the pharmaceutical company’s MolBART transformer, trained on a large,  unlabeled database of chemical compounds using the NVIDIA Megatron framework for building  large-scale transformer models. Reading Molecules, Medical Records “Just as AI language models can learn the relationships between words in a sentence, our aim is  that neural networks trained on molecular structure data will be able to learn the relationships  between atoms in real-world molecules,” said Ola Engkvist, head of molecular AI, discovery  sciences and R&D at AstraZeneca, when the work was announced last year. Separately, the University of Florida’s academic health center collaborated with NVIDIA  researchers to create GatorTron. The transformer model aims to extract insights from massive  volumes of clinical data to accelerate medical research. Transformers Grow Up Along the way, researchers found larger transformers performed better. For example, researchers from the Rostlab at the Technical University of Munich, which helped  pioneer work at the intersection of AI and biology, used natural-language processing to  understand proteins. In 18 months, they graduated from using RNNs with 90 million parameters  to transformer models with 567 million parameters. "
            },
            {
                "page_number": 5,
                "text": "Rostlab researchers show  language models trained without labeled samples picking up the signal of a protein sequence. The OpenAI lab showed bigger is better with its Generative Pretrained Transformer (GPT). The  latest version, GPT-3, has 175 billion parameters, up from 1.5 billion for GPT-2. With the extra heft, GPT-3 can respond to a user’s query even on tasks it was not specifically  trained to handle. It’s already being used by companies including Cisco, IBM and Salesforce. Tale of a Mega Transformer NVIDIA and Microsoft hit a high watermark in November, announcing the Megatron-Turing  Natural Language Generation model (MT-NLG) with 530 billion parameters. It debuted along  with a new framework, NVIDIA NeMo Megatron, that aims to let any business create its own  billion- or trillion-parameter transformers to power custom chatbots, personal assistants and  other AI applications that understand language. MT-NLG had its public debut as the brain for TJ, the Toy Jensen avatar that gave part of the  keynote at NVIDIA’s November 2021 GTC. “When we saw TJ answer questions — the power of our work demonstrated by our CEO — that  was exciting,” said Mostofa Patwary, who led the NVIDIA team that trained the model. "
            },
            {
                "page_number": 6,
                "text": "“Megatron helps me answer all those  tough questions Jensen throws at me,” TJ said at GTC 2022. Creating such models is not for the faint of heart. MT-NLG was trained using hundreds of billions  of data elements, a process that required thousands of GPUs running for weeks. “Training large transformer models is expensive and time-consuming, so if you’re not  successful the first or second time, projects might be canceled,” said Patwary. Trillion-Parameter Transformers Today, many AI engineers are working on trillion-parameter transformers and applications for  them. “We’re constantly exploring how these big models can deliver better applications. We also  investigate in what aspects they fail, so we can build even better and bigger ones,” Patwary said. To provide the computing muscle those models need, our latest accelerator — the NVIDIA H100  Tensor Core GPU — packs a Transformer Engine and supports a new FP8 format. That speeds  training while preserving accuracy. With those and other advances, “transformer model training can be reduced from weeks to  days” said Huang at GTC. MoE Means More for Transformers Last year, Google researchers described the Switch Transformer, one of the first trillion- parameter models. It uses AI sparsity, a complex mixture-of experts (MoE) architecture and  other advances to drive performance gains in language processing and up to 7x increases in pre- training speed. "
            },
            {
                "page_number": 7,
                "text": "The encoder for the Switch Transformer, the first model to have up to a trillion parameters. For its part, Microsoft Azure worked with NVIDIA to implement an MoE transformer for its  Translator service. Tackling Transformers’ Challenges Now some researchers aim to develop simpler transformers with fewer parameters that deliver  performance similar to the largest models. “I see promise in retrieval-based models that I’m super excited about because they could bend  the curve,” said Gomez, of Cohere, noting the Retro model from DeepMind as an example. Retrieval-based models learn by submitting queries to a database. “It’s cool because you can  be choosy about what you put in that knowledge base,” he said. "
            },
            {
                "page_number": 8,
                "text": "In the race for higher performance, transformer models have grown larger. The ultimate goal is to “make these models learn like humans do from context in the real world  with very little data,” said Vaswani, now co-founder of a stealth AI startup. He imagines future models that do more computation upfront so they need less data and sport  better ways users can give them feedback. “Our goal is to build models that will help people in their everyday lives,” he said of his new  venture. Safe, Responsible Models Other researchers are studying ways to eliminate bias or toxicity if models amplify wrong or  harmful language. For example, Stanford created the Center for Research on Foundation  Models to explore these issues. “These are important problems that need to be solved for safe deployment of models,” said  Shrimai Prabhumoye, a research scientist at NVIDIA who’s among many across the industry  working in the area. “Today, most models look for certain words or phrases, but in real life these issues may come  out subtly, so we have to consider the whole context,” added Prabhumoye. “That’s a primary concern for Cohere, too,” said Gomez. “No one is going to use these models if  they hurt people, so it’s table stakes to make the safest and most responsible models.” "
            },
            {
                "page_number": 9,
                "text": "Beyond the Horizon Vaswani imagines a future where self-learning, attention-powered transformers approach the  holy grail of AI. “We have a chance of achieving some of the goals people talked about when they coined the  term ‘general artificial intelligence’ and I find that north star very inspiring,” he said. “We are in a time where simple methods like neural networks are giving us an explosion of new  capabilities.” Transformer training and inference will get significantly accelerated with the NVIDIA H100 GPU. "
            }
        ],
        "images": [
            "Image_31",
            "Image_59",
            "Image_67",
            "Image_74",
            "Image_80",
            "Image_84"
        ]
    },
    {
        "file_name": "transormer_model2.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "What are Transformers in NLP? Transformers in Natural Language Processing (NLP) is a model architecture that revolutionized  the field of NLP, leading to remarkable improvements in language understanding tasks.  Transformers leverage attention mechanisms that are designed to capture dependencies in the  input data, irrespective of their distance from each other. History The Transformers architecture was first introduced in a paper titled 'Attention is All You Need'  by Vaswani et al., in 2017. It signifies a departure from traditional sequential processing  methods, replacing them with parallel processing for improved efficiency. Functionality and Features Transformers use a unique mechanism known as 'attention' to weigh the importance of  different words in input data. Key features include:    Self-attention: Measures dependencies between all words in a sentence, irrespective of  their distance.    Encoder-decoder structure: Consists of an encoder that processes the input and a  decoder that produces the output.    Positional encoding: Injects information about position of words in the sequence. Architecture Transformers in NLP are composed of three main parts: an encoder, decoder, and a final linear  and softmax layer. The encoder and decoder are stacks of identical layers, each with two sub- layers: a multi-head self-attention layer & a simple, position-wise fully connected feed-forward  network. Benefits and Use Cases Transformers have been used to achieve state-of-the-art results on a variety of NLP tasks, such  as translation, summarization, and sentiment analysis. They offer several advantages:    Handle long-term dependencies better than RNNs and LSTMs.    Parallelization leads to faster training.    Improved accuracy on several NLP benchmarks. Challenges and Limitations Despite their advantages, Transformers face some challenges like shorter attention span for  longer sequences, and high resource consumption considering memory and computational  power. Integration with Data Lakehouse Transformers can be integrated into a data lakehouse setup for text data analytics. They  empower data scientists to extract insights from massive unstructured text data stored in a data  lake, enabling state-of-the-art NLP analytics in a data lakehouse environment. Security Aspects "
            },
            {
                "page_number": 2,
                "text": "The security of Transformers in NLP is mostly dependent on the data and systems they are used  with. While they don't inherently provide any security features, it's important to ensure data  privacy and protection when dealing with sensitive text data. Performance Transformers are considered high-performing models by delivering state-of-the-art results on  many NLP tasks. However, their resource-intensive nature can sometimes be a downside in  constrained environments. FAQs What are Transformers in NLP? Standard Transformers are deep learning models used in NLP  that provide improved handling of sequence data through self-attention mechanisms. Why are Transformers important in NLP? They have led to breakthrough results in various NLP  tasks by capturing long-distance dependencies in text better than previous models. How do Transformers fit into a data lakehouse environment? Transformers can analyze and  extract insights from raw, unstructured text data found in data lakes, making them compatible  with a data lakehouse setup. What’s the difference between Transformers and RNNs? RNNs process words sequentially,  while Transformers process all words in parallel. Transformers also handle long-distance  dependencies better. What are the limitations of Transformers? They can be resource-intensive due to their high  computational and memory needs, and can struggle with attention span for longer sequences. Glossary Attention Mechanism: A process that assigns different weights to different parts of the input  data. Encoder/Decoder: The two components of the Transformer model that process the input and  produce the output. Self-Attention: A method by which a Transformer weighs the importance of different words in  the input data. Data Lakehouse: A hybrid data management platform combining the features of data lakes and  data warehouses. NLP: Natural Language Processing, a sub-field of artificial intelligence that focuses on the  interaction between computers and humans through natural language. "
            }
        ],
        "images": []
    },
    {
        "file_name": "ultimate rag.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "The Ultimate Guide to Retrieval-Augmented Generation (RAG) Remember the last time you asked chaGPT a question and it didn’t give you a satisfying  answer or it right off the bat said something that starts with “As of my last knowledge  update…” Why does this happen? ChatGPT, like all AI based on traditional language models, depends  on its training data, which may not include the latest information. What if you could enhance these models not by constant retraining, but by allowing them  to access fresh, real-world data when needed? This idea, known as Retrieval Augmented  Generation (RAG), represents an exciting development in AI, offering a way for machines to  stay current and more effectively answer complex questions. Understanding Retrieval-Augmented Generation Retrieval-Augmented Generation, commonly known as RAG, represents a groundbreaking  approach in artificial intelligence (AI) and natural language processing (NLP). This innovative framework integrates the capabilities of both retrieval-based and  generative models, significantly enhancing how AI systems interpret and generate text akin  to human speech. Specifically designed to refine the accuracy of responses from large  language models (LLMs), RAG extends the model's internal data representation by  leveraging external sources of knowledge. The deployment of RAG in LLM-driven question answering systems delivers significant  benefits: it ensures the model has access to the latest, verifiable facts, and it fosters  transparency by allowing users to review the sources, thereby boosting the  trustworthiness of the model's outputs. At its heart, RAG skillfully combines the capabilities of retrieval-based models, which  gather external information, with the abilities of generative models to create natural  language. RAG models excel beyond traditional language models in knowledge-rich activities such as  answering questions by enriching them with the information they retrieve, thereby  producing more informed and accurate responses. There are two fundamental components in the RAG architecture: a retriever and a  generator. Let's take a closer look at how each one plays a pivotal role in the functioning of  a RAG system. "
            },
            {
                "page_number": 2,
                "text": "1. Entity annotation The retriever module helps by finding the most appropriate information from a dataset  when it receives a query. It uses vectors from text embeddings to do this effectively. Essentially, it processes the query and pulls the most relevant information from a set of  semantic search vectors. Plus, they function as distinct models, but unlike language models, they don't engage in  'training' or typical machine learning processes. Instead, they act more like  enhancements or add-ons that supply extra context for comprehension and specialized  features for efficiently fetching information. 2. Generator Once the retriever locates relevant information, it must be relayed back to the application  and presented to the user. Alternatively, a generator is required that can transform the  retrieved data into content that is understandable for human readers. Behind the scenes, the generator takes the embeddings provided by the retriever,  combines them with the original query, and then processes them through a trained  language model for a natural language processing (NLP) pass, ultimately transforming  them into generated text. That said, here is a step by step process of how RAG flow goes. i.  A request is initiated. ii.  This request is forwarded to the RAG model. iii.  The RAG model transforms the request into textual embeddings, which are then  matched against a dataset. iv.  Using its semantic search capabilities, the RAG's retriever identifies the most  pertinent information and converts it into vector embeddings. v.  These parsed embeddings are relayed by the RAG’s retriever to the generator. vi.  The generator receives the embeddings and merges them with the initial request. vii.  Finally, the generator hands over the task to the language model, which crafts  content that sounds natural and is delivered to the user. "
            },
            {
                "page_number": 3,
                "text": "What is semantic search? Semantic search transcends traditional keyword-based search methods, which depend on  detecting specific indexed terms within the search query. Instead, it delves into the  contextual relevance of data by analyzing the conceptual resemblance of the input text. Consequently, it has proven to be an effective tool for enriching models with deeper  context, as these queries often require substantial contextual understanding. Semantic search uses a vector database that keeps track of text snippets (pulled from  various documents) and their vectors, which are essentially numerical versions of the text.  When you send a query in vector form to this database, it’s compared with all the stored  vectors to find and return the text snippets that best match up. Imagine you're using a retrieval-augmented generation (RAG) model in a customer service  chatbot designed for a large electronics retailer. Here’s how a semantic search RAG might  function in this scenario: User Query: 'I bought a laptop last month, and it's already overheating. What should I do?' Step 1: Query Processing The chatbot receives the user's query about an overheating laptop. Step 2: Semantic Search The RAG model uses semantic search to understand the context beyond the literal  keywords like 'laptop' and 'overheating.' It looks for conceptually similar issues and  solutions in its database. The search might pull up information snippets about common causes of laptop  overheating, warranty information, and typical troubleshooting steps. Step 3: Retrieval "
            },
            {
                "page_number": 4,
                "text": "The model retrieves documents or text snippets that include: o  Guidelines on safe laptop use to prevent overheating. o  Customer service responses for warranty claims related to overheating. o  Steps to troubleshoot overheating issues. Step 4: Response Generation Using the retrieved information, the RAG model generates a comprehensive response that  might include: o  Advising the user to ensure the laptop's vents are not blocked and to use the  laptop on a hard, flat surface to improve air circulation. o  Suggesting the user to check if the laptop is still under warranty and eligible  for a service check-up or replacement. o  Recommending a visit to a service center if the problem persists, ensuring  the advice is tailored to the user's warranty status and previous  troubleshooting attempts. o  This example shows how semantic search within a RAG framework can  enhance the relevance and accuracy of responses in customer service by  dynamically incorporating up-to-date and contextually appropriate  information. Benefits of RAG Retrieval Augmented Generation (RAG) offers several benefits that enhance its  effectiveness in generating information and text content. Here are five key advantages: o  Improved Accuracy and Relevance: RAG leverages a retrieval component to  fetch relevant documents or data that inform its responses, leading to more  accurate and contextually appropriate content. This makes it particularly  useful for tasks requiring detailed or domain-specific information. o  <='' strong=''> Unlike traditional models that generate responses based  solely on pre-learned patterns, RAG can access a vast corpus of information  in real-time. This ability to reference current, external data sources ensures  the generation of responses that are not only relevant but also rich in factual  details. o  Dynamic Learning: By integrating retrieval into the generation process, RAG  models can dynamically incorporate the latest information without needing  frequent retraining. This allows the models to remain up-to-date with new  knowledge and trends. o  Scalability: The retrieval component allows RAG models to effectively  manage and utilize large datasets, which might be impractical for traditional  models to embed directly within their parameters. This scalability is critical  for applications requiring access to extensive databases or libraries of  information. "
            },
            {
                "page_number": 5,
                "text": "o  Customization and Flexibility: The retrieval step in RAG allows for the  customization of the information sources it uses, which can be tailored to  specific domains or types of queries. This flexibility enables more effective  responses across a diverse range of topics and industries. Retrieval Augmented Generation (RAG) Use Cases Retrieval Augmented Generation (RAG) is a versatile technology with a wide array of  practical applications across various industries and fields. Here are some use cases where  RAG can be particularly effective: o  Question Answering Systems: RAG can enhance question answering  systems by providing precise, well-informed answers sourced from a broad  information database. This is particularly useful in customer support bots,  educational tools, and informational kiosks where accurate, detailed  responses are necessary. o  Content Creation and Summarization: In media and content production,  RAG can assist in generating articles, reports, and summaries by retrieving  relevant facts and figures, ensuring the content is both accurate and rich in  information. This helps content creators maintain factual integrity while  reducing research time. o  <='' strong=''> RAG can be employed to pull relevant case laws, prior  judgments, or medical research papers to assist professionals in these  fields. For lawyers, it can provide precedents and for medical professionals,  it can offer treatment histories and research outcomes, enhancing decision- making. o  language translation and localization: By accessing a diverse set of sources,  RAG can improve translation accuracy and context relevance in language  translation services. It can particularly enhance localization efforts by  retrieving culturally and contextually relevant information. o  Educational Tools and Tutoring: RAG can power educational platforms and  tutoring systems that provide students with customized explanations and  information retrieval based on specific queries. This supports personalized  learning and can help clarify complex subjects by providing detailed,  context-specific information. o  Interactive Entertainment and Gaming: In interactive narratives and gaming,  RAG can generate dynamic dialogues and story developments based on  player choices and game state, retrieving information that makes the  experience more immersive and responsive. Each of these applications demonstrates the flexibility and potential of RAG to transform  how information is retrieved and used to generate meaningful, context-aware content  across different domains. Conclusion "
            },
            {
                "page_number": 6,
                "text": "Retrieval-Augmented Generation models mark a thrilling leap forward in artificial  intelligence. These models go beyond simple keyword matching to grasp complex  concepts, providing incredibly relevant and context-aware responses. This advancement is transforming user experiences across multiple domains, from  customer service to educational platforms, ensuring responses are not only timely but  exceptionally accurate. As this technology progresses, we can expect even more  sophisticated tools capable of navigating the nuances of human language with  unprecedented precision. "
            }
        ],
        "images": [
            "Image_14",
            "Image_22"
        ]
    },
    {
        "file_name": "unsupervised.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "What is unsupervised learning? Unsupervised learning, also known as unsupervised machine learning, uses machine  learning (ML) algorithms to analyze and cluster unlabeled data sets. These algorithms  discover hidden patterns or data groupings without the need for human intervention. Unsupervised learning's ability to discover similarities and differences in information make it  the ideal solution for exploratory data analysis, cross-selling strategies, customer  segmentation and image recognition. Common unsupervised learning approaches Unsupervised learning models are utilized for three main tasks—clustering, association,  and dimensionality reduction. Below we’ll define each learning method and highlight  common algorithms and approaches to conduct them effectively. Clustering Clustering is a data mining technique which groups unlabeled data based on their  similarities or differences. Clustering algorithms are used to process raw, unclassified data  objects into groups represented by structures or patterns in the information. Clustering  algorithms can be categorized into a few types, specifically exclusive, overlapping,  hierarchical, and probabilistic. Exclusive and Overlapping Clustering Exclusive clustering is a form of grouping that stipulates a data point can exist only in one  cluster. This can also be referred to as “hard” clustering. The K-means clustering algorithm  is an example of exclusive clustering.    K-means clustering is a common example of an exclusive clustering method where  data points are assigned into K groups, where K represents the number of clusters based  on the distance from each group’s centroid. The data points closest to a given centroid  will be clustered under the same category. A larger K value will be indicative of smaller  groupings with more granularity whereas a smaller K value will have larger groupings and  less granularity. K-means clustering is commonly used in market segmentation,  document clustering, image segmentation, and image compression. Overlapping clusters differs from exclusive clustering in that it allows data points to belong  to multiple clusters with separate degrees of membership. “Soft” or fuzzy k-means  clustering is an example of overlapping clustering. Hierarchical clustering Hierarchical clustering, also known as hierarchical cluster analysis (HCA), is an  unsupervised clustering algorithm that can be categorized in two ways: agglomerative or  divisive. Agglomerative clustering is considered a “bottoms-up approach.” Its data points are  isolated as separate groupings initially, and then they are merged together iteratively on the  basis of similarity until one cluster has been achieved. Four different methods are  commonly used to measure similarity: "
            },
            {
                "page_number": 2,
                "text": "1. Ward’s linkage: This method states that the distance between two clusters is defined by the increase in the sum of squared after the clusters are merged. 2. Average linkage: This method is defined by the mean distance between two points in each cluster. 3. Complete (or maximum) linkage: This method is defined by the maximum distance between two points in each cluster. 4. Single (or minimum) linkage: This method is defined by the minimum distance between two points in each cluster. Euclidean distance is the most common metric used to calculate these distances; however,  other metrics, such as Manhattan distance, are also cited in clustering literature. Divisive clustering can be defined as the opposite of agglomerative clustering; instead it takes  a “top-down” approach. In this case, a single data cluster is divided based on the differences  between data points. Divisive clustering is not commonly used, but it is still worth noting in  the context of hierarchical clustering. These clustering processes are usually visualized using  a dendrogram, a tree-like diagram that documents the merging or splitting of data points at  each iteration. Probabilistic clustering A probabilistic model is an unsupervised technique that helps us solve density estimation or  “soft” clustering problems. In probabilistic clustering, data points are clustered based on  the likelihood that they belong to a particular distribution. The Gaussian Mixture Model  (GMM) is the one of the most commonly used probabilistic clustering methods.    Gaussian Mixture Models are classified as mixture models, which means that they are  made up of an unspecified number of probability distribution functions. GMMs are  primarily leveraged to determine which Gaussian, or normal, probability distribution a  given data point belongs to. If the mean or variance are known, then we can determine  which distribution a given data point belongs to. However, in GMMs, these variables are  not known, so we assume that a latent, or hidden, variable exists to cluster data points  appropriately. While it is not required to use the Expectation-Maximization (EM)  algorithm, it is a commonly used to estimate the assignment probabilities for a given  data point to a particular data cluster. Association Rules An association rule is a rule-based method for finding relationships between variables in a  given dataset. These methods are frequently used for market basket analysis, allowing  companies to better understand relationships between different products. Understanding  consumption habits of customers enables businesses to develop better cross-selling  strategies and recommendation engines. Examples of this can be seen in Amazon’s  “Customers Who Bought This Item Also Bought” or Spotify’s 'Discover Weekly'  playlist. While there are a few different algorithms used to generate association rules, such  as Apriori, Eclat, and FP-Growth, the Apriori algorithm is most widely used. Apriori algorithms "
            },
            {
                "page_number": 3,
                "text": "   Apriori algorithms have been popularized through market basket analyses, leading to  different recommendation engines for music platforms and online retailers. They are  used within transactional datasets to identify frequent itemsets, or collections of items,  to identify the likelihood of consuming a product given the consumption of another  product. For example, if I play Black Sabbath’s radio on Spotify, starting with their  song “Orchid”, one of the other songs on this channel will likely be a Led Zeppelin  song, such as “Over the Hills and Far Away.” This is based on my prior listening  habits as well as the ones of others. Apriori algorithms use a hash tree to count  itemsets, navigating through the dataset in a breadth-first manner. Dimensionality reduction While more data generally yields more accurate results, it can also impact the performance of  machine learning algorithms (e.g. overfitting) and it can also make it difficult to visualize  datasets. Dimensionality reduction is a technique used when the number of features, or  dimensions, in a given dataset is too high. It reduces the number of data inputs to a  manageable size while also preserving the integrity of the dataset as much as possible. It is  commonly used in the preprocessing data stage, and there are a few different dimensionality  reduction methods that can be used, such as: Principal component analysis Principal component analysis (PCA) is a type of dimensionality reduction algorithm which is  used to reduce redundancies and to compress datasets through feature extraction. This method  uses a linear transformation to create a new data representation, yielding a set of 'principal  components.' The first principal component is the direction which maximizes the variance of  the dataset. While the second principal component also finds the maximum variance in the  data, it is completely uncorrelated to the first principal component, yielding a direction that is  perpendicular, or orthogonal, to the first component. This process repeats based on the  number of dimensions, where a next principal component is the direction orthogonal to the  prior components with the most variance. Singular value decomposition Singular value decomposition (SVD) is another dimensionality reduction approach which  factorizes a matrix, A, into three, low-rank matrices. SVD is denoted by the formula, A = USVT,  where U and V are orthogonal matrices. S is a diagonal matrix, and S values are considered  singular values of matrix A. Similar to PCA, it is commonly used to reduce noise and compress  data, such as image files. Autoencoders Autoencoders leverage neural networks to compress data and then recreate a new  representation of the original data’s input. Looking at the image below, you can see that the  hidden layer specifically acts as a bottleneck to compress the input layer prior to reconstructing  within the output layer. The stage from the input layer to the hidden layer is referred to as  “encoding” while the stage from the hidden layer to the output layer is known as “decoding.” Applications of unsupervised learning "
            },
            {
                "page_number": 4,
                "text": "Machine learning techniques have become a common method to improve a product user  experience and to test systems for quality assurance. Unsupervised learning provides an  exploratory path to view data, allowing businesses to identify patterns in large volumes of data  more quickly when compared to manual observation. Some of the most common real-world  applications of unsupervised learning are:    News Sections: Google News uses unsupervised learning to categorize articles on the  same story from various online news outlets. For example, the results of a presidential  election could be categorized under their label for “US” news.    Computer vision: Unsupervised learning algorithms are used for visual perception  tasks, such as object recognition.    Medical imaging: Unsupervised machine learning provides essential features to  medical imaging devices, such as image detection, classification and segmentation,  used in radiology and pathology to diagnose patients quickly and accurately.    Anomaly detection: Unsupervised learning models can comb through large amounts of  data and discover atypical data points within a dataset. These anomalies can raise  awareness around faulty equipment, human error, or breaches in security.    Customer personas: Defining customer personas makes it easier to understand  common traits and business clients' purchasing habits. Unsupervised learning allows  businesses to build better buyer persona profiles, enabling organizations to align their  product messaging more appropriately.    Recommendation Engines: Using past purchase behavior data, unsupervised learning  can help to discover data trends that can be used to develop more effective cross- selling strategies. This is used to make relevant add-on recommendations to customers  during the checkout process for online retailers. Unsupervised vs. supervised vs. semi-supervised learning Unsupervised learning and supervised learning are frequently discussed together. Unlike  unsupervised learning algorithms, supervised learning algorithms use labeled data. From that  data, it either predicts future outcomes or assigns data to specific categories based on the  regression or classification problem that it is trying to solve. While supervised learning  algorithms tend to be more accurate than unsupervised learning models, they require upfront  human intervention to label the data appropriately. However, these labelled datasets allow  supervised learning algorithms to avoid computational complexity as they don’t need a large  training set to produce intended outcomes. Common regression and classification techniques  are linear and logistic regression, naïve bayes, KNN algorithm, and random forest. Semi-supervised learning occurs when only part of the given input data has been labelled.  Unsupervised and semi-supervised learning can be more appealing alternatives as it can be  time-consuming and costly to rely on domain expertise to label data appropriately for  supervised learning. For a deep dive into the differences between these approaches, check out 'Supervised vs.  Unsupervised Learning: What's the Difference?' Challenges of unsupervised learning "
            },
            {
                "page_number": 5,
                "text": "While unsupervised learning has many benefits, some challenges can occur when it allows  machine learning models to execute without any human intervention. Some of these challenges  can include:    Computational complexity due to a high volume of training data    Longer training times    Higher risk of inaccurate results    Human intervention to validate output variables    Lack of transparency into the basis on which data was clustered "
            }
        ],
        "images": []
    },
    {
        "file_name": "vector_search.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "What is Vector Search Imagine you’re a software developer looking for database optimization techniques, especially to  boost the efficiency of queries in large-scale databases. In a traditional SQL database, you  might use keywords like “B-Tree indexing” or simply “Indexing” to find related blogs or articles.  However, this keyword-based approach might overlook important blogs or articles that use  different but related phrases, such as “SQL tuning” or “indexing strategies”. Consider another scenario where you’re aware of the context but not the exact name of a  specific technique. Traditional databases, being reliant on exact keyword matches, fall short in  such situations as they cannot search based on context alone. What we need, then, is a search technique that goes beyond simple keyword matching and  delivers results based on semantic similarities. This is where vector search comes into play.  Unlike traditional keyword matching techniques, vector search compares your query’s  semantics with the database entries, returning more relevant and accurate results. In this blog, we’ll discuss everything related to vector search, starting with the basic concepts  and then moving on to more advanced techniques. Let’s begin by overviewing vector search. An Overview of Vector Search Vector search is a sophisticated data retrieval technique that focuses on matching the  contextual meanings of search queries and data entries, rather than simple text matching. To  implement this technique, we must first convert both the search query and a specific column of  the dataset into numerical representations, known as vector embeddings. Then, we calculate  the distance (Cosine similarity or Euclidean distance) between the query vector and the vector  embeddings in the database. Next, we then identify the closest or most similar entries based on  these calculated distances. Lastly, we return the top-k results with the smallest distances to the  query vector. "
            },
            {
                "page_number": 2,
                "text": "Typical Scenarios for Vector Search    Similarity Search: Use to find other vectors in the feature space that are similar to a  given vector, widely applied in fields such as image, audio, and text analysis.    Recommendation Systems: Achieve personalized recommendations by analyzing  vector representations of users and items, such as movie, product, or music  recommendations.    Natural Language Processing: Search for semantic similarity in text data, supporting  semantic search and relevance analysis.    Question-Answering (QA) System: Search for related passages whose vector  representations are most similar to the input question. The final answer can be  generated with a Large Language Model (LLM) based on the question and the retrieved  passages. Brute-force vector search works really well for semantic search when the dataset is small and  your queries are simple. However, its performance decreases as the dataset grows or the  queries become more complex, resulting in a few drawbacks. Challenges in Implementing Vector Search Let’s discuss some of the issues associated with using a simple vector search, specifically  when the dataset size increases:    Performance: As discussed above, brute-force vector search computes the distance  between a query vector and all the vectors in the database. It works well for smaller  datasets but as the number of vectors increases to millions of entries, the search time  and the computational cost to find the distance between the millions of entries grows.    Scalability: Data is currently growing exponentially, making it very difficult for brute-force  vector search to achieve results with the same speed and accuracy when querying  massive datasets. This requires innovative ways to manage voluminous data while  maintaining the same speed and accuracy.    Combining with structured data: In simple applications, either a SQL query is used for  querying structured data or a vector search for unstructured data, but applications often  require the capabilities of both. Integrating these two can be technically challenging, "
            },
            {
                "page_number": 3,
                "text": "especially when they are processed in different systems. When we utilize vector search  and simultaneously apply SQL WHERE clauses for filtering, the query processing time  increases as a result of the increased variety and size of data. As a solution to these challenges, efficient vector indexing techniques are available. Common Vector Indexing Techniques To address the challenges of large-scale vector data, various indexing techniques are employed  to organize and facilitate efficient approximate vector searches. Let’s explore some of these  techniques. Hierarchical Navigable Small World (HSNW) The HNSW algorithm leverages a multi-layered graph structure to store and efficiently search  through vectors. At each layer, vectors are connected not only to other vectors on the same  layer but also to vectors in the layers below. This structure allows for efficient exploration of  nearby vectors while keeping the search space manageable. The top layers contain a small  number of nodes, while the number of nodes increases exponentially as we descend the  hierarchy. The bottom layer finally encompasses all data points in the database. This  hierarchical design defines the distinctive architecture of the HNSW algorithm. "
            },
            {
                "page_number": 4,
                "text": "The search process begins with a selected vector, from which it computes the distances to  connected vectors in both the current and preceding layers. This method is greedy,  continuously progressing towards the vector nearest to the current position, iterating until a  vector is identified that is the closest among all its connected vectors. While the HNSW index  typically excels in straightforward vector searches, it demands considerable resources and  requires extensive time for construction. Moreover, the accuracy and efficiency of filtered  searches can substantially decline due to diminished graph connectivity under these  conditions. Inverted Vector File (IVF) Index The IVF index efficiently manages high-dimensional data searches by using cluster centroids as  its inverted index. It segments vectors into clusters based on geometric proximity, with each  cluster’s centroid serving as a simplified representation. When searching for the most similar  items to a query vector, the algorithm first identifies the centroids closest to the query. Then, it  searches only within the associated list of vectors for those centroids, rather than the entire  dataset. IVF take less time to build compared with HSNW, but also achieves lower accuracy and  speed during the search process. "
            },
            {
                "page_number": 5,
                "text": "MyScale in Action: Solutions and Practical Applications As a SQL vector database, MyScale is designed to handle complex queries, allows fast data  retrieval, and store large amounts of data efficiently. What makes it outperform specialized  vector databases is it combines a fast SQL execution engine (based on ClickHouse) with our  proprietary Multi-scale Tree Graph (MSTG) algorithm. MSTG combines the advantages of both  tree and graph-based algorithms, allowing MyScale to build fast and search fast, and maintain  speed and accuracy under different filtered search ratios, all while upholding resource and cost  efficiency. Now let’s take a look at several practical applications where MyScale can be very helpful:    Knowledge based QA applications: When developing a Question-answering (QA)  system, MyScale is an ideal vector database with its ability self-query as well as flexible  filtering for highly relevant results from your documents. Additionally, MyScale excels in  scalability, allowing you to manage multiple users concurrently easily. To learn more,  you can get help from our abstractive QA documentation. Additionally, you can utilize  self-queries with advanced algorithms to improve the accuracy and speed of your  search results.    Large-scale AI chatbot: Developing a large-scale chatbot is a challenging task,  especially when you must manage numerous users concurrently and ensure they are  treated separately. Furthermore, the chatbot must provide accurate answers. MyScale  has simplified building chatbots through its SQL-compatible role-based access control  and large-scale multi-tenancy through data partitioning and filtered search, allowing you  to manage multiple users.    Image search: If you’re creating a system that performs semantic or similar image  search, MyScale can easily accomodate your growing image data while remaining  performant and resource efficient. You can also write more complex SQL and vector join "
            },
            {
                "page_number": 6,
                "text": "queries to match images by metadata or visual content. For more detailed information,  see our image search project documentation. In addition to these practical applications, by incorporating MyScale’s SQL and vector  capabilities, you can develop advanced recommendation systems, object detection  applications, and many more. Conclusion Vector search surpasses traditional term matching by interpreting the semantics within vector  embeddings. This approach is not only effective for text but also extends to images, audio, and  various multi-modal unstructured data, as demonstrated in models like ImageBind. However,  this technology faces challenges such as computational and storage demands, as well as  semantic fuzziness of high-dimensional vectors. MyScale solves these issues by innovatively  merging SQL and vector search into a unified, high-performance, cost-effective system. This  fusion enables a wide array of applications, from QA systems to AI chatbots and image  searches, illustrating its versatility and efficiency. Finally, welcome to connect with us on Twitter and Discord. We love to hear and discuss your  insights. "
            }
        ],
        "images": [
            "Image_16",
            "Image_26",
            "Image_29",
            "Image_32",
            "Image_42"
        ]
    },
    {
        "file_name": "vector_search_ibm.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "What is vector search? Vector search is a search technique used to find similar items or data points, typically  represented as vectors, in large collections. Vectors, or embeddings, are numerical  representations of words, entities, documents, images or videos. Vectors capture the semantic  relationships between elements, enabling effective processing by machine learning models and  artificial intelligence applications. Vector search vs. traditional search In contrast to traditional search, which typically uses keyword search, vector search relies on  vector similarity search techniques like k-nearest neighbor search (knn) to retrieve data points  similar to a query vector based on some distance metric. Vectors capture semantic  relationships and similarities between data points, enabling semantic search instead of simple  keyword search. To illustrate the difference between traditional keyword and vector search, let’s go through an  example. Say you are looking for information on the best pizza restaurant and you search for  “best pizza restaurant” in a traditional keyword search engine. The keyword search looks for  pages that contain the exact words “best”, “pizza” and “restaurant” and only returns results like  “Best Pizza Restaurant” or “Pizza restaurant near me”. Traditional keyword search focuses on  matching the keywords rather than understanding the context or intent behind the search. By contrast, in a semantic vector search, the search engine understands the intent behind the  query. Semantic, by definition, means relating to meaning in language, that is, semantic search  understands the meaning and context of a query. In this case, it would look for content that talks  about top-rated or highly recommended pizza places, even if the exact words 'best pizza  restaurant' are not used in the content. The results are more contextually relevant and might  include articles or guides that discuss high quality pizza places in various locations. Traditional search methods typically represent data using discrete tokens or features, such as  keywords, tags or metadata. As shown in our example above, these methods rely on exact  matches to retrieve relevant results. By contrast, vector search represents data as dense  vectors (a vector in which most or all of the elements are non-zero) in a continuous vector  space, the mathematical space in which data is represented as vectors. Each dimension of the  dense vector corresponds to a latent feature or aspect of the data, an underlying characteristic  or attribute that is not directly observed but is inferred from the data through mathematical  models or algorithms. These latent features capture the hidden patterns and relationships in the  data, enabling more meaningful and accurate representations of items as vectors in a high- dimensional space. Traditional search methods may struggle with scalability for large datasets or high-dimensional  data due to computational and memory constraints. By contrast, vector embeddings are easier  to scale to larger datasets and more complex models. Unlike sparse representations of data  where most of the values are zeros across dimensions, embeddings are dense vector  representations having non-zero values in most dimensions. This allows vector embeddings to  store more information in a smaller, lower-dimensional space, requiring less memory.1 As a  result, machine learning algorithms and models can use embeddings more efficiently with  fewer compute resources. Vectorization proces "
            },
            {
                "page_number": 2,
                "text": "For this explainer, we will focus on the vector representations applicable under natural  language processing (NLP), that is, vectors that represent words, entities or documents. We will illustrate the vectorization process by vectorizing a small corpus of sentences: “the cat  sat on the mat”, “the dog played in the yard” and “birds chirped in the trees”. The first step to building vector embeddings is to clean and process the raw dataset. This may  involve the removal of noise and standardization of the text. For our example, we won’t do any  cleaning since the text is already cleaned and standardized. Next, an embedding model is chosen to be trained on the dataset. The trained embedding  model is used to generate embeddings for each data point in the dataset. For text data, popular  open-source embedding models include Word2Vec, GloVe, FastText or pre-trained transformer- based models like BERT or RoBERTa2. For our example, we’ll use Word2Vec to generate our embeddings. Next, the embeddings are stored in a vector database or a vector search plugin for a search  engine, like Elasticsearch, is used. In vector search, relevance of a search result is established  by assessing the similarity between the query vector, which is generated by vectorizing the  query, and the document vector, which is a representation of the data being queried. Indexes  need to be created in the vector database to enable fast and efficient retrieval of embeddings  based on similar queries. Techniques such as hierarchical navigable small world (HNSW) can  be used to index the embeddings and facilitate similarity search at query time. HNSW organizes  the dataset and enables rapid search for nearest neighbors by clustering similar vectors  together during the index construction process. Finally, a mechanism or procedure to generate vectors for new queries must be established.  This typically involves creating an API or service that takes user search queries as input in real- time, processes it using the same vector model and generates a corresponding vector  representation. This vector can then be used to search on the database to get the most relevant  results. Finding similarity with distance measurements and ANN algorithms. In vector search, relevance is determined by measuring the similarity between query and  document vectors. To compare two vectors against each other and determine their similarity,  some distance measurement may be used, such as Euclidean distance or cosine similarity3. Euclidean distance Euclidean distance is a measure of the straight-line distance between two points. It is  calculated as the square root of the sum of the squared differences between the corresponding  coordinates of the two points. This formula can be extended to higher-dimensional spaces by adding more terms to account  for additional dimensions. Cosine similarity Cosine similarity is a measure of similarity between two vectors in a multi-dimensional space. It  calculates the cosine of the angle between the two vectors, indicating how closely the vectors  align with each other. "
            },
            {
                "page_number": 3,
                "text": "Mathematically, the cosine similarity, cos(θ), between two vectors is calculated as the dot  product of the two vectors divided by the product of their magnitudes. Cosine similarity ranges from -1 to 1, where:    1 indicates that the vectors are perfectly aligned (pointing in the same direction),    0 indicates that the vectors are orthogonal (perpendicular to each other) and    -1 indicates that the vectors are pointing in opposite directions. Cosine similarity is particularly useful when dealing with vectors, as it focuses on the directional  relationship between vectors rather than their magnitudes. Approximate-nearest neighbor (ANN) Although the distance metrics mentioned previously can be used to measure vector similarity, it  becomes inefficient and slow to compare all possible vectors against the query vector at query  time for similarity search. To solve for this, we can use an approximate-nearest neighbor (ANN)  search. Instead of finding an exact match, ANN algorithms efficiently search for the vectors that are  approximately closest to a given query based on some distance metric like Euclidean distance  or cosine similarity. By allowing for some level of approximation, these algorithms can  significantly reduce the computational cost of nearest neighbor search without the need to  compute embedding similarities across an entire corpus. One of the most popular ANN algorithms is HNSW graphs. The hierarchical navigable small  world graph structure indexes the dataset and facilitates fast search for nearest neighbors by  grouping similar vectors together as it builds the index. HNSW organizes data into  neighborhoods, linking them with probable connections. When indexing a dense vector, it  identifies the suitable neighborhood and its potential connections, storing them in a graph  structure. During an HNSW search with a dense vector query, it locates the optimal  neighborhood entry point and returns the nearest neighbors. Applications of vector search Vector search has numerous use cases across domains due to its ability to efficiently retrieve  similar items based on their vector representations. Some common applications of vector  search include: Information retrieval Vector search is used in search engines to retrieve documents, articles, web pages or other  textual content based on their similarity to a query. It enables users to find relevant information  even if the exact terms used in the query are not present in the documents. Retrieval Augmented Generation (RAG) Vector search is instrumental in the Retrieval Augmented Generation (RAG) framework for  retrieving relevant context from a large corpus of text. RAG is a framework for generative AI that  combines vector search with generative language models to generate responses. In traditional language generation tasks, large language models (LLMs) like OpenAI’s GPT  (Generative Pre-trained Transformer) or IBM’s Granite Models are used to construct responses "
            },
            {
                "page_number": 4,
                "text": "based on the input prompt. However, these models may struggle to produce responses that are  contextually relevant, factually accurate or up to date. RAG addresses this limitation by  incorporating a retrieval step before response generation. During retrieval, vector search can be  used to identify contextually pertinent information, such as relevant passages or documents  from a large corpus of text, typically stored in a vector database. Next, an LLM is used to  generate a response based on the retrieved context. Beyond language generation, RAG and vector search have further applications in various other  NLP tasks, including question answering, chatbots, summarization and content generation Hybrid search Vector search can be integrated into hybrid search approaches to enhance the effectiveness  and flexibility of the search process. Hybrid search combines vector search with other search  techniques, such as keyword-based search or metadata-based search. Vector search may be  used to retrieve items based on their similarity to a query, while other search methods may be  used to retrieve items based on exact matches or specific criteria. Video and image search Vector stores are used in image and video search engines to index and retrieve visual content  based on similarity. Image and video embeddings are stored as vectors, enabling users to  search for visually similar images or videos across large datasets. Recommendation systems Recommendation engines in streaming services as well as e-commerce, social media and  visual media platforms can be powered by vector search. Vector search allows for the  recommendation of products, movies, music or other items based on their similarity to items  that users have interacted with or liked previously. Geospatial analysis Vector search is used in geospatial data applications to to retrieve spatial data such as points of  interest, geographic features or spatial trajectories based on their proximity or similarity to a  query location or pattern. It enables efficient spatial search and analysis in geographic  information systems and location-based services. "
            }
        ],
        "images": []
    },
    {
        "file_name": "what is rag.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "What is retrieval-augmented generation (RAG)? Retrieval-augmented generation (RAG) is an advanced artificial intelligence (AI) technique  that combines information retrieval with text generation, allowing AI models to retrieve  relevant information from a knowledge source and incorporate it into generated text. In the dynamic landscape of artificial intelligence, Retrieval-augmented generation has emerged  as a game-changer, revolutionizing the way we generate and interact with text. RAG seamlessly  marries the power of information retrieval with natural language generation using tools like large  language models (LLMs), offering a transformative approach to content creation. Origins and evolution of retrieval-augmented generation In their pivotal 2020 paper, Facebook researchers tackled the limitations of large pre-trained  language models. They introduced retrieval-augmented generation (RAG), a method that  combines two types of memory: one that's like the model's prior knowledge and another that's  like a search engine, making it smarter in accessing and using information. RAG impressed by  outperforming other models in tasks that required a lot of knowledge, like question-answering,  and by generating more accurate and varied text. This breakthrough has been embraced and  extended by researchers and practitioners and is a powerful tool for building generative AI  applications. Whether you are a seasoned AI expert or a newcomer to the field, this guide will equip you with  the knowledge needed to harness the capabilities of RAG and stay at the forefront of AI  innovation. An introduction to retrieval-augmented generation (RAG) Retrieval-augmented generation, commonly known as RAG, has been making waves in the  realm of natural language processing (NLP). At its core, RAG is a hybrid framework that  integrates retrieval models and generative models to produce text that is not only contextually  accurate but also information-rich. Significance in natural language processing (NLP) The significance of RAG in NLP cannot be overstated. Traditional language models, especially  early ones, could generate text based on the data they were trained on but could not often  source additional, specific information during the generation process. RAG fills this gap  effectively, creating a bridge between the wide-ranging capabilities of retrieval models and the  text-generating prowess of generative models, such as large language models (LLMs). By doing  so, RAG pushes the boundaries of what is possible in NLP, making it an indispensable tool for  tasks like question-answering, summarization, and much more. Synergy of retrieval and generative models Though we'll delve into more technical details in a later section, it's worth noting how RAG  marries retrieval and generative models. In a nutshell, the retrieval model acts as a specialized  'librarian,' pulling in relevant information from a database or a corpus of documents. This  information is then fed to the generative model, which acts as a 'writer,' crafting coherent and  informative text based on the retrieved data. The two work in tandem to provide answers that are  not only accurate but also contextually rich. For a deeper understanding of generative models  like LLMs, you may want to explore our guide on large language models. "
            },
            {
                "page_number": 2,
                "text": "Step by step on how retrieval-augmented generation works Retrieval-augmented generation is a technique that enhances traditional language model  responses by incorporating real-time, external data retrieval. It starts with the user's input,  which is then used to fetch relevant information from various external sources. This process  enriches the context and content of the language model's response. By combining the user's  query with up-to-date external information, RAG creates responses that are not only relevant  and specific but also reflect the latest available data. This approach significantly improves the  quality and accuracy of responses in various applications, from chatbots to information  retrieval systems. Now, let's delve into the detailed steps of how RAG operates: Step 1: Initial query processing RAG begins by comprehensively analyzing the user's input. This step involves understanding the  intent, context, and specific information requirements of the query. The accuracy of this initial  analysis is crucial as it guides the retrieval process to fetch the most relevant external data. Step 2: Retrieving external data Once the query is understood, RAG taps into a range of external data sources. These sources  could include up-to-date databases, APIs, or extensive document repositories. The goal here is  to access a breadth of information that extends beyond the language model's initial training  data. This step is vital in ensuring that the response generated is informed by the most current  and relevant information available. Step 3: Data vectorization for relevancy matching The external data, along with the user query, is transformed into numerical vector  representations. This conversion is a critical part of the process, as it enables the system to  perform complex mathematical calculations to determine the relevancy of the external data to  the user's query. The precision in this matching process directly influences the quality and  relevance of the information retrieved. Step 4: Augmentation of language model prompts With the relevant external data identified, the next step involves augmenting the language  model's prompt with this information. This augmentation is more than just adding data; it  involves integrating the new information in a way that maintains the context and flow of the  original query. This enhanced prompt allows the language model to generate responses that are  not only contextually rich but also grounded in accurate and up-to-date information. Step 5: Ongoing data updates To maintain the efficacy of the RAG system, the external data sources are regularly updated.  This ensures that the system's responses remain relevant over time. The update process can be  automated or done in periodic batches, depending on the nature of the data and the  application's requirements. This aspect of RAG highlights the importance of data dynamism and  freshness in generating accurate and useful responses. Key components of retrieval-augmented generation "
            },
            {
                "page_number": 3,
                "text": "Understanding the inner workings of retrieval-augmented generation (RAG) requires a deep dive  into its two foundational elements: retrieval models and generative models. These two  components are the cornerstones of RAG's remarkable capability to source, synthesize, and  generate information-rich text. Let's unpack what each of these models brings to the table and  what synergies they bring in a RAG framework. Retrieval models Retrieval models act as the information gatekeepers in the RAG architecture. Their primary  function is to search through a large corpus of data to find relevant pieces of information that  can be used for text generation. Think of them as specialized librarians who know exactly which  'books' to pull off the 'shelves' when you ask a question. These models use algorithms to rank  and select the most pertinent data, offering a way to introduce external knowledge into the text  generation process. By doing so, retrieval models set the stage for more informed, context-rich  language generation, elevating the capabilities of traditional language models. Retrieval models can be implemented through several mechanisms. One of the most common  techniques is through the use of vector embeddings and vector search, but also commonly  used are document indexing databases that utilize technologies like BM25 (Best Match 25) and  TF-IDF (Term Frequency—Inverse Document Frequency). Generative models Once the retrieval model has sourced the appropriate information, generative models come into  play. These models act as creative writers, synthesizing the retrieved information into coherent  and contextually relevant text. Usually built upon large language models (LLMs), generative  models can create text that is grammatically correct, semantically meaningful, and aligned with  the initial query or prompt. They take the raw data selected by the retrieval models and give it a  narrative structure, making the information easily digestible and actionable. In the RAG  framework, generative models serve as the final piece of the puzzle, providing the textual output  we interact with. Why is retrieval-augmented generation important? In the ever-evolving field of natural language processing (NLP), the quest for more intelligent,  context-aware systems is ongoing. This is where retrieval-augmented generation (RAG) comes  into the picture, addressing some of the limitations of traditional generative models. So, what  drives the increasing adoption of RAG? Firstly, RAG provides a solution for generating text that isn't just fluent but also factually  accurate and information-rich. By combining retrieval models with generative models, RAG  ensures that the text it produces is both well-informed and well-written. Retrieval models bring  the 'what'—the factual content—while generative models contribute the 'how'—the art of  composing these facts into coherent and meaningful language. Secondly, the dual nature of RAG offers an inherent advantage in tasks requiring external  knowledge or contextual understanding. For instance, in question-answering systems,  traditional generative models might struggle to offer precise answers. In contrast, RAG can pull  in real-time information through its retrieval component, making its responses more accurate  and detailed. "
            },
            {
                "page_number": 4,
                "text": "Lastly, scenarios demanding multi-step reasoning or synthesis of information from various  sources are where RAG truly shines. Think of legal research, scientific literature reviews, or even  complex customer service queries. RAG's capability to search, select, and synthesize  information makes it unparalleled in handling such intricate tasks. In summary, RAG's hybrid architecture delivers superior text generation capabilities, making it  an ideal choice for applications requiring depth, context, and factual accuracy. Exploring the technical implementation of retrieval-augmented generation with large  language models (LLMs) If the concept of retrieval-augmented generation (RAG) has piqued your interest, diving into its  technical implementation will offer invaluable insights. With large language models (LLMs) as  the backbone, RAG employs intricate processes, from data sourcing to the final output. Let's  peel back the layers to uncover the mechanics of RAG and understand how it leverages LLMs to  execute its powerful retrieval and generation capabilities. Source data The starting point of any RAG system is its source data, often consisting of a vast corpus of text  documents, websites, or databases. This data serves as the knowledge reservoir that the  retrieval model scans through to find relevant information. It's crucial to have diverse, accurate,  and high-quality source data for optimal functioning. It is also important to manage and reduce  redundancy in the source data—for example, software documentation between version 1 and  version 1.1 will be almost entirely identical to each other. Data chunking Before the retrieval model can search through the data, it's typically divided into manageable  'chunks' or segments. This chunking process ensures that the system can efficiently scan  through the data and enables quick retrieval of relevant content. Effective chunking strategies  can drastically improve the model's speed and accuracy: a document may be its own chunk,  but it could also be split up into chapters/sections, paragraphs, sentences, or even just “chunks  of words.” Remember: the goal is to be able to feed the Generative Model with information that  will enhance its generation. Text-to-vector conversion (embeddings) The next step involves converting the textual data into a format that the model can readily use.  When using a vector database, this means transforming the text into mathematical vectors via a  process known as “embedding”. These are almost always generated using complex software  models that have been built with machine learning techniques. These vectors encapsulate the  semantics and context of the text, making it easier for the retrieval model to identify relevant  data points. Many embedding models can be fine-tuned to create good semantic matching;  general-purpose embedding models such as GPT and LLaMa may not perform as well against  scientific information as a model like SciBERT, for example. Links between source data and embeddings The link between the source data and embeddings is the linchpin of the RAG architecture. A  well-orchestrated match between them ensures that the retrieval model fetches the most  relevant information, which in turn informs the generative model to produce meaningful and "
            },
            {
                "page_number": 5,
                "text": "accurate text. In essence, this link facilitates the seamless integration between the retrieval and  generative components, making the RAG model a unified system. Retrieval-augmented generation vs. semantic search RAG and semantic search are both advanced AI techniques but serve different purposes. RAG  combines information retrieval with a language model’s text generation, enhancing the model's  responses with external, contextually relevant data. It's used in applications like chatbots for  accurate, detailed responses. Semantic search, on the other hand, focuses on understanding  the intent and contextual meaning behind a search query. It improves the relevance of search  results by interpreting the nuances of language, rather than relying on keyword matching. While  RAG enriches response generation with external data, semantic search refines the process of  finding the most relevant information based on query understanding. Here is a list highlighting the key differences between RAG and semantic search: Purpose    RAG: Enhances language models by integrating external information for response  generation.    Semantic search: Improves search results relevance by understanding search intent  and context. Functionality    RAG: Retrieves and incorporates external data into language model responses.    Semantic search: Analyzes and interprets user queries for more meaningful search  outcomes. Primary use    RAG: Used in chatbots and AI-driven communication tools for accurate, detailed  responses.    Semantic search: Employed in search engines and data retrieval systems for finding  relevant information. Data handling    RAG: Focuses on augmenting text generation with additional, relevant information.    Semantic Search: Concentrates on the semantic interpretation of queries to find the  best matches. Examples and applications of retrieval-augmented generation Retrieval-augmented generation (RAG) has a diverse array of applications, spanning multiple  domains that require sophisticated natural language processing (NLP) capabilities. Its unique  approach of combining retrieval and generative components not only sets it apart from  traditional models but also provides a comprehensive solution to a myriad of NLP tasks. Here  are some compelling examples and applications that exhibit the versatility of RAG. Text summarization "
            },
            {
                "page_number": 6,
                "text": "As highlighted earlier, one of the standout applications of RAG is text summarization. Imagine  an AI-driven news aggregation platform that not only fetches the latest news but also  summarizes complex articles into digestible snippets. By leveraging RAG, the platform can  generate concise, coherent, and contextually relevant summaries, providing a rich user  experience. Question-answering systems RAG shows remarkable prowess in question-answering systems. Traditionally, QA models could  falter when the query requires a deep understanding of multiple documents or datasets.  However, RAG can scan through an extensive corpus to retrieve the most relevant information  and craft detailed, accurate answers. This makes it an indispensable tool in building intelligent  chatbots for customer service applications. Content generation In the realm of content generation, RAG offers unprecedented flexibility. Whether it's auto- generating emails, crafting social media posts, or even writing code, RAG's dual approach of  retrieval and generation ensures that the output is not just grammatically correct but also rich in  context and relevance. Addressing NLP challenges The architecture of RAG makes it exceptionally equipped to handle a wide range of NLP  challenges, from sentiment analysis to machine translation. Its capacity to understand context,  analyze large datasets, and generate meaningful output makes it a cornerstone technology for  any application that relies on language understanding. To get started on building applications with these capabilities, check out this chatbot quickstart  guide, which showcases how to utilize RAG and other advanced techniques. These examples merely scratch the surface; the applications of RAG are limited only by our  imagination and the challenges that the realm of NLP continues to present. Benefits of retrieval-augmented generation The benefits of RAG are extensive and diverse, profoundly impacting the field of artificial  intelligence and natural language processing. This advanced approach not only enhances the  capabilities of language models but also addresses some of the key limitations found in  traditional models. Here's a more detailed look at these benefits: Enhanced accuracy RAG systems incorporate current, external data to improve the accuracy of responses. This  results in output that is not only relevant but also reflects the latest information, reducing the  likelihood of outdated or incorrect answers. Dynamic content By continuously updating its external data sources, RAG ensures that the responses are current  and evolve with changing information. This dynamism is particularly valuable in fields where  data is constantly changing, like news or scientific research. Expanded knowledge base "
            },
            {
                "page_number": 7,
                "text": "RAG extends beyond the limitations of a model's training data by accessing diverse external  information sources. This broadens the scope of knowledge the model can draw upon,  enhancing the depth and breadth of its responses. Improved user trust Accurate and reliable responses, underpinned by current and authoritative data, significantly  enhance user trust in AI-driven applications. This is crucial in domains where credibility and  accuracy are paramount. Customization and control Organizations can tailor the external sources RAG draws from, allowing control over the type  and scope of information integrated into the model’s responses. This customization ensures  that the output aligns with specific needs and objectives. Efficiency in information retrieval RAG streamlines the process of sourcing and integrating information, making the response  generation not only more accurate but also more efficient. This efficiency is key in applications  where speed and precision are essential. Potential challenges and limitations of retrieval-augmented generation While RAG presents significant advancements in AI, it also encounters unique challenges: Model complexity RAG's intricate architecture, merging retrieval and generative processes, demands extensive  computational resources. This complexity adds to the challenge in debugging and optimizing  the system for efficient performance. Data preparation challenges Preparing suitable data for RAG involves ensuring the text is clean, relevant, and not redundant.  The process of segmenting this text for optimal use by the generative model is complex and  requires a careful selection of an embedding model that can perform well across diverse data  sets. Prompt engineering for LLM Effective use of RAG requires skillful prompt engineering to frame the retrieved information  appropriately for the LLM. This step is crucial to ensure that the generative model produces  high-quality responses. Performance trade-off The dual process of RAG, involving both data retrieval and text generation, can lead to increased  response times. This is particularly challenging in real-time applications, where a balance  between the depth of retrieval and the speed of response is essential. Best practices for retrieval-augmented generation implementation When venturing into the realm of retrieval-augmented generation (RAG), practitioners must  navigate a complex landscape to ensure effective implementation. Below, we outline some "
            },
            {
                "page_number": 8,
                "text": "pivotal best practices that serve as a guide to optimize the capabilities of large language models  (LLMs) via RAG. Data preparation The cornerstone of a successful RAG implementation is the quality of your data. It is imperative  to invest time and effort into data cleaning and preprocessing to enable optimal model  performance. This entails text normalization, which involves standardizing text formats, and  entity recognition and resolution, which helps the model identify and contextualize key  elements in the text. Also, eliminating irrelevant or sensitive information such as personally  identifiable information (PII) is crucial to align with privacy standards. Regular updates RAG thrives on real-time or frequently updated information. Establish a robust data pipeline that  allows for periodic updates to your data source. The frequency of these updates could range  from daily to quarterly, depending on your specific use case. Automated workflows to handle  this process are highly recommended. Frameworks such as the open-source Langstream can  combine streaming with embedding models, making this task easier. Output evaluation Measuring the model's performance is a two-pronged approach. On one end, manual evaluation  offers qualitative insights into the model's capabilities. This could involve a panel of domain  experts scrutinizing a sample set of model outputs. On the other end, automated evaluation  metrics such as BLEU, ROUGE, or METEOR can provide a quantitative assessment. User  feedback, if applicable, is another powerful tool for performance assessment. Continuous improvement The world of AI is ever-evolving, and continuous improvement is not just an ideal but a  necessity. This could mean anything from updating the training data, revising model  parameters, or even tweaking the architectural setup based on the latest research and  performance metrics. End-to-end integration For a smooth operational experience, integrating your RAG workflows into your existing MLOps  protocols is essential. This includes following best practices in continuous integration and  continuous deployment (CI/CD), implementing robust monitoring systems, and conducting  regular model audits. By adhering to these best practices, you not only optimize the performance of your RAG model  but also align it well with broader machine learning and data management ecosystems. This  holistic approach ensures that you extract the maximum utility from your RAG implementations. Embracing retrieval-augmented generation with DataStax Retrieval-augmented generation is a pivotal innovation in natural language processing (NLP),  integrating the capabilities of retrieval models and generative models to produce coherent,  context-rich text. RAG merges retrieval models, which act as 'librarians' scanning large databases for pertinent  information, with generative models, which function as 'writers,' synthesizing this information "
            },
            {
                "page_number": 9,
                "text": "into text more relevant to the task. It is versatile and applicable in diverse areas such as real- time news summarization, automated customer service, and complex research tasks. RAG requires retrieval models such as vector search across embeddings, combined with a  generative model typically built upon LLMs which can synthesize the retrieved information into a  useful response. Even though it is more complicated than using an LLM on its own, RAG has been proven to  improve the accuracy and quality of AI-backed applications. Check out this recorded webinar  which discusses, in part, how companies like Shopify and Instacart have incorporated RAG in  their products. Solutions such as LangChain’s Cassandra vector store, the aforementioned Langstream, and  DataStax Astra DB can reduce the development and operational burden of applications that  incorporate vector search. Astra DB Vector is the only vector database for building production-level AI applications on real- time data, seamlessly incorporating a NoSQL database with streaming capabilities. If you’d like  to get started with the most scalable vector database, you can register now and get going in  minutes! "
            }
        ],
        "images": []
    },
    {
        "file_name": "word_embedings.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "A Guide on Word Embeddings in NLP Word embedding in NLP is an important term that is used for representing words for text  analysis in the form of real-valued vectors. It is an advancement in NLP that has improved the  ability of computers to understand text-based content in a better way. It is considered one of the  most significant breakthroughs of deep learning for solving challenging natural language  processing problems. In this approach, words and documents are represented in the form of numeric vectors allowing  similar words to have similar vector representations. The extracted features are fed into a  machine learning model so as to work with text data and preserve the semantic and syntactic  information. This information once received in its converted form is used by NLP algorithms that  easily digest these learned representations and process textual information. Due to the perks this technology brings on the table, the popularity of ML NLP is surging making  it one of the most chosen fields by the developers. Now that you have a basic understanding of the topic, let us start from scratch by introducing  you to word embeddings, its techniques, and applications. What is word embedding? Word embedding or word vector is an approach with which we represent documents and words.  It is defined as a numeric vector input that allows words with similar meanings to have the same  representation. It can approximate meaning and represent a word in a lower dimensional space.  These can be trained much faster than the hand-built models that use graph embeddings like  WordNet. For instance, a word embedding with 50 values holds the capability of representing 50 unique  features. Many people choose pre-trained word embedding models like Flair, fastText, SpaCy,  and others. We will discuss it further in the article. Let’s move on to learn it briefly with an example of the  same. The problem Given a supervised learning task to predict which tweets are about real disasters and which  ones are not (classification). Here the independent variable would be the tweets (text) and the  target variable would be the binary values (1: Real Disaster, 0: Not real Disaster). Now, Machine Learning and Deep Learning algorithms only take numeric input. So, how do we  convert tweets to their numeric values? We will dive deep into the techniques to solve such  problems, but first let’s look at the solution provided by word embedding. The solution Word Embeddings in NLP is a technique where individual words are represented as real-valued  vectors in a lower-dimensional space and captures inter-word semantics. Each word is  represented by a real-valued vector with tens or hundreds of dimensions. Term frequency-inverse document frequency (TF-IDF) "
            },
            {
                "page_number": 2,
                "text": "Term frequency-inverse document frequency is the machine learning algorithm that is used for  word embedding for text. It comprises two metrics, namely term frequency (TF) and inverse  document frequency (IDF). This algorithm works on a statistical measure of finding word relevance in the text that can be in  the form of a single document or various documents that are referred to as corpus. The term frequency (TF) score measures the frequency of words in a particular document. In  simple words, it means that the occurrence of words is counted in the documents. The inverse document frequency or the IDF score measures the rarity of the words in the text. It  is given more importance over the term frequency score because even though the TF score gives  more weightage to frequently occurring words, the IDF score focuses on rarely used words in  the corpus that may hold significant information. TF-IDF algorithm finds application in solving simpler natural language processing and machine  learning problems for tasks like information retrieval, stop words removal, keyword extraction,  and basic text analysis. However, it does not capture the semantic meaning of words efficiently  in a sequence. Now let’s understand it further with an example. We will see how vectorization is done in TF-IDF. To create TF-IDF vectors, we use Scikit-learn’s TF-IDF Vectorizer. After applying it to the previous  4 sample tweets, we obtain - Output of TfidfVectorizer The rows represent each document, the columns represent the vocabulary, and the values of tf- idf(i,j) are obtained through the above formula. This matrix obtained can be used along with the  target variable to train a machine learning/deep learning model. Let us now discuss two different approaches to word embeddings. We’ll also look at the hands- on part! Bag of words (BOW) "
            },
            {
                "page_number": 3,
                "text": "A bag of words is one of the popular word embedding techniques of text where each value in  the vector would represent the count of words in a document/sentence. In other words, it  extracts features from the text. We also refer to it as vectorization. To get you started, here’s how you can proceed to create BOW.    In the first step, you have to tokenize the text into sentences.    Next, the sentences tokenized in the first step have further tokenized words.    Eliminate any stop words or punctuation.    Then, convert all the words to lowercase.    Finally, move to create a frequency distribution chart of the words. We will discuss BOW with proper example in the continuous bag of word selection below. Word2Vec Word2Vec method was developed by Google in 2013. Presently, we use this technique for all  advanced natural language processing (NLP) problems. It was invented for training word  embeddings and is based on a distributional hypothesis. In this hypothesis, it uses skip-grams or a continuous bag of words (CBOW). These are basically shallow neural networks that have an input layer, an output layer, and a  projection layer. It reconstructs the linguistic context of words by considering both the order of  words in history as well as the future. The method involves iteration over a corpus of text to learn the association between the words.  It relies on a hypothesis that the neighboring words in a text have semantic similarities with each  other. It assists in mapping semantically similar words to geometrically close embedding  vectors. It uses the cosine similarity metric to measure semantic similarity. Cosine similarity is equal to  Cos(angle) where the angle is measured between the vector representation of two  words/documents.    So if the cosine angle is one, it means that the words are overlapping.    And if the cosine angle is a right angle or 90°, It means words hold no contextual  similarity and are independent of each other. To summarize, we can say that this metric assigns similar vector representations to the same  boards. Two variants of Word2Vec Word2Vec has two neural network-based variants: Continuous Bag of Words (CBOW) and Skip- gram. 1. CBOW - The continuous bag of words variant includes various inputs that are taken by the  neural network model. Out of this, it predicts the targeted word that closely relates to the  context of different words fed as input. It is fast and a great way to find better numerical "
            },
            {
                "page_number": 4,
                "text": "representation for frequently occurring words. Let us understand the concept of context and the  current word for CBOW. In CBOW, we define a window size. The middle word is the current word and the surrounding  words (past and future words) are the context. CBOW utilizes the context to predict the current  words. Each word is encoded using One Hot Encoding in the defined vocabulary and sent to the  CBOW neural network. "
            },
            {
                "page_number": 5,
                "text": "The hidden layer is a standard fully-connected dense layer. The output layer generates  probabilities for the target word from the vocabulary. As we have discussed earlier about the bag of words (BOW) and it being also termed as  vectorizer, we will take an example here to clarify it further. Let's take a small part of disaster tweets, 4 tweets, to understand how BOW works:- ‘kind true sadly’, ‘swear jam set world ablaze’, ‘swear true car accident’, ‘car sadly car caught up fire’ To create BOW, we use Scikit-learn’s CountVectorizer, which tokenizes a collection of text  documents, builds a vocabulary of known words, and encodes new documents using that  vocabulary. Output of CountVectorizer Here the rows represent each document (4 in our case), the columns represent the vocabulary  (unique words in all the documents) and the values represent the count of the words of the  respective rows. In the same way, we can apply CountVectorizer to the complete training data tweets (11,370  documents) and obtain a matrix that can be used along with the target variable to train a  machine learning/deep learning model. 2. Skip-gram — Skip-gram is a slightly different word embedding technique in comparison to  CBOW as it does not predict the current word based on the context. Instead, each current word  is used as an input to a log-linear classifier along with a continuous projection layer. This way, it  predicts words in a certain range before and after the current word. This variant takes only one word as an input and then predicts the closely related context words.  That is the reason it can efficiently represent rare words. "
            },
            {
                "page_number": 6,
                "text": "The end goal of Word2Vec (both variants) is to learn the weights of the hidden layer. The hidden  consequences will be used as our word embeddings!! Let's now see the code for creating  custom word embeddings using Word2Vec- Import Libraries from gensim.models import Word2Vec import nltk import re from nltk.corpus import stopwords Preprocess the Text "
            },
            {
                "page_number": 7,
                "text": "#Word2Vec inputs a corpus of documents split into constituent words. corpus = [] for i in range(0,len(X)): tweet = re.sub(“[^a-zA-Z]”,” “,X[i]) tweet = tweet.lower() tweet = tweet.split() corpus.append(tweet) Here is the exciting part! Let's try to see the most similar words (vector representations) of some  random words from the tweets -  model.wv.most_similar(‘disaster’) Output - List of tuples of words and their predicted probability  The embedding vector of ‘disaster’ - "
            },
            {
                "page_number": 8,
                "text": "List of tuples of words and their predicted probability  The embedding vector of ‘disaster’ - dimensionality = 100 "
            },
            {
                "page_number": 9,
                "text": "Challenges with the bag of words and TF-IDF Now let’s discuss the challenges with the two text vectorization techniques we have discussed  till now. In BOW, the size of the vector is equal to the number of elements in the vocabulary. If most of  the values in the vector are zero then the bag of words will be a sparse matrix. Sparse  representations are harder to model both for computational reasons and also for informational  reasons. Also, in BOW there is a lack of meaningful relations and no consideration for the order of words.  Here’s more that adds to the challenge with this word embedding technique.    Massive amount of weights: Large amounts of input vectors invite massive amounts of  weight for a neural network.    No meaningful relations or consideration for word order: The bag of words does not  consider the order in which the words appear in the sentences or a text.    Computationally intensive: With more weight comes the need for more computation to  train and predict. While the TF-IDF model contains the information on the more important words and the less  important ones, it does not solve the challenge of high dimensionality and sparsity, and unlike  BOW it also makes no use of semantic similarities between words. GloVe: Global Vector for word representation GloVe method of word embedding in NLP was developed at Stanford by Pennington, et al. It is  referred to as global vectors because the global corpus statistics were captured directly by the  model. It finds great performance in world analogy and named entity recognition problems. This technique reduces the computational cost of training the model because of a simpler least  square cost or error function that further results in different and improved word embeddings. It  leverages local context window methods like the skip-gram model of Mikolov and Global Matrix  factorization methods for generating low dimensional word representations. Latent semantic analysis (LSA) is a Global Matrix factorization method that does not do well on  world analogy but leverages statistical information indicating a sub-optimal vector space  structure. On the contrary, the skip-gram method performs better on the analogy task. However, it does  not utilize the statistics of the corpus properly because of no training on global co-occurrence  counts. So, unlike Word2Vec, which creates word embeddings using local context, GloVe focuses on  global context to create word embeddings which gives it an edge over Word2Vec. In GloVe, the  semantic relationship between the words is obtained using a co-occurrence matrix. Consider two sentences - I am a data science enthusiast I am looking for a data science job The co-occurrence matrix involved in GloVe would look like this for the above sentences - "
            },
            {
                "page_number": 10,
                "text": "Window Size = 1 Each value in this matrix represents the count of co-occurrence with the corresponding word in  row/column. Observe here - this co-occurrence matrix is created using global word co- occurrence count (no. of times the words appeared consecutively; for window size=1). If a text  corpus has 1m unique words, the co-occurrence matrix would be 1m x 1m in shape. The core  idea behind GloVe is that the word co-occurrence is the most important statistical information  available for the model to ‘learn’ the word representation. Let's now see an example from Stanford’s GloVe paper of how the co-occurrence probability  rations work in GloVe. “For example, consider the co-occurrence probabilities for target words  ice and steam with various probe words from the vocabulary. Here are some actual probabilities  from a corpus of 6 billion words:” Here, Let's take k = solid i.e, words related to ice but unrelated to steam. The expected Pik /Pjk ratio  will be large. Similarly, for words k which are related to steam but not to ice, say k = gas, the ratio  will be small. For words like water or fashion, which are either related to both ice and steam or  neither to both respectively, the ratio should be approximately one. The probability ratio is able to better distinguish relevant words (solid and gas) from irrelevant  words (fashion and water) than the raw probability. It is also able to better discriminate between "
            },
            {
                "page_number": 11,
                "text": "two relevant words. Hence in GloVe, the starting point for word vector learning is ratios of co- occurrence probabilities rather than the probabilities themselves. Enough of the theory. Time for the code! Import Libraries import nltk import re from nltk.corpus import stopwords from glove import Corpus, Glove Text Preprocessing #GloVe inputs a corpus of documents splitted into constituent words corpus = [] for i in range(0,len(X)): tweet = re.sub(“[^a-zA-Z]”,” “,X[i]) tweet = tweet.lower() tweet = tweet.split() corpus.append(tweet) "
            },
            {
                "page_number": 12,
                "text": "Train the word Embeddings corpus = Corpus() corpus.fit(text_corpus,window = 5) glove = Glove(no_components=100, learning_rate=0.05) #no_components = dimensionality of word embeddings = 100 glove.fit(corpus.matrix, epochs=100, no_threads=4, verbose=True) glove.add_dictionary(corpus.dictionary) Find most similar - glove.most_similar(“storm”,number=10) Output - List of tuples of words and their predicted probability BERT (Bidirectional encoder representations from transformers) This natural language processing (NLP) based language algorithm belongs to a class known as  transformers. It comes in two variants namely BERT-Base, which includes 110 million  parameters, and BERT-Large, which has 340 million parameters. It relies on an attention mechanism for generating high-quality world embeddings that are  contextualized. So when the embedding goes through the training process, they are passed  through each BERT layer so that its attention mechanism can capture the word associations  based on the words on the left and those on the right. It is an advanced technique in comparison to the discussed above as it creates better word  embedding. The credit goes to the pre-trend model on Wikipedia data sets and massive word  corpus. This technique can be further improved for task-specific data sets by fine-tuning the  embeddings. It finds great application in language translation tasks. "
            },
            {
                "page_number": 13,
                "text": "Conclusion Word embeddings can train deep learning models like GRU, LSTM, and Transformers, which  have been successful in NLP tasks such as sentiment classification, name entity recognition,  speech recognition, etc. Here’s a final checklist for a recap.    Bag of words: Extracts features from the text    TF-IDF: Information retrieval, keyword extraction    Word2Vec: Semantic analysis task    GloVe: Word analogy, named entity recognition tasks    BERT: language translation, question answering system In this blog, we discussed the two techniques for vectorizations in NLP: the Bag of Words and  TF-IDF, their drawbacks, and how word-embedding techniques like GloVe and Word2Vec  overcome their drawbacks by dimensionality reduction and context similarity. With all said  above, you would have a better understanding of how word embeddings benefits your day-to- day life as well. "
            }
        ],
        "images": [
            "Image_19",
            "Image_20",
            "Image_21",
            "Image_35",
            "Image_36",
            "Image_44",
            "Image_47",
            "Image_50",
            "Image_53",
            "Image_58",
            "Image_59",
            "Image_60",
            "Image_63",
            "Image_67",
            "Image_70"
        ]
    },
    {
        "file_name": "word_embedings2.pdf",
        "title": "Untitled Article",
        "pages": [
            {
                "page_number": 1,
                "text": "What are word embeddings? Word embeddings are a way of representing words as vectors in a multi-dimensional space,  where the distance and direction between vectors reflect the similarity and relationships among  the corresponding words. The development of embedding to represent text has played a crucial role in advancing natural  language processing (NLP) and machine learning (ML) applications. Word embeddings have  become integral to tasks such as text classification, sentiment analysis, machine translation  and more. Traditional methods of representing words in a way that machines can understand, such as  one-hot encoding, represent each word as a sparse vector with a dimension equal to the size of  the vocabulary. Here, only one element of the vector is 'hot' (set to 1) to indicate the presence  of that word. While simple, this approach suffers from the curse of dimensionality, lacks  semantic information and doesn't capture relationships between words. Word embeddings, on the other hand, are dense vectors with continuous values that are trained  using machine learning techniques, often based on neural networks. The idea is to learn  representations that encode semantic meaning and relationships between words. Word  embeddings are trained by exposing a model to a large amount of text data and adjusting the  vector representations based on the context in which words appear. One popular method for training word embeddings is Word2Vec, which uses a neural network to  predict the surrounding words of a target word in a given context. Another widely used approach  is GloVe (Global Vectors for Word Representation), which leverages global statistics to create  embeddings. Word embeddings have proven invaluable for NLP tasks, as they allow machine learning  algorithms to understand and process the semantic relationships between words in a more  nuanced way compared to traditional methods. How word embeddings are used Word embeddings are used in a variety of NLP tasks to enhance the representation of words and  capture semantic relationships, including: Text classification Word embeddings are often used as features in text classification tasks, such as sentiment  analysis, spam detection and topic categorization. Named Entity Recognition (NER) To accurately identify and classify entities (e.g., names of people, organizations, locations) in  text, word embeddings help the model understand the context and relationships between  words. Machine translation In machine translation systems, word embeddings help represent words in a language-agnostic  way, allowing the model to better understand the semantic relationships between words in the  source and target languages. Information retrieval "
            },
            {
                "page_number": 2,
                "text": "In information retrieval systems, word embeddings can enable more accurate matching of user  queries with relevant documents, which improves the effectiveness of search engines and  recommendation systems. Question answering Word embeddings contribute to the success of question answering systems by enhancing the  understanding of the context in which questions are posed and answers are found. Semantic similarity and clustering Word embeddings enable measuring semantic similarity between words or documents for tasks  like clustering related articles, finding similar documents or recommending similar items based  on their textual content. Text generation In text generation tasks, such as language modeling and autoencoders, word embeddings are  often used to represent the input text and generate coherent and contextually relevant output  sequences. Similarity and analogy Word embeddings can be used to perform word similarity tasks (e.g., finding words similar to a  given word) and word analogy tasks (e.g., 'king' is to 'queen' as 'man' is to 'woman'). Pre-training models Pre-trained word embeddings serve as a foundation for pre-training more advanced language  representation models, such as BERT (Bidirectional Encoder Representations from  Transformers) and GPT (Generative Pre-trained Transformer). A brief history of word embeddings In the 2000s, researchers began exploring neural language models (NLMs), which use neural  networks to model the relationships between words in a continuous space. These early models  laid the foundation for the later development of word embeddings. Bengio et al. (2003) introduced feedforward neural networks for language modeling. These  models were capable of capturing distributed representations of words, but they were limited in  their ability to handle large vocabularies. Researchers, including Mnih and Hinton (2009), explored probabilistic models for learning  distributed representations of words. These models focused on capturing semantic  relationships between words and were an important step toward word embeddings. The Word2Vec model, introduced by Tomas Mikolov and his colleagues at Google in 2013,  marked a significant breakthrough. Word2Vec leverages two models, Continuous Bag of Words  (CBOW) and Continuous Skip-gram, which efficiently learn word embeddings from large  corpora and have become widely adopted due to their simplicity and effectiveness. GloVe (Global Vectors for Word Representation), introduced by Pennington et al. in 2014, is  based on the idea of using global statistics (word co-occurrence frequencies) to learn vector  representations for words. It has been used in various NLP applications and is known for its  ability to capture semantic relationships. "
            },
            {
                "page_number": 3,
                "text": "Today, with the rise of deep learning, embedding layers have become a standard component of  neural network architectures for NLP tasks. Embeddings are now used not only for words but  also for entities, phrases and other linguistic units. In large part, word embeddings have allowed  language models like recurrent neural networks (RNNs), long short-term memory (LSTM)  networks, Embeddings from Language Models (ELMo), BERT, ALBERT (a light BERT) and GPT to  evolve at such a blistering pace. How word embeddings are created The primary goal of word embeddings is to represent words in a way that captures their  semantic relationships and contextual information. These vectors are numerical  representations in a continuous vector space, where the relative positions of vectors reflect the  semantic similarities and relationships between words. The reason vectors are used to represent words is that most machine learning algorithms,  including neural networks, are incapable of processing plain text in its raw form. They require  numbers as inputs to perform any task. The process of creating word embeddings involves training a model on a large corpus of text  (e.g., Wikipedia or Google News). The corpus is preprocessed by tokenizing the text into words,  removing stop words and punctuation and performing other text-cleaning tasks. A sliding context window is applied to the text, and for each target word, the surrounding words  within the window are considered as context words. The word embedding model is trained to  predict a target word based on its context words or vice versa. This allows models to capture diverse linguistic patterns and assign each word a unique vector,  which represents the word's position in a continuous vector space. Words with similar  meanings are positioned close to each other, and the distance and direction between vectors  encode the degree of similarity. The training process involves adjusting the parameters of the embedding model to minimize the  difference between predicted and actual words in context. Here's a simplified example of word embeddings for a very small corpus (6 words), where each  word is represented as a 3-dimensional vector: cat          [0.2, -0.4, 0.7]      dog         [0.6, 0.1, 0.5]      apple      [0.8, -0.2, -0.3]      orange    [0.7, -0.1, -0.6]      happy     [-0.5, 0.9, 0.2]      sad         [0.4, -0.7, -0.5] In this example, each word (e.g., 'cat,' 'dog,' 'apple') is associated with a unique vector. The  values in the vector represent the word's position in a continuous 3-dimensional vector space.  Words with similar meanings or contexts are expected to have similar vector representations.  For instance, the vectors for 'cat' and 'dog' are close together, reflecting their semantic  relationship. Likewise, the vectors for 'happy' and 'sad' have opposite directions, indicating  their contrasting meanings. "
            },
            {
                "page_number": 4,
                "text": "The example above is highly simplified for illustration purposes. Actual word embeddings  typically have hundreds of dimensions to capture more intricate relationships and nuances in  meaning. Foundational aspects of word embeddings Word embeddings have become a fundamental tool in NLP, providing a foundation for  understanding and representing language in a way that aligns with the underlying semantics of  words and phrases. Below are some of the key concepts and developments that have made using word embeddings  such a powerful technique in helping advance NLP. Distributional Hypothesis The Distributional Hypothesis posits that words with similar meanings tend to occur in similar  contexts. This concept forms the basis for many word embedding models, as they aim to  capture semantic relationships by analyzing patterns of word co-occurrence. Dimensionality reduction Unlike traditional one-hot encoding, word embeddings are dense vectors of lower  dimensionality. This reduces the computational complexity and memory requirements, making  them suitable for large-scale NLP applications. Semantic representation Word embeddings capture semantic relationships between words, allowing models to  understand and represent words in a continuous vector space where similar words are close to  each other. This semantic representation enables more nuanced understanding of language. Contextual information Word embeddings capture contextual information by considering the words that co-occur in a  given context. This helps models understand the meaning of a word based on its surrounding  words, leading to better representation of phrases and sentences. Generalization Word embeddings generalize well to unseen words or rare words because they learn to  represent words based on their context. This is particularly advantageous when working with  diverse and evolving vocabularies. Two approaches to word embeddings Frequency-based and prediction-based embedding methods represent two broad categories of  approaches in the context of word embeddings. These methods mainly differ in how they  generate vector representations for words. Frequency-based embeddings Frequency-based embeddings refer to word representations that are derived from the frequency  of words in a corpus. These embeddings are based on the idea that the importance or  significance of a word can be inferred from how frequently it occurs in the text. "
            },
            {
                "page_number": 5,
                "text": "One example of frequency-based embeddings is Term Frequency-Inverse Document  Frequency (TF-IDF). TF-IDF is designed to highlight words that are both frequent within a  specific document and relatively rare across the entire corpus, thus helping to identify terms  that are significant for a particular document. The TF-IDF score for a term (word) in a document is calculated using the following formula: TF-IDF (t,d,D) = TF(t,d) x IDF(t, D) Applications of TF-IDF include information retrieval, document ranking, text summarization and  text mining. Although frequency-based embeddings are straightforward and easy to understand, they lack  the depth of semantic information and context awareness provided by more advanced  prediction-based embeddings. Prediction-based embeddings Prediction-based embeddings are word representations derived from models that are trained to  predict certain aspects of a word's context or neighboring words. Unlike frequency-based  embeddings that focus on word occurrence statistics, prediction-based embeddings capture  semantic relationships and contextual information, providing richer representations of word  meanings. Prediction-based embeddings can differentiate between synonyms and handle polysemy  (multiple meanings of a word) more effectively. The vector space properties of prediction-based  embeddings enable tasks like measuring word similarity and solving analogies. Prediction- based embeddings can also generalize well to unseen words or contexts, making them robust in  handling out-of-vocabulary terms. Prediction-based methods, particularly those like Word2Vec and GloVe (discussed below), have  become dominant in the field of word embeddings due to their ability to capture rich semantic  meaning and generalize well to various NLP tasks. Word2Vec Developed by a team of researchers at Google, including Tomas Mikolov, in 2013, Word2Vec  (Word to Vector) has become a foundational technique for learning word embeddings in natural  language processing (NLP) and machine learning models. Word2Vec consists of two main models for generating vector representations: Continuous Bag  of Words (CBOW) and Continuous Skip-gram. In the context of Word2Vec, the Continuous Bag of Words (CBOW) model aims to predict a  target word based on its surrounding context words within a given window. It uses the context  words to predict the target word, and the learned embeddings capture semantic relationships  between words. The Continuous Skip-gram model, on the other hand, takes a target word as input and aims to  predict the surrounding context words. How the models are trained Given a sequence of words in a sentence, the CBOW model takes a fixed number of context  words (words surrounding the target word) as input. Each context word is represented as an "
            },
            {
                "page_number": 6,
                "text": "embedding (vector) through a shared embedding layer. These embeddings are learned during  the training process. The individual context word embeddings are aggregated, typically by summing or averaging  them. This aggregated representation serves as the input to the next layer. The aggregated representation is then used to predict the target word using a softmax activation  function. The model is trained to minimize the difference between its predicted probability  distribution over the vocabulary and the actual distribution (one-hot encoded representation)  for the target word. The CBOW model is trained by adjusting the weights of the embedding layer based on its ability  to predict the target word accurately. The Continuous Skip-gram model uses training data to predict the context words based on the  target word's embedding. Specifically, it outputs a probability distribution over the vocabulary,  indicating the likelihood of each word being in the context given the target word. The training objective is to maximize the likelihood of the actual context words given the target  word. This involves adjusting the weights of the embedding layer to minimize the difference  between the predicted probabilities and the actual distribution of context words. The model  also allows for a flexible context window size. It can be adjusted based on the specific  requirements of the task, allowing users to capture both local and global context relationships. The Skip-gram model is essentially 'skipping' from the target word to predict its context, which  makes it particularly effective in capturing semantic relationships and similarities between  words. Advantages and limitations Both models used by Word2Vec have their own advantages and limitations. Skip-gram works  well with handling vast amounts of text data and is found to represent rare words well. CBOW,  on the other hand, is faster and has better representations for more frequent words. As far as limitations, Word2Vec may not effectively handle polysemy, where a single word has  multiple meanings. The model might average or mix the representations of different senses of a  polysemous word. Word2Vec also treats words as atomic units and does not capture subword  information. Addressing some of these limitations has been the motivation for the development of more  advanced models, such as FastText, GloVe and transformer-based models (discussed below),  which aim to overcome some of Word2Vec’s shortcomings. GloVe GloVe (Global Vectors for Word Representation) is a word embedding model designed to  capture global statistical information about word co-occurrence patterns in a corpus. Introduced by Jeffrey Pennington, Richard Socher and Christopher D. Manning in 2014, the  GloVe model differs from Word2Vec by emphasizing the use of global information rather than  focusing solely on local context. "
            },
            {
                "page_number": 7,
                "text": "GloVe is based on the idea that the global statistics of word co-occurrence across the entire  corpus are crucial for capturing word semantics. It considers how frequently words co-occur  with each other in the entire dataset rather than just in the local context of individual words. The model aims to minimize the difference between the predicted co-occurrence probabilities  and the actual probabilities derived from the corpus statistics. GloVe is computationally efficient compared to some other methods, as it relies on global  statistics and employs matrix factorization techniques to learn the word vectors. The model can  be trained on large corpora without the need for extensive computational resources. GloVe introduces scalar weights for word pairs to control the influence of different word pairs on  the training process. These weights help mitigate the impact of very frequent or rare word pairs  on the learned embeddings. Training mechanism Unlike the Word2Vec models (CBOW and Skip-gram), which focus on predicting context words  given a target word or vice versa, GloVe uses a different approach that involves optimizing word  vectors based on their co-occurrence probabilities. The training process is designed to learn  embeddings that effectively capture the semantic relationships between words. The first step is constructing a co-occurrence matrix that represents how often words appear  together in the corpus. Next is formulating an objective function that describes the relationship between word vectors  and their co-occurrence probabilities. The objective function is optimized using gradient descent or other optimization algorithms. The  goal is to adjust the word vectors and biases to minimize the squared difference between the  predicted and actual logarithmic co-occurrence probabilities. Applications and use cases Users can download pre-trained GloVe embeddings and fine-tune them for specific applications  or use them directly. GloVe embeddings are widely used in NLP tasks, such as text classification, sentiment analysis,  machine translation and more. GloVe excels in scenarios where capturing global semantic relationships, understanding the  overall context of words and leveraging co-occurrence statistics are critical for the success of  natural language processing tasks. Beyond Word2Vec and GloVe The success of Word2Vec and GloVe have inspired further research into more sophisticated  language representation models, such as FastText, BERT and GPT. These models leverage  subword embeddings, attention mechanisms and transformers to effectively handle higher  dimension embeddings. Subword embeddings "
            },
            {
                "page_number": 8,
                "text": "Subword embeddings, such as FastText, represent words as combinations of subword units,  providing more flexibility and handling rare or out-of-vocabulary words. Subword embeddings  improve the robustness and coverage of word embeddings. Unlike GloVe, FastText embeds words by treating each word as being composed of character n- grams instead of a word whole. This feature enables it not only to learn rare words but also out- of-vocabulary words. Attention mechanisms and transformers Attention mechanisms and transformer models consider contextual information and  bidirectional relationships between words, leading to more advanced language representations. Attention mechanisms were introduced to improve the ability of neural networks to focus on  specific parts of the input sequence when making predictions. Instead of treating all parts of the  input equally, attention mechanisms allow the model to selectively attend to relevant portions  of the input. Transformers have become the backbone of various state-of-the-art models in NLP, including  BERT, GPT and T5 (Text-to-Text Transfer Transformer), among others. They excel in tasks such as  language modeling, machine translation, text generation and question answering. Transformers use a self-attention mechanism to capture relationships between different words  in a sequence. This mechanism allows each word to attend to all other words in the sequence,  capturing long-range dependencies. Transformers allow for more parallelization during training compared to RNNs and are  computationally efficient. "
            }
        ],
        "images": []
    }
]